# -*- coding: utf-8 -*-
"""RAG Agent Implementation (v2 â€” ê³ ë„í™”).

ë³€ê²½ì  (v1 ëŒ€ë¹„):
- ë©€í‹°í„´ ëŒ€í™” íˆìŠ¤í† ë¦¬ (ìµœê·¼ 5í„´)
- ë¹„ì¦ˆë‹ˆìŠ¤ í˜ë¥´ì†Œë‚˜ë³„ í†¤ ì „í™˜
- ìë™ ë¶„ì„ íŠ¸ë¦¬ê±° (ì™œ? â†’ íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰)
- í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ë¶„ë¦¬ (prompts.py)
"""

import os
import logging
from typing import Optional, List, Dict

from engine.config import WhyLabConfig
from engine.rag.store import VectorStore
from engine.rag.loader import KnowledgeLoader
from engine.rag.prompts import (
    SYSTEM_PROMPT,
    build_query_prompt,
    should_trigger_analysis,
)

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ìµœëŒ€ ë³´ê´€ ìˆ˜
MAX_HISTORY_TURNS = 5


class RAGAgent:
    """Retrieval-Augmented Generation Agent (v2).

    ì£¼ìš” ê¸°ëŠ¥:
    - ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
    - ë©€í‹°í„´ ëŒ€í™” íˆìŠ¤í† ë¦¬
    - ë¹„ì¦ˆë‹ˆìŠ¤ í˜ë¥´ì†Œë‚˜ë³„ ë‹µë³€ ìƒì„±
    - ìë™ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ íŠ¸ë¦¬ê±°
    """

    def __init__(self, config: WhyLabConfig):
        self.config = config
        self.logger = logging.getLogger("whylab.rag.agent")
        self.history: List[Dict[str, str]] = []

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.store = VectorStore(
            persist_directory=str(config.paths.data_dir / "knowledge_db")
        )
        self.loader = KnowledgeLoader()

        # LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.api_key = os.environ.get("GEMINI_API_KEY") or os.environ.get(
            "GOOGLE_API_KEY"
        )
        if self.api_key:
            import google.generativeai as genai

            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel("gemini-2.0-flash")
        else:
            self.logger.warning("LLM API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. RAG ê¸°ëŠ¥ ì œí•œë¨.")
            self.model = None

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì§€ì‹ ì¸ë±ì‹±
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def index_knowledge(self):
        """ìµœì‹  ë¦¬í¬íŠ¸ì™€ ë°ì´í„°ë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¸ë±ì‹±í•©ë‹ˆë‹¤."""
        self.logger.info("Knowledge Indexing ì‹œì‘...")

        # 1. Markdown ë¦¬í¬íŠ¸ ë¡œë“œ
        report_dir = self.config.paths.reports_dir
        reports = list(report_dir.glob("whylab_report_*.md"))
        if reports:
            latest_report = sorted(reports)[-1]
            self.logger.info(f"Report ë¡œë“œ: {latest_report}")
            docs, metas = self.loader.load_markdown_report(str(latest_report))
            if docs:
                ids = [
                    f"report_{latest_report.stem}_{i}" for i in range(len(docs))
                ]
                self.store.add_documents(docs, metas, ids)

        # 2. JSON ë°ì´í„° ë¡œë“œ
        json_path = self.config.paths.dashboard_data_dir / "latest.json"
        if json_path.exists():
            self.logger.info(f"Metric ë¡œë“œ: {json_path}")
            docs, metas = self.loader.load_json_metric(str(json_path))
            if docs:
                ids = [f"metric_{i}" for i in range(len(docs))]
                self.store.add_documents(docs, metas, ids)

        self.logger.info("Indexing ì™„ë£Œ")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _add_to_history(self, role: str, content: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€í•˜ê³ , ìµœëŒ€ í„´ìˆ˜ë¥¼ ì´ˆê³¼í•˜ë©´ ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±°."""
        self.history.append({"role": role, "content": content})
        # ìµœê·¼ Ní„´ë§Œ ìœ ì§€ (user + assistant = 2 * N ë©”ì‹œì§€)
        max_messages = MAX_HISTORY_TURNS * 2
        if len(self.history) > max_messages:
            self.history = self.history[-max_messages:]

    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.history.clear()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì§ˆì˜ ì‘ë‹µ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def ask(
        self,
        query: str,
        persona: str = "product_owner",
        auto_analyze: bool = True,
    ) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸.
            persona: ë‹µë³€ í˜ë¥´ì†Œë‚˜ ("growth_hacker"|"risk_manager"|"product_owner").
            auto_analyze: Trueì´ë©´, ì¸ê³¼ ì§ˆë¬¸ ê°ì§€ ì‹œ ìë™ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰.

        Returns:
            ì—ì´ì „íŠ¸ ë‹µë³€ ë¬¸ìì—´.
        """
        self._add_to_history("user", query)

        # 1. ìë™ ë¶„ì„ íŠ¸ë¦¬ê±° í™•ì¸
        if auto_analyze and should_trigger_analysis(query):
            self.logger.info("ğŸ”¬ ìë™ ë¶„ì„ íŠ¸ë¦¬ê±° ê°ì§€: '%s'", query)
            self._run_auto_analysis()

        # 2. Retrieve
        try:
            results = self.store.query(query, n_results=5)
            context_docs = results.get("documents", [[]])[0]
        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            context_docs = []

        context_text = (
            "\n\n".join(context_docs)
            if context_docs
            else "ê´€ë ¨ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        )

        # 3. LLM ìƒì„±
        if not self.model:
            answer = self._mock_response(context_text)
        else:
            answer = self._generate_with_llm(context_text, query, persona)

        self._add_to_history("assistant", answer)
        return answer

    def _generate_with_llm(
        self, context: str, query: str, persona: str
    ) -> str:
        """LLMìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        prompt = build_query_prompt(
            context=context,
            query=query,
            history=self.history[:-1],  # í˜„ì¬ ì§ˆë¬¸ ì œì™¸
            persona=persona,
        )

        try:
            response = self.model.generate_content(
                [SYSTEM_PROMPT, prompt]
            )
            return response.text.strip()
        except Exception as e:
            self.logger.error(f"LLM ìƒì„± ì‹¤íŒ¨: {e}")
            return f"[Error] ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

    def _mock_response(self, context: str) -> str:
        """API Key ì—†ì„ ë•Œ ê²€ìƒ‰ëœ Contextë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return (
            "[Mock Mode] API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
            f"ê²€ìƒ‰ëœ ë¬¸ë§¥:\n{context}\n\n"
            "(ì‹¤ì œ ë‹µë³€ì„ ìœ„í•´ì„œëŠ” GEMINI_API_KEY ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.)"
        )

    def _run_auto_analysis(self):
        """íŒŒì´í”„ë¼ì¸ì„ ìë™ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¸ë±ì‹±í•©ë‹ˆë‹¤."""
        try:
            from engine.orchestrator import Orchestrator

            orchestrator = Orchestrator(config=self.config)
            orchestrator.run_pipeline(scenario="A")
            self.logger.info("âœ… ìë™ ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ ì¸ë±ì‹± ì¤‘...")
            self.index_knowledge()
        except Exception as e:
            self.logger.warning(f"ìë™ ë¶„ì„ ì‹¤íŒ¨: {e}")
