# -*- coding: utf-8 -*-
"""ReportCell â€” ì‹¤í—˜ ê²°ê³¼ ìë™ ë¦¬í¬íŒ… + LLM ìì—°ì–´ í•´ì„.

ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ Markdown í˜•ì‹ì˜ ë¦¬í¬íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
Phase 3 ê¸°ëŠ¥(Debate, Conformal, Benchmark) ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬
ì‹¬ë„ ìˆëŠ” ì¸ê³¼ ì¶”ë¡  ë¦¬í¬íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import datetime
import os
from typing import Any, Dict, List, Optional

import numpy as np

from engine.cells.base_cell import BaseCell
from engine.config import WhyLabConfig


class ReportCell(BaseCell):
    """ë¶„ì„ ê²°ê³¼ë¥¼ Markdown ë¦¬í¬íŠ¸ + AI ì¸ì‚¬ì´íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ì…€."""

    def __init__(self, config: WhyLabConfig) -> None:
        super().__init__(name="report_cell", config=config)

    def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self.logger.info("ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")

        # 1. ë°ì´í„° ì¶”ì¶œ (Phase 2 & 3)
        ate = inputs.get("ate", 0.0)
        ate_ci_lower = inputs.get("ate_ci_lower", 0.0)
        ate_ci_upper = inputs.get("ate_ci_upper", 0.0)
        cate_preds = inputs.get("cate_predictions", np.array([]))
        feature_names = inputs.get("feature_names", [])
        scenario = inputs.get("scenario_name", "Unknown Scenario")
        
        # Phase 3 Data
        debate = inputs.get("debate_summary", {})
        conformal = inputs.get("conformal_results", {})
        benchmark = inputs.get("benchmark_results", {})
        sensitivity = inputs.get("sensitivity_results", {})
        est_acc = inputs.get("estimation_accuracy", {})
        feat_imp = inputs.get("feature_importance", {})

        # Stats
        cate_mean = float(np.mean(cate_preds)) if len(cate_preds) > 0 else 0.0
        cate_std = float(np.std(cate_preds)) if len(cate_preds) > 0 else 0.0
        n_samples = len(inputs.get("dataframe", []))
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # 2. AI ì¸ì‚¬ì´íŠ¸ ìƒì„±
        ai_insights = self._generate_insights(
            scenario=scenario, ate=ate, ci=(ate_ci_lower, ate_ci_upper),
            debate=debate, est_acc=est_acc
        )

        # 3. Markdown ë¦¬í¬íŠ¸ ìƒì„±
        report_content = self._generate_markdown(
            timestamp=timestamp, scenario=scenario,
            ate=ate, ci=(ate_ci_lower, ate_ci_upper),
            cate_stats={"mean": cate_mean, "std": cate_std},
            features=feature_names, n_samples=n_samples,
            debate=debate, conformal=conformal, benchmark=benchmark,
            sensitivity=sensitivity, est_acc=est_acc, ai_insights=ai_insights
        )

        # 4. íŒŒì¼ ì €ì¥
        output_dir = self.config.paths.reports_dir
        output_dir.mkdir(parents=True, exist_ok=True)
        filename = f"whylab_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        file_path = output_dir / filename

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        self.logger.info("ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: %s", file_path)

        return {
            "report_path": str(file_path),
            "report_content": report_content,
            "ai_insights": ai_insights,
        }

    def _generate_insights(self, scenario, ate, ci, debate, est_acc) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„± (LLM ì—°ë™ì€ í•„ìš”ì‹œ ë³µêµ¬ ê°€ëŠ¥)."""
        is_sig = not (ci[0] <= 0 <= ci[1])
        verdict = debate.get("verdict", "UNKNOWN")
        
        summary = (
            f"**{scenario}** ë¶„ì„ ê²°ê³¼, ì²˜ì¹˜ íš¨ê³¼(ATE)ëŠ” {ate:.4f}ë¡œ ì¶”ì •ë˜ì—ˆìœ¼ë©°, "
            f"í†µê³„ì ìœ¼ë¡œ {'ìœ ì˜í•©ë‹ˆë‹¤' if is_sig else 'ìœ ì˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'}. "
            f"AI Debate ì‹œìŠ¤í…œì€ ì´ë¥¼ **{verdict}**ë¡œ íŒì •í–ˆìŠµë‹ˆë‹¤."
        )
        
        return {
            "summary": summary,
            "headline": f"ATE {ate:.4f} ({verdict})",
            "generated_by": "rule_based"
        }

    def _generate_markdown(
        self, timestamp, scenario, ate, ci, cate_stats, features, n_samples,
        debate, conformal, benchmark, sensitivity, est_acc, ai_insights
    ) -> str:
        """Markdown ë¦¬í¬íŠ¸ ë³¸ë¬¸ ìƒì„±."""
        
        # Helper for significance
        is_sig = not (ci[0] <= 0 <= ci[1])
        sig_icon = "âœ…" if is_sig else "âš ï¸"
        
        # Debate Section
        debate_section = ""
        if debate:
            d_verdict = debate.get("verdict", "UNKNOWN")
            d_conf = debate.get("confidence", 0) * 100
            d_rec = debate.get("recommendation", "-")
            d_rounds = debate.get("rounds", 0)
            
            debate_section = f"""
## 2. ğŸ›ï¸ Causal Debate Verdict
> **Verdict**: **{d_verdict}** (Confidence: {d_conf:.1f}%) in {d_rounds} rounds.

- **Recommendation**: {d_rec}
- **Score**: Pro {debate.get('pro_score', 0):.1f} vs Con {debate.get('con_score', 0):.1f}
"""

        # Diagnostics Section
        diag_section = ""
        if sensitivity:
            s_status = sensitivity.get("status", "Unknown")
            e_val = sensitivity.get("e_value", {}).get("point", 0) if isinstance(sensitivity.get("e_value"), dict) else 0
            ov_score = sensitivity.get("overlap", {}).get("overlap_score", 0) if isinstance(sensitivity.get("overlap"), dict) else 0
            
            diag_section = f"""
## 3. ğŸ›¡ï¸ Statistical Diagnostics (Robustness)
- **Overall Status**: {s_status}
- **E-value**: {e_val:.2f} (Target > 1.5)
- **Overlap Score**: {ov_score:.2f} (Target > 0.5)
- **Placebo Test**: {sensitivity.get('placebo_test', {}).get('status', 'N/A')}
- **Refutation**: {sensitivity.get('random_common_cause', {}).get('status', 'N/A')}
"""

        # Conformal Section
        conf_section = ""
        if conformal:
            cov = conformal.get("coverage", 0) * 100
            width = conformal.get("ci_upper_mean", 0) - conformal.get("ci_lower_mean", 0)
            conf_section = f"""
## 4. ğŸ“ Conformal Prediction (Reliability)
- **Target Coverage**: 95%
- **Actual Coverage**: **{cov:.1f}%** (Valid if near 95%)
- **Avg CI Width**: {width:.4f}
"""
        
        # Benchmark Section
        bench_section = ""
        if benchmark:
            # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ ê²°ê³¼ë§Œ ì˜ˆì‹œë¡œ
            ds_name = list(benchmark.keys())[0] if benchmark else "N/A"
            methods = benchmark.get(ds_name, {})
            rows = []
            for m_name, metrics in methods.items():
                rows.append(f"| {m_name} | {metrics['pehe_mean']:.4f} | {metrics['ate_bias_mean']:.4f} |")
            
            table_body = "\n".join(rows)
            bench_section = f"""
## 5. ğŸ† Benchmark Performance ({ds_name})
| Method | âˆšPEHE (Lower is better) | ATE Bias |
|--------|--------------------------|----------|
{table_body}
"""

        return f"""# ğŸ§ª WhyLab Comprehensive Analysis Report

**Date**: {timestamp}  
**Scenario**: {scenario}  
**Samples**: {n_samples:,}  

---

## 1. ğŸ“‹ Executive Summary
{sig_icon} **Treatment Effect (ATE)**: {ate:.4f} (95% CI: [{ci[0]:.4f}, {ci[1]:.4f}])  
{ai_insights['summary']}

> **Model Quality**: Correlation with Ground Truth = **{est_acc.get('correlation', 0):.3f}**

{debate_section}
{diag_section}
{conf_section}

## 6. Heterogeneity Analysis (CATE)
- **Mean CATE**: {cate_stats['mean']:.4f}
- **Std Dev**: {cate_stats['std']:.4f}

{bench_section}

---
*Generated by: WhyLab Causal Engine (Phase 3)*
"""
