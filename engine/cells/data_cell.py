# -*- coding: utf-8 -*-
"""DataCell â€” SCM ê¸°ë°˜ í•©ì„± ë°ì´í„° ìƒì„± + DuckDB SQL íƒìƒ‰.

WhyLab íŒŒì´í”„ë¼ì¸ì˜ ì²« ë²ˆì§¸ ì…€.
êµ¬ì¡°ì  ì¸ê³¼ ëª¨ë¸(SCM)ì— ê¸°ë°˜í•œ í˜„ì‹¤ì ì¸ í•€í…Œí¬ í•©ì„± ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì‹œë‚˜ë¦¬ì˜¤ ì„¤ëª…:
    A. ì‹ ìš© í•œë„(T) â†’ ì—°ì²´ ì—¬ë¶€(Y) (Existing)
    B. íˆ¬ì ì¿ í°(T) â†’ íˆ¬ì ê°€ì… ì—¬ë¶€(Y) (New - HTE ë¶„ì„ìš©)

DuckDB í™œìš©:
    - ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¡œë“œ ë° Window Function ê¸°ë°˜ ì‹œê³„ì—´ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰.
    - AVG(spend) OVER ... ë“±ì˜ ì¿¼ë¦¬ ì‹¤í–‰.

ì¶œë ¥ í‚¤:
    - "dataframe": pd.DataFrame (í†µí•© ë°ì´í„°)
    - "feature_names": list[str]
    - "treatment_col": str
    - "outcome_col": str
    - "true_cate_col": str
    - "dag_edges": list[tuple]
    - "scenario": str ("A" or "B")
"""

from __future__ import annotations

from typing import Any, Dict, List, Tuple

import duckdb
import numpy as np
import pandas as pd

from engine.cells.base_cell import BaseCell
from engine.config import WhyLabConfig


def _normalize(arr: np.ndarray) -> np.ndarray:
    std = arr.std()
    if std < 1e-8:
        return arr - arr.mean()
    return (arr - arr.mean()) / std


def _sigmoid(x: np.ndarray) -> np.ndarray:
    return np.where(
        x >= 0,
        1.0 / (1.0 + np.exp(-x)),
        np.exp(x) / (1.0 + np.exp(x)),
    )


class DataCell(BaseCell):
    """SCM ê¸°ë°˜ í•©ì„± í•€í…Œí¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  DuckDBë¡œ ì „ì²˜ë¦¬í•˜ëŠ” ì…€."""

    def __init__(self, config: WhyLabConfig) -> None:
        super().__init__(name="data_cell", config=config)

    def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ìƒì„± ë° ì „ì²˜ë¦¬ ì‹¤í–‰.

        Args:
            inputs:
                - scenario (str, optional): "A"(ì‹ ìš©) ë˜ëŠ” "B"(ë§ˆì¼€íŒ…). ê¸°ë³¸ê°’ "A".

        Returns:
            ìƒì„±ëœ ë°ì´í„°ì…‹ ë° ë©”íƒ€ë°ì´í„°.
        """
        scenario = inputs.get("scenario", "A")
        
        # 0. ì™¸ë¶€ ë°ì´í„° ë¡œë“œ (CLI ì¸ì ìš°ì„ )
        if self.config.data.input_path:
            self.logger.info("ğŸ“‚ ì™¸ë¶€ ë°ì´í„° ë¡œë“œ: %s", self.config.data.input_path)
            df_final, meta = self._load_external_data()
            # ì™¸ë¶€ ë°ì´í„°ëŠ” ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ì˜¤ë²„ë¼ì´ë“œ
            scenario = "External Data"
        else:
            self.logger.info("ì‹œë‚˜ë¦¬ì˜¤ %s ë°ì´í„° ìƒì„± ì‹œì‘", scenario)

            # 1. ê¸°ë³¸ ì™¸ìƒ ë³€ìˆ˜ ìƒì„±
            df_base = self._generate_base_features()
    
            # 2. ì‹œë‚˜ë¦¬ì˜¤ë³„ ì²˜ë¦¬
            if scenario == "A":
                df_final, meta = self._generate_scenario_A(df_base)
            elif scenario == "B":
                df_final, meta = self._generate_scenario_B(df_base)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹œë‚˜ë¦¬ì˜¤: {scenario}")

        # 3. DuckDB ì „ì²˜ë¦¬ (Window Function ë“±)
        # ì™¸ë¶€ ë°ì´í„°ë¼ë„ DuckDB ì „ì²˜ë¦¬ë¥¼ í†µê³¼ì‹œì¼œ ì¼ê´€ì„± ìœ ì§€ (ì„ íƒì )
        try:
            df_processed = self._apply_duckdb_preprocessing(df_final)
        except Exception as e:
            self.logger.warning("DuckDB ì „ì²˜ë¦¬ ì‹¤íŒ¨ (ì™¸ë¶€ ë°ì´í„° êµ¬ì¡° ë¶ˆì¼ì¹˜?): %s", e)
            df_processed = df_final

        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        meta["dataframe"] = df_processed
        
        # DuckDBë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ feature_namesì— ì¶”ê°€
        if "feature_names" in meta:
            new_cols = set(df_processed.columns) - set(df_final.columns)
            if new_cols:
                meta["feature_names"] = list(set(meta["feature_names"]) | new_cols)
        
        meta["scenario"] = scenario

        self.logger.info(
            "ìµœì¢… ë°ì´í„°: shape=%s, ì»¬ëŸ¼=%s",
            df_processed.shape, list(df_processed.columns)
        )

        return meta

    def _load_external_data(self) -> Tuple[pd.DataFrame, Dict]:
        """ì„¤ì •ëœ ê²½ë¡œ/URIì—ì„œ ì™¸ë¶€ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        ì»¤ë„¥í„° íŒ©í† ë¦¬ë¥¼ í†µí•´ CSV/Parquet/SQL/BigQuery ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
        """
        cfg = self.config.data
        input_path = cfg.input_path
        
        # URI íŒ¨í„´ìœ¼ë¡œ ì†ŒìŠ¤ íƒ€ì… ìë™ ê°ì§€
        source_type = self._detect_source_type(input_path)
        
        try:
            from engine.connectors import create_connector, ConnectorConfig
            
            connector_config = ConnectorConfig(
                source_type=source_type,
                uri=input_path,
                query=getattr(cfg, 'query', None),
                table=getattr(cfg, 'table', None),
                treatment_col=cfg.treatment_col,
                outcome_col=cfg.outcome_col,
                feature_cols=cfg.feature_cols or [],
            )
            
            connector = create_connector(connector_config)
            with connector:
                result = connector.fetch_with_meta()
            
            df = result.pop("dataframe")
            self.logger.info(
                "%s ì»¤ë„¥í„° ë¡œë“œ ì„±ê³µ: %d rows", source_type.upper(), len(df)
            )
            return df, result
            
        except ImportError:
            # ì»¤ë„¥í„° ëª¨ë“ˆ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§ í´ë°±
            self.logger.warning("ì»¤ë„¥í„° ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ CSV ë¡œë”ë¡œ í´ë°±.")
            return self._load_csv_fallback()
    
    @staticmethod
    def _detect_source_type(path: str) -> str:
        """ê²½ë¡œ/URIë¡œë¶€í„° ë°ì´í„° ì†ŒìŠ¤ íƒ€ì…ì„ ìë™ ê°ì§€í•©ë‹ˆë‹¤."""
        path_lower = path.lower()
        if path_lower.startswith(("postgresql://", "postgres://")):
            return "postgresql"
        elif path_lower.startswith("mysql"):
            return "mysql"
        elif path_lower.startswith("sqlite"):
            return "sqlite"
        elif path_lower.endswith(".parquet"):
            return "parquet"
        elif path_lower.endswith((".xlsx", ".xls")):
            return "excel"
        elif path_lower.endswith(".tsv"):
            return "tsv"
        elif "bigquery" in path_lower or path_lower.startswith("bq://"):
            return "bigquery"
        return "csv"
    
    def _load_csv_fallback(self) -> Tuple[pd.DataFrame, Dict]:
        """ì»¤ë„¥í„° ì—†ì´ ê¸°ì¡´ CSV ë¡œë“œ ë¡œì§ (í´ë°±)."""
        cfg = self.config.data
        try:
            df = pd.read_csv(cfg.input_path)
        except Exception as e:
            raise RuntimeError(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {cfg.input_path}") from e
        
        self.logger.info("CSV ë¡œë“œ ì„±ê³µ: %d rows", len(df))
        
        if cfg.treatment_col not in df.columns:
            raise ValueError(f"ì²˜ì¹˜ ë³€ìˆ˜ '{cfg.treatment_col}'ì´(ê°€) ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        if cfg.outcome_col not in df.columns:
            raise ValueError(f"ê²°ê³¼ ë³€ìˆ˜ '{cfg.outcome_col}'ì´(ê°€) ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        
        features = cfg.feature_cols
        if not features:
            exclude = {cfg.treatment_col, cfg.outcome_col, "user_id", "id", "index", "Unnamed: 0"}
            features = [
                c for c in df.columns
                if c not in exclude and pd.api.types.is_numeric_dtype(df[c])
            ]
            self.logger.info("í”¼ì²˜ ìë™ ì¶”ë¡ : %s", features)
        
        return df, {
            "treatment_col": cfg.treatment_col,
            "outcome_col": cfg.outcome_col,
            "feature_names": features,
            "dag_edges": [],
            "true_cate_col": None,
        }
    def _generate_base_features(self) -> pd.DataFrame:
        cfg = self.config.data
        rng = np.random.default_rng(cfg.random_seed)
        n = cfg.n_samples

        income = rng.lognormal(
            mean=cfg.income_log_mean, sigma=cfg.income_log_sigma, size=n
        )
        age = rng.normal(loc=cfg.age_mean, scale=cfg.age_std, size=n).clip(
            cfg.age_min, cfg.age_max
        )
        credit_score = rng.normal(
            loc=cfg.credit_score_mean, scale=cfg.credit_score_std, size=n
        ).clip(cfg.credit_score_min, cfg.credit_score_max)
        app_usage = rng.exponential(scale=cfg.app_usage_scale, size=n).clip(
            cfg.app_usage_min, cfg.app_usage_max
        )
        # ì†Œë¹„ëŠ” ì†Œë“ì— ë¹„ë¡€ + ë…¸ì´ì¦ˆ
        consumption = (
            cfg.consumption_income_coef * income + rng.normal(0, 500, n)
        ).clip(0, None)

        # ê°€ìƒì˜ ì‹œê³„ì—´ ìƒì„±ì„ ìœ„í•œ 'month' ì»¬ëŸ¼ (DuckDB ì‹¤ìŠµìš©)
        # ê° ìœ ì €ë³„ë¡œ ë‹¨ì¼ ë ˆì½”ë“œì§€ë§Œ, ê³¼ê±° ì´ë ¥ì„ ê°€ì •í•˜ì—¬ ìƒì„±
        # ì‹¤ì œë¡œëŠ” ìœ ì €ë³„ ë‹¤ê±´ì´ í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ê°„ì†Œí™”
        months = np.random.randint(1, 13, size=n)

        return pd.DataFrame({
            "user_id": np.arange(n),
            "month": months,
            "income": income,
            "age": age,
            "credit_score": credit_score,
            "app_usage_time": app_usage,
            "consumption": consumption,
        })

    def _generate_scenario_A(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """ì‹œë‚˜ë¦¬ì˜¤ A: ì‹ ìš© í•œë„(Continuous) â†’ ì—°ì²´ ì—¬ë¶€(Binary)."""
        cfg = self.config.data
        rng = np.random.default_rng(cfg.random_seed)
        n = len(df)

        income_z = _normalize(df["income"].values)
        age_z = _normalize(df["age"].values)
        credit_z = _normalize(df["credit_score"].values)
        consumption_z = _normalize(df["consumption"].values)

        # Treatment: ì‹ ìš© í•œë„ (êµë€: ì†Œë“/ì‹ ìš© ë†’ìœ¼ë©´ í•œë„ ë†’ìŒ)
        treat_logit = (
            cfg.treat_income_coef * income_z
            + cfg.treat_age_coef * age_z
            + cfg.treat_credit_coef * credit_z
            + rng.normal(0, cfg.treat_noise_std, n)
        )
        credit_limit = (
            cfg.treat_min
            + (cfg.treat_max - cfg.treat_min) * _sigmoid(treat_logit)
        )
        credit_limit_z = _normalize(credit_limit)

        # CATE (Ground Truth)
        true_cate = (
            cfg.cate_income_coef * income_z
            + cfg.cate_age_coef * age_z
            + cfg.cate_credit_coef * credit_z
        )

        # Outcome: ì—°ì²´ ì—¬ë¶€
        # Y = T * CATE + g(X) + noise
        outcome_logit = (
            true_cate * credit_limit_z
            + cfg.outcome_income_coef * income_z
            + cfg.outcome_consumption_coef * consumption_z
            + cfg.outcome_credit_coef * credit_z
            + rng.normal(0, cfg.outcome_noise_std, n)
        )
        default_prob = _sigmoid(outcome_logit)
        is_default = default_prob  # DMLì€ ì—°ì†í˜• outcomeì—ì„œ ìµœì  ë™ì‘ â†’ í™•ë¥  ìœ ì§€

        df["credit_limit"] = credit_limit
        df["is_default"] = is_default            # ì—°ì²´ í™•ë¥  (0~1 ì—°ì†)
        df["is_default_binary"] = (default_prob > 0.5).astype(int)  # ì´ì§„ ì°¸ì¡°ìš©
        df["true_cate"] = true_cate
        df["default_prob"] = default_prob

        feature_names = ["income", "age", "credit_score", "app_usage_time", "consumption"]
        dag_edges = [
            ("income", "credit_limit"), ("income", "is_default"),
            ("age", "credit_limit"), ("age", "is_default"),
            ("credit_score", "credit_limit"), ("credit_score", "is_default"),
            ("credit_limit", "is_default"),
        ]

        return df, {
            "treatment_col": "credit_limit",
            "outcome_col": "is_default",
            "true_cate_col": "true_cate",
            "feature_names": feature_names,
            "dag_edges": dag_edges,
        }

    def _generate_scenario_B(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """ì‹œë‚˜ë¦¬ì˜¤ B: íˆ¬ì ì¿ í°(Binary) â†’ ê°€ì… ì—¬ë¶€(Binary)."""
        cfg = self.config.data
        rng = np.random.default_rng(cfg.random_seed)
        n = len(df)

        income_z = _normalize(df["income"].values)
        age_z = _normalize(df["age"].values)
        app_z = _normalize(df["app_usage_time"].values)

        # Treatment: ì¿ í° ì§€ê¸‰ ì—¬ë¶€ (ë¬´ì‘ìœ„ê°€ ì•„ë‹ˆë¼ íƒ€ê²ŸíŒ… ê°€ì •)
        # í™œë™ì„±(app_usage) ë†’ì€ ìœ ì €ì—ê²Œ ë” ë§ì´ ì§€ê¸‰
        treat_prob = _sigmoid(0.5 * app_z - 0.2 * age_z)
        coupon_sent = rng.binomial(1, treat_prob)

        # CATE (Ground Truth) - HTE
        # 20ëŒ€(age â†“), ì €ì†Œë“(income â†“)ì¼ìˆ˜ë¡ íš¨ê³¼ í¼
        true_cate_logit = (
            cfg.cate_b_base
            + cfg.cate_b_income_coef * income_z
            + cfg.cate_b_age_coef * age_z
        )
        true_cate_prob = _sigmoid(true_cate_logit) - 0.5 # í™•ë¥  ì¦ê°€ë¶„ (ì•½ì‹)
        # ì‹¤ì œ íš¨ê³¼ëŠ” í™•ë¥  ìŠ¤ì¼€ì¼ì—ì„œ ì‘ë™
        
        # Outcome: ê°€ì… ì—¬ë¶€
        # Base Prob + T * CATE + Noise
        base_logit = -2.0 + 0.5 * income_z + 0.3 * app_z  # ê¸°ë³¸ì ìœ¼ë¡œ ëˆ ë§ê³  í™œë™ì„± ë†’ìœ¼ë©´ ê°€ì…
        outcome_prob = _sigmoid(base_logit + coupon_sent * true_cate_logit)
        is_joined = rng.binomial(1, outcome_prob)

        df["coupon_sent"] = coupon_sent
        df["is_joined"] = is_joined
        df["true_cate"] = true_cate_prob # ê·¼ì‚¬ì¹˜

        feature_names = ["income", "age", "credit_score", "app_usage_time"]
        dag_edges = [
            ("app_usage_time", "coupon_sent"),
            ("age", "coupon_sent"),
            ("coupon_sent", "is_joined"),
            ("income", "is_joined"),
            ("app_usage_time", "is_joined"),
        ]

        return df, {
            "treatment_col": "coupon_sent",
            "outcome_col": "is_joined",
            "true_cate_col": "true_cate",
            "feature_names": feature_names,
            "dag_edges": dag_edges,
        }

    def _apply_duckdb_preprocessing(self, df: pd.DataFrame) -> pd.DataFrame:
        """DuckDB Window Functionì„ í™œìš©í•œ ì „ì²˜ë¦¬.

        ë³´ê³ ì„œ ê°€ì´ë“œì— ë”°ë¼ SQLì„ ì‚¬ìš©í•˜ì—¬ íŒŒìƒ ë³€ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        (ì‹¤ì œë¡œëŠ” 1ì¸ë‹¹ 1í–‰ì´ì§€ë§Œ, ìœˆë„ìš° í•¨ìˆ˜ ë¬¸ë²• ë°ëª¨ë¥¼ ìœ„í•´ ê°€ìƒ ì ìš©)
        """
        self.logger.info("DuckDB ì „ì²˜ë¦¬ ì‹œì‘")
        
        con = duckdb.connect()
        con.register("raw_data", df)

        # ìœˆë„ìš° í•¨ìˆ˜ ë°ëª¨ (ì—¬ê¸°ì„œëŠ” idìˆœ ì •ë ¬ì„ ì‹œê³„ì—´ë¡œ ê°€ì •)
        query = """
        SELECT 
            *,
            -- 3ê°œì›” í‰ê·  ì†Œë¹„ (ê°€ìƒ: í˜„ì¬ í–‰ í¬í•¨ ì „í›„ ë§¥ë½)
            AVG(consumption) OVER (
                ORDER BY income -- ì„ì˜ ì •ë ¬
                ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING
            ) AS avg_spend_3m,
            
            -- ìµœê·¼ ì‹ ìš©ì ìˆ˜ ìµœëŒ€ê°’
            MAX(credit_score) OVER (
                PARTITION BY age_group_10
            ) AS max_credit_score_6m
        FROM (
             SELECT *, FLOOR(age / 10) * 10 AS age_group_10 FROM raw_data
        )
        """
        
        df_processed = con.execute(query).df()
        con.close()
        
        # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
        if "age_group_10" in df_processed.columns:
            df_processed = df_processed.drop(columns=["age_group_10"])

        self.logger.info("DuckDB ì „ì²˜ë¦¬ ì™„ë£Œ")
        return df_processed
