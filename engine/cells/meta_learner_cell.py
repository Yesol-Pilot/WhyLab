# -*- coding: utf-8 -*-
"""MetaLearnerCell â€” 5ì¢… ë©”íƒ€ëŸ¬ë„ˆ + Oracle ì•™ìƒë¸” ì„ íƒ.

ë‹¨ì¼ LinearDML ë˜í¼ë¥¼ ë„˜ì–´, 5ê°€ì§€ ë©”íƒ€ëŸ¬ë„ˆ(S/T/X/DR/R)ë¥¼ ì§ì ‘ êµ¬í˜„í•˜ê³ 
Cross-Validated MSE ê¸°ë°˜ìœ¼ë¡œ ìµœì  ë©”íƒ€ëŸ¬ë„ˆë¥¼ ìë™ ì„ íƒí•©ë‹ˆë‹¤.

ì°¸ê³ ë¬¸í—Œ:
  - KÃ¼nzel et al. (2019) "Metalearners for estimating HTE"
  - Kennedy (2023) "Towards optimal doubly robust estimation"
  - Nie & Wager (2021) "Quasi-oracle estimation of HTE"
"""

from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold

from engine.cells.base_cell import BaseCell
from engine.config import WhyLabConfig


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê°œë³„ ë©”íƒ€ëŸ¬ë„ˆ êµ¬í˜„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class _BaseMetaLearner:
    """ë©”íƒ€ëŸ¬ë„ˆ ê³µí†µ ì¸í„°í˜ì´ìŠ¤."""

    name: str = "base"

    def __init__(self, base_model_factory=None, config=None):
        """base_model_factory: sklearn í˜¸í™˜ ëª¨ë¸ ìƒì„± í•¨ìˆ˜."""
        self._config = config
        self._factory = base_model_factory or self._default_factory

    def _default_factory(self):
        if self._config is not None:
            from engine.gpu_factory import create_lgbm_regressor
            return create_lgbm_regressor(self._config)
        from lightgbm import LGBMRegressor
        return LGBMRegressor(
            n_estimators=200, max_depth=5, num_leaves=31,
            learning_rate=0.05, verbose=-1,
        )

    def fit(self, X: np.ndarray, T: np.ndarray, Y: np.ndarray) -> "_BaseMetaLearner":
        raise NotImplementedError

    def predict_cate(self, X: np.ndarray) -> np.ndarray:
        raise NotImplementedError


class SLearner(_BaseMetaLearner):
    """S-Learner: Të¥¼ í”¼ì²˜ì— í¬í•¨í•œ ë‹¨ì¼ ëª¨ë¸.

    Î¼Ì‚(x, t) = f(X, T)
    Ï„Ì‚(x) = Î¼Ì‚(x, 1) - Î¼Ì‚(x, 0)
    """

    name = "S-Learner"

    def fit(self, X, T, Y):
        self.model_ = self._factory()
        XT = np.column_stack([X, T.reshape(-1, 1)])
        self.model_.fit(XT, Y)
        return self

    def predict_cate(self, X):
        n = X.shape[0]
        X1 = np.column_stack([X, np.ones((n, 1))])
        X0 = np.column_stack([X, np.zeros((n, 1))])
        return self.model_.predict(X1) - self.model_.predict(X0)


class TLearner(_BaseMetaLearner):
    """T-Learner: ì²˜ì¹˜/í†µì œ ë¶„ë¦¬ ëª¨ë¸.

    Î¼Ì‚â‚€(x) = E[Y | X=x, T=0]
    Î¼Ì‚â‚(x) = E[Y | X=x, T=1]
    Ï„Ì‚(x) = Î¼Ì‚â‚(x) - Î¼Ì‚â‚€(x)
    """

    name = "T-Learner"

    def fit(self, X, T, Y):
        # ì´ì§„í™”: ì¤‘ì•™ê°’ ê¸°ì¤€ ë¶„ë¦¬
        self.threshold_ = np.median(T)
        mask1 = T >= self.threshold_
        mask0 = ~mask1

        self.model_1_ = self._factory()
        self.model_0_ = self._factory()
        self.model_1_.fit(X[mask1], Y[mask1])
        self.model_0_.fit(X[mask0], Y[mask0])
        return self

    def predict_cate(self, X):
        return self.model_1_.predict(X) - self.model_0_.predict(X)


class XLearner(_BaseMetaLearner):
    """X-Learner (KÃ¼nzel et al., 2019): ìœ ì‚¬ì”ì°¨ + PS ê°€ì¤‘.

    Step 1: T-Learner í•™ìŠµ
    Step 2: ìœ ì‚¬ì”ì°¨ DÌ‚â‚ = Y - Î¼Ì‚â‚€(X), DÌ‚â‚€ = Î¼Ì‚â‚(X) - Y
    Step 3: CATE í•™ìŠµ Ï„Ì‚â‚(x), Ï„Ì‚â‚€(x)
    Step 4: Ï„Ì‚(x) = g(x)Â·Ï„Ì‚â‚€(x) + (1-g(x))Â·Ï„Ì‚â‚(x)
    """

    name = "X-Learner"

    def fit(self, X, T, Y):
        self.threshold_ = np.median(T)
        mask1 = T >= self.threshold_
        mask0 = ~mask1

        # Step 1: T-Learner
        mu1 = self._factory()
        mu0 = self._factory()
        mu1.fit(X[mask1], Y[mask1])
        mu0.fit(X[mask0], Y[mask0])

        # Step 2: ìœ ì‚¬ì”ì°¨ (Imputed Treatment Effects)
        D1 = Y[mask1] - mu0.predict(X[mask1])  # ì²˜ì¹˜êµ°: ê´€ì¸¡ - ë°˜ì‚¬ì‹¤
        D0 = mu1.predict(X[mask0]) - Y[mask0]  # í†µì œêµ°: ë°˜ì‚¬ì‹¤ - ê´€ì¸¡

        # Step 3: CATE í•™ìŠµ
        self.tau1_ = self._factory()
        self.tau0_ = self._factory()
        self.tau1_.fit(X[mask1], D1)
        self.tau0_.fit(X[mask0], D0)

        # Step 4: Propensity (ì²˜ì¹˜ í™•ë¥ ) â€” ê°€ì¤‘ì¹˜
        from sklearn.linear_model import LogisticRegression
        ps_model = LogisticRegression(max_iter=1000, random_state=42)
        T_binary = (T >= self.threshold_).astype(int)
        ps_model.fit(X, T_binary)
        self.ps_model_ = ps_model

        return self

    def predict_cate(self, X):
        g = self.ps_model_.predict_proba(X)[:, 1]  # P(T=1|X)
        tau1 = self.tau1_.predict(X)
        tau0 = self.tau0_.predict(X)
        return g * tau0 + (1 - g) * tau1


class DRLearner(_BaseMetaLearner):
    """DR-Learner (Kennedy, 2023): Doubly Robust CATE ì¶”ì •.

    Î“Ì‚áµ¢ = Î¼Ì‚â‚(Xáµ¢) - Î¼Ì‚â‚€(Xáµ¢)
        + Táµ¢Â·(Yáµ¢ - Î¼Ì‚â‚(Xáµ¢))/Ãª(Xáµ¢)
        - (1-Táµ¢)Â·(Yáµ¢ - Î¼Ì‚â‚€(Xáµ¢))/(1-Ãª(Xáµ¢))

    Ï„Ì‚(x) = E[Î“Ì‚ | X=x]  (2ë‹¨ê³„ íšŒê·€)
    """

    name = "DR-Learner"

    def fit(self, X, T, Y):
        self.threshold_ = np.median(T)
        T_binary = (T >= self.threshold_).astype(int)
        mask1 = T_binary == 1
        mask0 = T_binary == 0

        # Outcome ëª¨ë¸ (ì²˜ì¹˜ë³„)
        mu1 = self._factory()
        mu0 = self._factory()
        mu1.fit(X[mask1], Y[mask1])
        mu0.fit(X[mask0], Y[mask0])

        # Propensity Score
        from sklearn.linear_model import LogisticRegression
        ps = LogisticRegression(max_iter=1000, random_state=42)
        ps.fit(X, T_binary)
        e_hat = np.clip(ps.predict_proba(X)[:, 1], 0.01, 0.99)

        # Doubly Robust Score êµ¬ì„±
        mu1_pred = mu1.predict(X)
        mu0_pred = mu0.predict(X)
        gamma = (mu1_pred - mu0_pred
                 + T_binary * (Y - mu1_pred) / e_hat
                 - (1 - T_binary) * (Y - mu0_pred) / (1 - e_hat))

        # 2ë‹¨ê³„: Î“Ì‚ë¥¼ Xì— íšŒê·€
        self.final_model_ = self._factory()
        self.final_model_.fit(X, gamma)
        return self

    def predict_cate(self, X):
        return self.final_model_.predict(X)


class RLearner(_BaseMetaLearner):
    """R-Learner (Nie & Wager, 2021): Robinson Decomposition.

    á»¸ = Y - mÌ‚(X)     (outcome ì”ì°¨)
    TÌƒ = T - Ãª(X)      (treatment ì”ì°¨)
    Ï„Ì‚ = argmin_Ï„ Î£ [(á»¸ - Ï„(X)Â·TÌƒ)Â² + Î»Â·||Ï„||Â²]
    """

    name = "R-Learner"

    def fit(self, X, T, Y):
        # mÌ‚(X) = E[Y|X] (marginal outcome model)
        m_model = self._factory()
        m_model.fit(X, Y)
        Y_tilde = Y - m_model.predict(X)

        # Ãª(X) = E[T|X] (marginal treatment model)
        e_model = self._factory()
        e_model.fit(X, T)
        T_tilde = T - e_model.predict(X)

        # TÌƒÂ² ê°€ì¤‘ ì”ì°¨ íšŒê·€: min Î£ (á»¸/TÌƒ - Ï„(X))Â² Â· TÌƒÂ²
        # ì•ˆì „ ê°€ë“œ: |TÌƒ| ì´ ë§¤ìš° ì‘ìœ¼ë©´ ë¶ˆì•ˆì • â†’ í´ë¦¬í•‘
        eps = 0.01
        T_tilde_safe = np.where(np.abs(T_tilde) < eps, eps * np.sign(T_tilde + 1e-10), T_tilde)
        pseudo_outcome = Y_tilde / T_tilde_safe

        # ê°€ì¤‘ì¹˜: TÌƒÂ²
        weights = T_tilde ** 2

        self.final_model_ = self._factory()
        self.final_model_.fit(X, pseudo_outcome, sample_weight=weights)
        return self

    def predict_cate(self, X):
        return self.final_model_.predict(X)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”íƒ€ëŸ¬ë„ˆ ì…€ (í†µí•©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class MetaLearnerCell(BaseCell):
    """5ì¢… ë©”íƒ€ëŸ¬ë„ˆ + Oracle ì„ íƒ ì…€.

    ëª¨ë“  ë©”íƒ€ëŸ¬ë„ˆë¥¼ í•™ìŠµí•˜ê³ , Cross-Validated MSE ê¸°ë°˜ìœ¼ë¡œ
    ìµœì  ë©”íƒ€ëŸ¬ë„ˆë¥¼ ìë™ ì„ íƒí•˜ê±°ë‚˜ ì•™ìƒë¸”í•©ë‹ˆë‹¤.
    """

    LEARNER_REGISTRY = {
        "S-Learner": SLearner,
        "T-Learner": TLearner,
        "X-Learner": XLearner,
        "DR-Learner": DRLearner,
        "R-Learner": RLearner,
    }

    def __init__(self, config: WhyLabConfig) -> None:
        super().__init__(name="meta_learner_cell", config=config)

    def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """5ì¢… ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ â†’ Oracle ì„ íƒ â†’ ì•™ìƒë¸” CATE.

        Args:
            inputs: CausalCell ì¶œë ¥ (dataframe, feature_names, treatment_col, outcome_col).

        Returns:
            ê°œë³„ ë©”íƒ€ëŸ¬ë„ˆ ê²°ê³¼ + Oracle ì„ íƒ + ì•™ìƒë¸” CATE.
        """
        self.validate_inputs(
            inputs, ["dataframe", "feature_names", "treatment_col", "outcome_col"],
        )

        df = inputs["dataframe"]
        X_cols = inputs["feature_names"]
        T_col = inputs["treatment_col"]
        Y_col = inputs["outcome_col"]

        X = df[X_cols].values.astype(np.float64)
        T = df[T_col].values.astype(np.float64)
        Y = df[Y_col].values.astype(np.float64)

        cfg = self.config.dml

        self.logger.info("ğŸ§¬ ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ ì‹œì‘ (5ì¢…)")

        # â”€â”€ 1. ê° ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ + CV-MSE í‰ê°€ â”€â”€
        learner_results: Dict[str, Dict] = {}

        for name, LearnerClass in self.LEARNER_REGISTRY.items():
            self.logger.info("   â–¶ %s í•™ìŠµ ì¤‘...", name)
            try:
                result = self._train_and_evaluate(
                    LearnerClass, X, T, Y, cv_folds=cfg.cv_folds,
                )
                learner_results[name] = result
                self.logger.info(
                    "     %s: ATE=%.5f, CV-MSE=%.6f",
                    name, result["ate"], result["cv_mse"],
                )
            except Exception as e:
                self.logger.warning("     %s ì‹¤íŒ¨: %s", name, e)
                learner_results[name] = {
                    "ate": 0.0, "cate": np.zeros(len(X)),
                    "cv_mse": float("inf"), "error": str(e),
                }

        # â”€â”€ 2. Oracle ì„ íƒ: CV-MSE ìµœì†Œ â”€â”€
        valid = {k: v for k, v in learner_results.items() if v["cv_mse"] < float("inf")}
        if not valid:
            self.logger.error("ëª¨ë“  ë©”íƒ€ëŸ¬ë„ˆ ì‹¤íŒ¨!")
            return {**inputs, "meta_learner_results": {}}

        best_name = min(valid, key=lambda k: valid[k]["cv_mse"])
        best_result = valid[best_name]

        # â”€â”€ 3. ì•™ìƒë¸” (MSE ì—­ìˆ˜ ê°€ì¤‘ í‰ê· ) â”€â”€
        mse_values = np.array([v["cv_mse"] for v in valid.values()])
        # Softmax of negative MSE â†’ ë‚®ì€ MSEì— ë†’ì€ ê°€ì¤‘ì¹˜
        weights = np.exp(-mse_values / (mse_values.mean() + 1e-10))
        weights = weights / weights.sum()

        cate_stack = np.column_stack([v["cate"] for v in valid.values()])
        ensemble_cate = (cate_stack * weights[np.newaxis, :]).sum(axis=1)
        ensemble_ate = float(np.mean(ensemble_cate))

        # í•©ì˜ìœ¨: ATE ë¶€í˜¸ê°€ ê°™ì€ ë©”íƒ€ëŸ¬ë„ˆ ë¹„ìœ¨
        signs = [np.sign(v["ate"]) for v in valid.values()]
        majority_sign = np.sign(ensemble_ate) if ensemble_ate != 0 else 1
        consensus = sum(1 for s in signs if s == majority_sign) / len(signs)

        self.logger.info(
            "ğŸ† Oracle: %s (CV-MSE=%.6f), ì•™ìƒë¸” ATE=%.5f, í•©ì˜ìœ¨=%.0f%%",
            best_name, best_result["cv_mse"], ensemble_ate, consensus * 100,
        )

        meta_results = {
            "learners": {
                name: {
                    "ate": float(r["ate"]),
                    "cv_mse": float(r["cv_mse"]),
                    "cate_mean": float(np.mean(r["cate"])),
                    "cate_std": float(np.std(r["cate"])),
                }
                for name, r in learner_results.items()
            },
            "oracle": {
                "best_learner": best_name,
                "cv_mse": float(best_result["cv_mse"]),
                "ate": float(best_result["ate"]),
            },
            "ensemble": {
                "ate": ensemble_ate,
                "weights": {n: float(w) for n, w in zip(valid.keys(), weights)},
                "consensus": consensus,
            },
        }

        return {
            **inputs,
            "meta_learner_results": meta_results,
            "ensemble_cate": ensemble_cate,
            "ensemble_ate": ensemble_ate,
            "best_learner": best_name,
        }

    def _train_and_evaluate(
        self,
        LearnerClass: type,
        X: np.ndarray,
        T: np.ndarray,
        Y: np.ndarray,
        cv_folds: int = 5,
    ) -> Dict[str, Any]:
        """ë©”íƒ€ëŸ¬ë„ˆë¥¼ í•™ìŠµí•˜ê³  CV-MSEë¡œ í‰ê°€í•©ë‹ˆë‹¤.

        CV ë°©ì‹:
          - K-Foldë¡œ ë¶„í• 
          - ê° foldì—ì„œ í•™ìŠµ â†’ ê²€ì¦ ë°ì´í„°ì˜ CATE ì˜ˆì¸¡
          - T-transformation MSE: (á»¸ - Ï„Ì‚(X)Â·TÌƒ)Â²
            (Ground truth ì—†ì´ë„ í‰ê°€ ê°€ëŠ¥í•œ R-Risk)
        """
        kf = KFold(n_splits=min(cv_folds, len(X)), shuffle=True, random_state=42)
        oos_mse = []

        for train_idx, val_idx in kf.split(X):
            X_tr, T_tr, Y_tr = X[train_idx], T[train_idx], Y[train_idx]
            X_val, T_val, Y_val = X[val_idx], T[val_idx], Y[val_idx]

            learner = LearnerClass(config=self.config)
            learner.fit(X_tr, T_tr, Y_tr)
            cate_val = learner.predict_cate(X_val)

            # R-Risk: (Y - mÌ‚(X) - Ï„Ì‚(X)Â·(T - Ãª(X)))Â²
            # mÌ‚(X)ì™€ Ãª(X)ë¥¼ ê°„ì´ í•™ìŠµ (GPU ê°€ì†)
            from engine.gpu_factory import create_lgbm_regressor
            m = create_lgbm_regressor(self.config, lightweight=True)
            e = create_lgbm_regressor(self.config, lightweight=True)
            m.fit(X_tr, Y_tr)
            e.fit(X_tr, T_tr)

            m_val = m.predict(X_val)
            e_val = e.predict(X_val)

            Y_tilde = Y_val - m_val
            T_tilde = T_val - e_val
            residual = Y_tilde - cate_val * T_tilde
            mse = float(np.mean(residual ** 2))
            oos_mse.append(mse)

        # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
        final_learner = LearnerClass(config=self.config)
        final_learner.fit(X, T, Y)
        cate = final_learner.predict_cate(X)
        ate = float(np.mean(cate))

        return {
            "ate": ate,
            "cate": cate,
            "cv_mse": float(np.mean(oos_mse)),
        }
