# -*- coding: utf-8 -*-
"""ConformalCell ‚Äî Î∂ÑÌè¨Î¨¥Í∞ÄÏ†ï CATE ÏòàÏ∏°Íµ¨Í∞Ñ.

Ï†ïÍ∑úÎ∂ÑÌè¨ Í∞ÄÏ†ï ÏóÜÏù¥, Split Conformal PredictionÏúºÎ°ú
Í∞úÎ≥Ñ Îã®ÏúÑ ÏàòÏ§ÄÏùò Ïú†Ìö®Ìïú CATE Ïã†Î¢∞Íµ¨Í∞ÑÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.

Ï∞∏Í≥†Î¨∏Ìóå:
  - Lei & Cand√®s (2021) "Conformal Inference of Counterfactuals"
  - Vovk et al. (2005) "Algorithmic Learning in a Random World"
"""

from __future__ import annotations

import logging
from typing import Any, Dict

import numpy as np
import pandas as pd

from engine.cells.base_cell import BaseCell
from engine.config import WhyLabConfig


class ConformalCell(BaseCell):
    """Î∂ÑÌè¨Î¨¥Í∞ÄÏ†ï(Distribution-Free) CATE ÏòàÏ∏°Íµ¨Í∞Ñ ÏÖÄ.

    Split Conformal Prediction:
      1. Îç∞Ïù¥ÌÑ∞Î•º Train/CalibrationÏúºÎ°ú Î∂ÑÌï†
      2. TrainÏúºÎ°ú CATE Î™®Îç∏ ÌïôÏäµ
      3. CalibrationÏóêÏÑú Ï†ÅÌï©ÎèÑ Ï†êÏàò(conformity score) Í≥ÑÏÇ∞
      4. QuantileÎ°ú ÏòàÏ∏°Íµ¨Í∞Ñ Ìè≠ Í≤∞Ï†ï
      5. ÏÉà Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ [œÑÃÇ(x) - q, œÑÃÇ(x) + q] Íµ¨Í∞Ñ ÏÉùÏÑ±
    """

    def __init__(self, config: WhyLabConfig) -> None:
        super().__init__(name="conformal_cell", config=config)

    def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Conformal CATE ÏòàÏ∏°Íµ¨Í∞ÑÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.

        Args:
            inputs: MetaLearnerCell ÎòêÎäî CausalCell Ï∂úÎ†•.
                ÌïÑÏàò: dataframe, feature_names, treatment_col, outcome_col

        Returns:
            conformal_ci: (n, 2) Î∞∞Ïó¥ ‚Äî Í∞Å Í∞úÏ≤¥Ïùò CATE ÏòàÏ∏°Íµ¨Í∞Ñ
            conformal_width: float ‚Äî ÌèâÍ∑† Íµ¨Í∞Ñ Ìè≠
            coverage: float ‚Äî Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò Ï†ÅÏ§ëÎ•†
        """
        self.validate_inputs(
            inputs,
            ["dataframe", "feature_names", "treatment_col", "outcome_col"],
        )

        df = inputs["dataframe"]
        X_cols = inputs["feature_names"]
        T_col = inputs["treatment_col"]
        Y_col = inputs["outcome_col"]
        alpha = self.config.dml.alpha  # Ïú†ÏùòÏàòÏ§Ä (Í∏∞Î≥∏ 0.05)

        X = df[X_cols].values.astype(np.float64)
        T = df[T_col].values.astype(np.float64)
        Y = df[Y_col].values.astype(np.float64)
        n = len(X)

        self.logger.info("üìê Conformal CATE ÏòàÏ∏°Íµ¨Í∞Ñ ÏÉùÏÑ± (Œ±=%.2f)", alpha)

        # ‚îÄ‚îÄ 1. Train/Calibration Î∂ÑÌï† (6:4) ‚îÄ‚îÄ
        np.random.seed(self.config.data.random_seed)
        perm = np.random.permutation(n)
        split = int(n * 0.6)
        train_idx, cal_idx = perm[:split], perm[split:]

        X_tr, T_tr, Y_tr = X[train_idx], T[train_idx], Y[train_idx]
        X_cal, T_cal, Y_cal = X[cal_idx], T[cal_idx], Y[cal_idx]

        self.logger.info("   Î∂ÑÌï†: Train=%d, Calibration=%d", len(train_idx), len(cal_idx))

        # ‚îÄ‚îÄ 2. CATE ÌïôÏäµ: modeÏóê Îî∞Îùº Î∂ÑÍ∏∞ ‚îÄ‚îÄ
        from engine.cells.meta_learner_cell import DRLearner

        conformal_mode = inputs.get("conformal_mode", "split")  # "split" | "cqr"

        if conformal_mode == "cqr":
            self.logger.info("   ‚ö° CQR (Conformalized Quantile Regression)")
            return self._cqr_conformal(
                X, T, Y, X_tr, T_tr, Y_tr, X_cal, T_cal, Y_cal,
                train_idx, cal_idx, alpha, inputs,
            )

        # Split Conformal (Í∏∞Î≥∏)
        model = DRLearner(config=self.config)
        model.fit(X_tr, T_tr, Y_tr)
        cate_cal = model.predict_cate(X_cal)

        # ‚îÄ‚îÄ 3. Conformity Score: DR Score Í∏∞Î∞ò ‚îÄ‚îÄ
        # Î∞òÏÇ¨Ïã§ÏùÑ ÏßÅÏ†ë Í¥ÄÏ∏°Ìï† Ïàò ÏóÜÏúºÎØÄÎ°ú,
        # DR (Doubly Robust) scoreÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ITE Í∑ºÏÇ¨
        scores = self._compute_dr_scores(X_cal, T_cal, Y_cal, X_tr, T_tr, Y_tr)
        residuals = np.abs(scores - cate_cal)

        # ‚îÄ‚îÄ 4. Quantile Í≥ÑÏÇ∞ ‚îÄ‚îÄ
        # q = ‚åà(1-Œ±)(n_cal+1)‚åâ / n_cal Î≤àÏß∏ ÏûîÏ∞®
        level = np.ceil((1 - alpha) * (len(cal_idx) + 1)) / len(cal_idx)
        level = min(level, 1.0)
        q = float(np.quantile(residuals, level))

        self.logger.info("   Quantile q=%.5f (level=%.3f)", q, level)

        # ‚îÄ‚îÄ 5. Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú ÏòàÏ∏°Íµ¨Í∞Ñ ‚îÄ‚îÄ
        # Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î°ú ÏµúÏ¢Ö Î™®Îç∏ Ïû¨ÌïôÏäµ
        final_model = DRLearner(config=self.config)
        final_model.fit(X, T, Y)
        cate_all = final_model.predict_cate(X)

        ci_lower = cate_all - q
        ci_upper = cate_all + q

        # ‚îÄ‚îÄ 6. Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò Ï†ÅÏ§ëÎ•† (Empirical Coverage) ‚îÄ‚îÄ
        cate_cal_final = final_model.predict_cate(X_cal)
        cal_lower = cate_cal_final - q
        cal_upper = cate_cal_final + q
        # scoresÍ∞Ä DR-ITE Í∑ºÏÇ¨Ïù¥ÎØÄÎ°ú Ïù¥Í≤ÉÏù¥ CIÏóê Ìè¨Ìï®ÎêòÎäî ÎπÑÏú®
        covered = np.mean((scores >= cal_lower) & (scores <= cal_upper))

        width = float(np.mean(ci_upper - ci_lower))

        self.logger.info(
            "   Í≤∞Í≥º: ÌèâÍ∑† Íµ¨Í∞Ñ Ìè≠=%.5f, Ï†ÅÏ§ëÎ•†=%.1f%% (Î™©Ìëú %.0f%%)",
            width, covered * 100, (1 - alpha) * 100,
        )

        conformal_results = {
            "mode": "split",
            "alpha": alpha,
            "quantile_q": q,
            "mean_width": width,
            "coverage": float(covered),
            "target_coverage": 1 - alpha,
            "n_train": len(train_idx),
            "n_calibration": len(cal_idx),
            "ci_lower_mean": float(np.mean(ci_lower)),
            "ci_upper_mean": float(np.mean(ci_upper)),
            "interpretation": (
                f"Conformal {(1-alpha)*100:.0f}% ÏòàÏ∏°Íµ¨Í∞Ñ: "
                f"Ìè≠={width:.5f}, Ïã§Ï†ú Ï†ÅÏ§ëÎ•†={covered*100:.1f}%"
            ),
        }

        return {
            **inputs,
            "conformal_ci_lower": ci_lower,
            "conformal_ci_upper": ci_upper,
            "conformal_cate": cate_all,
            "conformal_results": conformal_results,
        }

    def _cqr_conformal(
        self,
        X: np.ndarray, T: np.ndarray, Y: np.ndarray,
        X_tr: np.ndarray, T_tr: np.ndarray, Y_tr: np.ndarray,
        X_cal: np.ndarray, T_cal: np.ndarray, Y_cal: np.ndarray,
        train_idx: np.ndarray, cal_idx: np.ndarray,
        alpha: float,
        inputs: dict,
    ) -> dict:
        """CQR (Conformalized Quantile Regression).

        Romano et al. (2019) Í∏∞Î∞òÏùò Ï†ÅÏùëÏ†Å Íµ¨Í∞Ñ:
        XÏóê Îî∞Îùº CI Ìè≠Ïù¥ Î≥ÄÌï® ‚Äî Î∂àÌôïÏã§ ÏòÅÏó≠ÏóêÏÑú ÎÑìÍ≥†, ÌôïÏã§ ÏòÅÏó≠ÏóêÏÑú Ï¢ÅÏùå.

        Step 1: Î∂ÑÏúÑÏàò ÌöåÍ∑Ä q_lo(x), q_hi(x) ÌïôÏäµ
        Step 2: CalibrationÏóêÏÑú Ï†ÅÌï©ÎèÑ Ï†êÏàò
          s·µ¢ = max(q_lo(X·µ¢) - Œì·µ¢, Œì·µ¢ - q_hi(X·µ¢))
        Step 3: q = Quantile(1-Œ±, {s·µ¢})
        Step 4: CI(x) = [q_lo(x) - q, q_hi(x) + q]
        """
        from lightgbm import LGBMRegressor
        from engine.gpu_factory import create_lgbm_regressor

        # DR ScoreÎ°ú ITE Í∑ºÏÇ¨
        scores_cal = self._compute_dr_scores(
            X_cal, T_cal, Y_cal, X_tr, T_tr, Y_tr,
        )
        scores_tr = self._compute_dr_scores(
            X_tr, T_tr, Y_tr, X_tr, T_tr, Y_tr,
        )

        # Î∂ÑÏúÑÏàò ÌöåÍ∑Ä: ÌïòÌïú q_{Œ±/2}(x), ÏÉÅÌïú q_{1-Œ±/2}(x)
        q_lo_model = create_lgbm_regressor(self.config, lightweight=True)
        q_hi_model = create_lgbm_regressor(self.config, lightweight=True)

        # LightGBM quantile objective
        q_lo_model.set_params(objective="quantile", alpha=alpha / 2)
        q_hi_model.set_params(objective="quantile", alpha=1 - alpha / 2)

        q_lo_model.fit(X_tr, scores_tr)
        q_hi_model.fit(X_tr, scores_tr)

        # CalibrationÏóêÏÑú Ï†ÅÌï©ÎèÑ Ï†êÏàò
        q_lo_cal = q_lo_model.predict(X_cal)
        q_hi_cal = q_hi_model.predict(X_cal)

        conformity = np.maximum(
            q_lo_cal - scores_cal,
            scores_cal - q_hi_cal,
        )

        # Î∂ÑÏúÑÏàò
        level = np.ceil((1 - alpha) * (len(cal_idx) + 1)) / len(cal_idx)
        level = min(level, 1.0)
        q = float(np.quantile(conformity, level))

        self.logger.info("   CQR Quantile q=%.5f", q)

        # Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Ïóê Ï†ÅÏùëÏ†Å Íµ¨Í∞Ñ ÏÉùÏÑ±
        q_lo_all = q_lo_model.predict(X)
        q_hi_all = q_hi_model.predict(X)

        ci_lower = q_lo_all - q
        ci_upper = q_hi_all + q

        # CATE Ï†êÏ∂îÏ†ï (Ï§ëÏïôÍ∞í)
        from engine.cells.meta_learner_cell import DRLearner
        final_model = DRLearner(config=self.config)
        final_model.fit(X, T, Y)
        cate_all = final_model.predict_cate(X)

        # Ï†ÅÏ§ëÎ•†
        scores_all_cal = self._compute_dr_scores(
            X_cal, T_cal, Y_cal, X_tr, T_tr, Y_tr,
        )
        cal_lo = q_lo_model.predict(X_cal) - q
        cal_hi = q_hi_model.predict(X_cal) + q
        covered = np.mean((scores_all_cal >= cal_lo) & (scores_all_cal <= cal_hi))

        widths = ci_upper - ci_lower
        mean_width = float(np.mean(widths))
        std_width = float(np.std(widths))

        self.logger.info(
            "   CQR Í≤∞Í≥º: ÌèâÍ∑†Ìè≠=%.5f¬±%.5f, Ï†ÅÏ§ëÎ•†=%.1f%%",
            mean_width, std_width, covered * 100,
        )

        conformal_results = {
            "mode": "cqr",
            "alpha": alpha,
            "quantile_q": q,
            "mean_width": mean_width,
            "width_std": std_width,
            "coverage": float(covered),
            "target_coverage": 1 - alpha,
            "n_train": len(train_idx),
            "n_calibration": len(cal_idx),
            "ci_lower_mean": float(np.mean(ci_lower)),
            "ci_upper_mean": float(np.mean(ci_upper)),
            "adaptive": True,
            "interpretation": (
                f"CQR {(1-alpha)*100:.0f}% Ï†ÅÏùëÏ†Å ÏòàÏ∏°Íµ¨Í∞Ñ: "
                f"ÌèâÍ∑†Ìè≠={mean_width:.5f}¬±{std_width:.5f}, "
                f"Ï†ÅÏ§ëÎ•†={covered*100:.1f}%"
            ),
        }

        return {
            **inputs,
            "conformal_ci_lower": ci_lower,
            "conformal_ci_upper": ci_upper,
            "conformal_cate": cate_all,
            "conformal_results": conformal_results,
        }

    def _compute_dr_scores(
        self,
        X_cal: np.ndarray,
        T_cal: np.ndarray,
        Y_cal: np.ndarray,
        X_tr: np.ndarray,
        T_tr: np.ndarray,
        Y_tr: np.ndarray,
    ) -> np.ndarray:
        """DR Score Í≥ÑÏÇ∞: ITE (Individual Treatment Effect) Í∑ºÏÇ¨.

        ŒìÃÇ·µ¢ = ŒºÃÇ‚ÇÅ(X·µ¢) - ŒºÃÇ‚ÇÄ(X·µ¢)
            + T·µ¢¬∑(Y·µ¢ - ŒºÃÇ‚ÇÅ(X·µ¢))/√™(X·µ¢)
            - (1-T·µ¢)¬∑(Y·µ¢ - ŒºÃÇ‚ÇÄ(X·µ¢))/(1-√™(X·µ¢))
        """
        from engine.gpu_factory import create_lgbm_regressor
        from sklearn.linear_model import LogisticRegression

        # Ïù¥ÏßÑÌôî
        threshold = np.median(T_tr)
        T_tr_bin = (T_tr >= threshold).astype(int)
        T_cal_bin = (T_cal >= threshold).astype(int)

        # Outcome Î™®Îç∏ (Ï≤òÏπòÎ≥Ñ) ‚Äî GPU Í∞ÄÏÜç
        mask1 = T_tr_bin == 1
        mask0 = T_tr_bin == 0

        mu1 = create_lgbm_regressor(self.config, lightweight=True)
        mu0 = create_lgbm_regressor(self.config, lightweight=True)
        mu1.fit(X_tr[mask1], Y_tr[mask1])
        mu0.fit(X_tr[mask0], Y_tr[mask0])

        # Propensity Score
        ps = LogisticRegression(max_iter=1000, random_state=42)
        ps.fit(X_tr, T_tr_bin)
        e_hat = np.clip(ps.predict_proba(X_cal)[:, 1], 0.01, 0.99)

        # DR Score
        mu1_cal = mu1.predict(X_cal)
        mu0_cal = mu0.predict(X_cal)
        gamma = (mu1_cal - mu0_cal
                 + T_cal_bin * (Y_cal - mu1_cal) / e_hat
                 - (1 - T_cal_bin) * (Y_cal - mu0_cal) / (1 - e_hat))

        return gamma
