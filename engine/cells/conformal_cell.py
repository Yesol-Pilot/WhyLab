# -*- coding: utf-8 -*-
"""ConformalCell ‚Äî Î∂ÑÌè¨Î¨¥Í∞ÄÏ†ï CATE ÏòàÏ∏°Íµ¨Í∞Ñ.

Ï†ïÍ∑úÎ∂ÑÌè¨ Í∞ÄÏ†ï ÏóÜÏù¥, Split Conformal PredictionÏúºÎ°ú
Í∞úÎ≥Ñ Îã®ÏúÑ ÏàòÏ§ÄÏùò Ïú†Ìö®Ìïú CATE Ïã†Î¢∞Íµ¨Í∞ÑÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.

Ï∞∏Í≥†Î¨∏Ìóå:
  - Lei & Cand√®s (2021) "Conformal Inference of Counterfactuals"
  - Vovk et al. (2005) "Algorithmic Learning in a Random World"
"""

from __future__ import annotations

import logging
from typing import Any, Dict

import numpy as np
import pandas as pd

from engine.cells.base_cell import BaseCell
from engine.config import WhyLabConfig


class ConformalCell(BaseCell):
    """Î∂ÑÌè¨Î¨¥Í∞ÄÏ†ï(Distribution-Free) CATE ÏòàÏ∏°Íµ¨Í∞Ñ ÏÖÄ.

    Split Conformal Prediction:
      1. Îç∞Ïù¥ÌÑ∞Î•º Train/CalibrationÏúºÎ°ú Î∂ÑÌï†
      2. TrainÏúºÎ°ú CATE Î™®Îç∏ ÌïôÏäµ
      3. CalibrationÏóêÏÑú Ï†ÅÌï©ÎèÑ Ï†êÏàò(conformity score) Í≥ÑÏÇ∞
      4. QuantileÎ°ú ÏòàÏ∏°Íµ¨Í∞Ñ Ìè≠ Í≤∞Ï†ï
      5. ÏÉà Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ [œÑÃÇ(x) - q, œÑÃÇ(x) + q] Íµ¨Í∞Ñ ÏÉùÏÑ±
    """

    def __init__(self, config: WhyLabConfig) -> None:
        super().__init__(name="conformal_cell", config=config)

    def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Conformal CATE ÏòàÏ∏°Íµ¨Í∞ÑÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.

        Args:
            inputs: MetaLearnerCell ÎòêÎäî CausalCell Ï∂úÎ†•.
                ÌïÑÏàò: dataframe, feature_names, treatment_col, outcome_col

        Returns:
            conformal_ci: (n, 2) Î∞∞Ïó¥ ‚Äî Í∞Å Í∞úÏ≤¥Ïùò CATE ÏòàÏ∏°Íµ¨Í∞Ñ
            conformal_width: float ‚Äî ÌèâÍ∑† Íµ¨Í∞Ñ Ìè≠
            coverage: float ‚Äî Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò Ï†ÅÏ§ëÎ•†
        """
        self.validate_inputs(
            inputs,
            ["dataframe", "feature_names", "treatment_col", "outcome_col"],
        )

        df = inputs["dataframe"]
        X_cols = inputs["feature_names"]
        T_col = inputs["treatment_col"]
        Y_col = inputs["outcome_col"]
        alpha = self.config.dml.alpha  # Ïú†ÏùòÏàòÏ§Ä (Í∏∞Î≥∏ 0.05)

        X = df[X_cols].values.astype(np.float64)
        T = df[T_col].values.astype(np.float64)
        Y = df[Y_col].values.astype(np.float64)
        n = len(X)

        self.logger.info("üìê Conformal CATE ÏòàÏ∏°Íµ¨Í∞Ñ ÏÉùÏÑ± (Œ±=%.2f)", alpha)

        # ‚îÄ‚îÄ 1. Train/Calibration Î∂ÑÌï† (6:4) ‚îÄ‚îÄ
        np.random.seed(self.config.data.random_seed)
        perm = np.random.permutation(n)
        split = int(n * 0.6)
        train_idx, cal_idx = perm[:split], perm[split:]

        X_tr, T_tr, Y_tr = X[train_idx], T[train_idx], Y[train_idx]
        X_cal, T_cal, Y_cal = X[cal_idx], T[cal_idx], Y[cal_idx]

        self.logger.info("   Î∂ÑÌï†: Train=%d, Calibration=%d", len(train_idx), len(cal_idx))

        # ‚îÄ‚îÄ 2. DR-LearnerÎ°ú CATE ÌïôÏäµ (Í∞ÄÏû• Ïù¥Î°†Ï†Å Î≥¥Ïû• Í∞ïÌï®) ‚îÄ‚îÄ
        from engine.cells.meta_learner_cell import DRLearner

        model = DRLearner()
        model.fit(X_tr, T_tr, Y_tr)
        cate_cal = model.predict_cate(X_cal)

        # ‚îÄ‚îÄ 3. Conformity Score: DR Score Í∏∞Î∞ò ‚îÄ‚îÄ
        # Î∞òÏÇ¨Ïã§ÏùÑ ÏßÅÏ†ë Í¥ÄÏ∏°Ìï† Ïàò ÏóÜÏúºÎØÄÎ°ú,
        # DR (Doubly Robust) scoreÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ITE Í∑ºÏÇ¨
        scores = self._compute_dr_scores(X_cal, T_cal, Y_cal, X_tr, T_tr, Y_tr)
        residuals = np.abs(scores - cate_cal)

        # ‚îÄ‚îÄ 4. Quantile Í≥ÑÏÇ∞ ‚îÄ‚îÄ
        # q = ‚åà(1-Œ±)(n_cal+1)‚åâ / n_cal Î≤àÏß∏ ÏûîÏ∞®
        level = np.ceil((1 - alpha) * (len(cal_idx) + 1)) / len(cal_idx)
        level = min(level, 1.0)
        q = float(np.quantile(residuals, level))

        self.logger.info("   Quantile q=%.5f (level=%.3f)", q, level)

        # ‚îÄ‚îÄ 5. Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú ÏòàÏ∏°Íµ¨Í∞Ñ ‚îÄ‚îÄ
        # Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î°ú ÏµúÏ¢Ö Î™®Îç∏ Ïû¨ÌïôÏäµ
        final_model = DRLearner()
        final_model.fit(X, T, Y)
        cate_all = final_model.predict_cate(X)

        ci_lower = cate_all - q
        ci_upper = cate_all + q

        # ‚îÄ‚îÄ 6. Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò Ï†ÅÏ§ëÎ•† (Empirical Coverage) ‚îÄ‚îÄ
        cate_cal_final = final_model.predict_cate(X_cal)
        cal_lower = cate_cal_final - q
        cal_upper = cate_cal_final + q
        # scoresÍ∞Ä DR-ITE Í∑ºÏÇ¨Ïù¥ÎØÄÎ°ú Ïù¥Í≤ÉÏù¥ CIÏóê Ìè¨Ìï®ÎêòÎäî ÎπÑÏú®
        covered = np.mean((scores >= cal_lower) & (scores <= cal_upper))

        width = float(np.mean(ci_upper - ci_lower))

        self.logger.info(
            "   Í≤∞Í≥º: ÌèâÍ∑† Íµ¨Í∞Ñ Ìè≠=%.5f, Ï†ÅÏ§ëÎ•†=%.1f%% (Î™©Ìëú %.0f%%)",
            width, covered * 100, (1 - alpha) * 100,
        )

        conformal_results = {
            "alpha": alpha,
            "quantile_q": q,
            "mean_width": width,
            "coverage": float(covered),
            "target_coverage": 1 - alpha,
            "n_train": len(train_idx),
            "n_calibration": len(cal_idx),
            "ci_lower_mean": float(np.mean(ci_lower)),
            "ci_upper_mean": float(np.mean(ci_upper)),
            "interpretation": (
                f"Conformal {(1-alpha)*100:.0f}% ÏòàÏ∏°Íµ¨Í∞Ñ: "
                f"Ìè≠={width:.5f}, Ïã§Ï†ú Ï†ÅÏ§ëÎ•†={covered*100:.1f}%"
            ),
        }

        return {
            **inputs,
            "conformal_ci_lower": ci_lower,
            "conformal_ci_upper": ci_upper,
            "conformal_cate": cate_all,
            "conformal_results": conformal_results,
        }

    def _compute_dr_scores(
        self,
        X_cal: np.ndarray,
        T_cal: np.ndarray,
        Y_cal: np.ndarray,
        X_tr: np.ndarray,
        T_tr: np.ndarray,
        Y_tr: np.ndarray,
    ) -> np.ndarray:
        """DR Score Í≥ÑÏÇ∞: ITE (Individual Treatment Effect) Í∑ºÏÇ¨.

        ŒìÃÇ·µ¢ = ŒºÃÇ‚ÇÅ(X·µ¢) - ŒºÃÇ‚ÇÄ(X·µ¢)
            + T·µ¢¬∑(Y·µ¢ - ŒºÃÇ‚ÇÅ(X·µ¢))/√™(X·µ¢)
            - (1-T·µ¢)¬∑(Y·µ¢ - ŒºÃÇ‚ÇÄ(X·µ¢))/(1-√™(X·µ¢))
        """
        from engine.gpu_factory import create_lgbm_regressor
        from sklearn.linear_model import LogisticRegression

        # Ïù¥ÏßÑÌôî
        threshold = np.median(T_tr)
        T_tr_bin = (T_tr >= threshold).astype(int)
        T_cal_bin = (T_cal >= threshold).astype(int)

        # Outcome Î™®Îç∏ (Ï≤òÏπòÎ≥Ñ) ‚Äî GPU Í∞ÄÏÜç
        mask1 = T_tr_bin == 1
        mask0 = T_tr_bin == 0

        mu1 = create_lgbm_regressor(self.config, lightweight=True)
        mu0 = create_lgbm_regressor(self.config, lightweight=True)
        mu1.fit(X_tr[mask1], Y_tr[mask1])
        mu0.fit(X_tr[mask0], Y_tr[mask0])

        # Propensity Score
        ps = LogisticRegression(max_iter=1000, random_state=42)
        ps.fit(X_tr, T_tr_bin)
        e_hat = np.clip(ps.predict_proba(X_cal)[:, 1], 0.01, 0.99)

        # DR Score
        mu1_cal = mu1.predict(X_cal)
        mu0_cal = mu0.predict(X_cal)
        gamma = (mu1_cal - mu0_cal
                 + T_cal_bin * (Y_cal - mu1_cal) / e_hat
                 - (1 - T_cal_bin) * (Y_cal - mu0_cal) / (1 - e_hat))

        return gamma
