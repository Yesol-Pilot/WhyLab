# -*- coding: utf-8 -*-
"""CausalLoopAgent â€” ë°˜ë³µ ìê¸°êµì • ì¸ê³¼ ë°œê²¬ ì—ì´ì „íŠ¸.

ê°€ì„¤ â†’ ê²€ì¦ â†’ ë°˜ì¦ â†’ ìˆ˜ì • ìˆœí™˜ ì›Œí¬í”Œë¡œ.
LLMì´ ê°€ì„¤ì„ ìƒì„±í•˜ê³ , í†µê³„ì  ë°©ë²•ì´ ê²€ì¦í•˜ê³ ,
ë¶ˆì¼ì¹˜ ì‹œ ì—ì´ì „íŠ¸ê°€ ê°€ì„¤ì„ ìˆ˜ì •í•˜ì—¬ ë°˜ë³µí•©ë‹ˆë‹¤.

Sprint 2 í™•ì¥: LLM ì‹¤ì—°ë™ + Reflexion ë©”ëª¨ë¦¬ íŒ¨í„´.
- LLM ì—°ë™: GeminiClientë¥¼ í†µí•´ ì¸ê³¼ ê°€ì„¤ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ìƒì„±
- Reflexion: ì´ì „ ë°˜ë³µì˜ ì„±ê³µ/ì‹¤íŒ¨ë¥¼ ëª…ì‹œì  ë©”ëª¨ë¦¬ë¡œ ì¶•ì 

í•™ìˆ  ì°¸ì¡°:
  - Shinn et al. (2023). "Reflexion: Language Agents with Verbal
    Reinforcement Learning." NeurIPS.
"""

from __future__ import annotations

import json
import logging
import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


@dataclass
class CausalHypothesis:
    """ì¸ê³¼ ê°€ì„¤."""
    edges: List[Tuple[str, str]]  # (ì›ì¸, ê²°ê³¼) ìŒ ë¦¬ìŠ¤íŠ¸
    confidence: float = 0.0
    rationale: str = ""
    iteration: int = 0


@dataclass
class ReflexionEntry:
    """Reflexion ë©”ëª¨ë¦¬ í•­ëª©."""
    iteration: int
    hypothesis_summary: str
    validated_edges: List[Tuple[str, str]]
    rejected_edges: List[Tuple[str, str]]
    lesson: str  # LLMì´ ìƒì„±í•œ êµí›ˆ ë˜ëŠ” ê·œì¹™ ê¸°ë°˜ êµí›ˆ


@dataclass
class LoopState:
    """ë°˜ë³µ ìƒíƒœ."""
    hypotheses: List[CausalHypothesis] = field(default_factory=list)
    validations: List[Dict[str, Any]] = field(default_factory=list)
    refutations: List[Dict[str, Any]] = field(default_factory=list)
    reflexion_memory: List[ReflexionEntry] = field(default_factory=list)
    converged: bool = False
    iterations: int = 0
    final_dag: List[Tuple[str, str]] = field(default_factory=list)
    llm_used: bool = False


class CausalLoopAgent:
    """ë°˜ë³µ ìê¸°êµì • ì¸ê³¼ ë°œê²¬ ì—ì´ì „íŠ¸.

    CausalLoop í”„ë¡œì„¸ìŠ¤:
    1. **ê°€ì„¤ ìƒì„±** (Hypothesize): LLM ë˜ëŠ” PC ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì´ˆê¸° DAG ê°€ì„¤ ìƒì„±
    2. **ê²€ì¦** (Validate): ì¡°ê±´ë¶€ ë…ë¦½ ê²€ì • + ìƒê´€ ë¶„ì„ìœ¼ë¡œ ê°€ì„¤ ê²€ì¦
    3. **ë°˜ì¦** (Refute): ë°˜ì¦ ì¦ê±°(ì—­ë°©í–¥ ì¸ê³¼, ìˆ¨ê²¨ì§„ êµë€) íƒìƒ‰
    4. **ìˆ˜ì •** (Revise): ë¶ˆì¼ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì„¤ ìˆ˜ì •
    5. **ìˆ˜ë ´ íŒë‹¨**: ìˆ˜ì • ì—†ìœ¼ë©´ ìˆ˜ë ´, ì•„ë‹ˆë©´ 1ë¡œ ë³µê·€

    Args:
        max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜.
        convergence_threshold: ìˆ˜ë ´ íŒë‹¨ ì„ê³„ê°’ (ë³€ê²½ëœ ì—£ì§€ ë¹„ìœ¨).
        significance_level: í†µê³„ ê²€ì • ìœ ì˜ ìˆ˜ì¤€.
        use_llm: LLM ê°€ì„¤ ìƒì„± í™œì„±í™” ì—¬ë¶€.
    """

    def __init__(
        self,
        max_iterations: int = 5,
        convergence_threshold: float = 0.05,
        significance_level: float = 0.05,
        use_llm: bool = True,
    ) -> None:
        self.max_iterations = max_iterations
        self.convergence_threshold = convergence_threshold
        self.significance_level = significance_level
        self.use_llm = use_llm
        self._llm_client = None

    def _get_llm(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì§€ì—° ë¡œë“œí•©ë‹ˆë‹¤."""
        if self._llm_client is None and self.use_llm:
            try:
                from engine.agents.llm_adapter import GeminiClient
                self._llm_client = GeminiClient()
                if not self._llm_client.is_available():
                    logger.info("LLM API ë¯¸ì‚¬ìš© (í‚¤ ì—†ìŒ). ê·œì¹™ ê¸°ë°˜ ëª¨ë“œ.")
                    self._llm_client = None
            except Exception as e:
                logger.warning("LLM ë¡œë“œ ì‹¤íŒ¨: %s. ê·œì¹™ ê¸°ë°˜ ëª¨ë“œ.", e)
                self._llm_client = None
        return self._llm_client

    def run(
        self,
        df: pd.DataFrame,
        treatment: str,
        outcome: str,
        features: Optional[List[str]] = None,
    ) -> LoopState:
        """CausalLoopë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            df: ë¶„ì„ ëŒ€ìƒ ë°ì´í„°í”„ë ˆì„.
            treatment: ì²˜ì¹˜ ë³€ìˆ˜ëª….
            outcome: ê²°ê³¼ ë³€ìˆ˜ëª….
            features: ê³µë³€ëŸ‰ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™ ê°ì§€).

        Returns:
            LoopState: ë°˜ë³µ ê²°ê³¼.
        """
        if features is None:
            features = [c for c in df.columns if c not in [treatment, outcome]]

        all_vars = features + [treatment, outcome]
        state = LoopState()

        logger.info("ğŸ”„ CausalLoop ì‹œì‘ â€” ë³€ìˆ˜ %dê°œ, ìµœëŒ€ %díšŒ ë°˜ë³µ", len(all_vars), self.max_iterations)

        for iteration in range(1, self.max_iterations + 1):
            state.iterations = iteration

            # â”€â”€â”€â”€ 1. ê°€ì„¤ ìƒì„± (LLM ë˜ëŠ” ê·œì¹™ ê¸°ë°˜) â”€â”€â”€â”€
            hypothesis = self._hypothesize(df, all_vars, treatment, outcome, state)
            state.hypotheses.append(hypothesis)

            # â”€â”€â”€â”€ 2. ê²€ì¦ â”€â”€â”€â”€
            validation = self._validate(df, hypothesis, all_vars)
            state.validations.append(validation)

            # â”€â”€â”€â”€ 3. ë°˜ì¦ â”€â”€â”€â”€
            refutation = self._refute(df, hypothesis, treatment, outcome, features)
            state.refutations.append(refutation)

            # â”€â”€â”€â”€ 4. Reflexion ë©”ëª¨ë¦¬ ì¶•ì  â”€â”€â”€â”€
            reflexion = self._reflect(
                hypothesis, validation, refutation, iteration,
            )
            state.reflexion_memory.append(reflexion)

            # â”€â”€â”€â”€ 5. ìˆ˜ë ´ íŒë‹¨ â”€â”€â”€â”€
            if self._check_convergence(state, hypothesis, validation, refutation):
                state.converged = True
                state.final_dag = hypothesis.edges
                logger.info(
                    "âœ… CausalLoop ìˆ˜ë ´ (ë°˜ë³µ %díšŒ) â€” ì—£ì§€ %dê°œ, LLM=%s",
                    iteration, len(hypothesis.edges),
                    "ON" if state.llm_used else "OFF",
                )
                break

            # â”€â”€â”€â”€ 6. ìˆ˜ì • (ë‹¤ìŒ ë°˜ë³µì—ì„œ ê°€ì„¤ ìƒì„± ì‹œ ë°˜ì˜) â”€â”€â”€â”€
            logger.info(
                "ğŸ” ë°˜ë³µ %d: ìˆ˜ì • í•„ìš” â€” ê¸°ê° %dê°œ, ì‹ ê·œ í›„ë³´ %dê°œ, êµí›ˆ: %s",
                iteration,
                refutation.get("rejected_count", 0),
                refutation.get("new_candidate_count", 0),
                reflexion.lesson[:80],
            )

        if not state.converged:
            # ìµœëŒ€ ë°˜ë³µ ë„ë‹¬ â€” ë§ˆì§€ë§‰ ê°€ì„¤ ì‚¬ìš©
            state.final_dag = state.hypotheses[-1].edges
            logger.warning("âš ï¸ CausalLoop ìµœëŒ€ ë°˜ë³µ ë„ë‹¬. ë§ˆì§€ë§‰ ê°€ì„¤ ì‚¬ìš©.")

        return state

    def _hypothesize(
        self,
        df: pd.DataFrame,
        all_vars: List[str],
        treatment: str,
        outcome: str,
        state: LoopState,
    ) -> CausalHypothesis:
        """ì¸ê³¼ ê°€ì„¤ì„ ìƒì„±í•©ë‹ˆë‹¤.

        LLM ì‚¬ìš© ê°€ëŠ¥ ì‹œ: LLMì—ê²Œ í†µê³„ì  ìš”ì•½ + Reflexion ë©”ëª¨ë¦¬ë¥¼ ì „ë‹¬.
        LLM ë¯¸ì‚¬ìš© ì‹œ: ê¸°ì¡´ ìƒê´€ê´€ê³„ ê¸°ë°˜ ê·œì¹™.
        """
        iteration = state.iterations

        # LLM ê°€ì„¤ ìƒì„± ì‹œë„
        llm = self._get_llm()
        if llm is not None:
            try:
                llm_hypothesis = self._hypothesize_with_llm(
                    df, all_vars, treatment, outcome, state, llm,
                )
                if llm_hypothesis is not None:
                    state.llm_used = True
                    return llm_hypothesis
            except Exception as e:
                logger.warning("LLM ê°€ì„¤ ìƒì„± ì‹¤íŒ¨: %s. ê·œì¹™ ê¸°ë°˜ fallback.", e)

        # Fallback: ê·œì¹™ ê¸°ë°˜ ê°€ì„¤ ìƒì„±
        return self._hypothesize_rule_based(df, all_vars, treatment, outcome, state)

    def _hypothesize_with_llm(
        self,
        df: pd.DataFrame,
        all_vars: List[str],
        treatment: str,
        outcome: str,
        state: LoopState,
        llm_client,
    ) -> Optional[CausalHypothesis]:
        """LLM ê¸°ë°˜ ì¸ê³¼ ê°€ì„¤ ìƒì„±.

        í”„ë¡¬í”„íŠ¸ êµ¬ì„±:
        1. ë³€ìˆ˜ ëª©ë¡ + ê¸°ìˆ í†µê³„ ìš”ì•½
        2. ìƒê´€í–‰ë ¬ (ìƒìœ„ ê´€ê³„ë§Œ)
        3. Reflexion ë©”ëª¨ë¦¬ (ì´ì „ êµí›ˆë“¤)
        4. ê°€ì„¤ ì¶œë ¥ í˜•ì‹ ì§€ì • (JSON)
        """
        iteration = state.iterations
        numeric_cols = [c for c in all_vars if df[c].dtype in [np.float64, np.int64, float, int]]

        # í†µê³„ ìš”ì•½ ìƒì„±
        stats_summary = ""
        if numeric_cols:
            desc = df[numeric_cols].describe().round(3).to_string()
            corr = df[numeric_cols].corr().round(3)
            # ìƒê´€ê³„ìˆ˜ ìƒìœ„ 10ê°œë§Œ
            corr_pairs = []
            for i, v1 in enumerate(numeric_cols):
                for v2 in numeric_cols[i + 1:]:
                    r = corr.loc[v1, v2]
                    corr_pairs.append((v1, v2, r))
            corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
            top_corr = corr_pairs[:10]
            corr_str = "\n".join(f"  {v1} â†” {v2}: r={r:.3f}" for v1, v2, r in top_corr)
            stats_summary = f"ê¸°ìˆ í†µê³„:\n{desc}\n\nìƒìœ„ ìƒê´€ê´€ê³„:\n{corr_str}"

        # Reflexion ë©”ëª¨ë¦¬ êµ¬ì„±
        reflexion_str = ""
        if state.reflexion_memory:
            entries = []
            for entry in state.reflexion_memory[-3:]:  # ìµœê·¼ 3ê°œë§Œ
                entries.append(
                    f"  ë°˜ë³µ {entry.iteration}: "
                    f"ê²€ì¦ë¨={entry.validated_edges}, "
                    f"ê¸°ê°ë¨={entry.rejected_edges}, "
                    f"êµí›ˆ={entry.lesson}"
                )
            reflexion_str = "\nê³¼ê±° êµí›ˆ (Reflexion Memory):\n" + "\n".join(entries)

        prompt = f"""ë‹¹ì‹ ì€ ì¸ê³¼ì¶”ë¡  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ê³¼ ê·¸ë˜í”„(DAG) ê°€ì„¤ì„ ì œì•ˆí•˜ì„¸ìš”.

## ì„¤ì •
- ì²˜ì¹˜ ë³€ìˆ˜: {treatment}
- ê²°ê³¼ ë³€ìˆ˜: {outcome}
- ì „ì²´ ë³€ìˆ˜: {all_vars}
- ë°˜ë³µ: {iteration}/{self.max_iterations}

## ë°ì´í„° í†µê³„
{stats_summary}
{reflexion_str}

## ìš”êµ¬ ì‚¬í•­
1. ì¸ê³¼ì ìœ¼ë¡œ íƒ€ë‹¹í•œ ë°©í–¥ì˜ ì—£ì§€ë§Œ ì œì•ˆí•˜ì„¸ìš” (ìƒê´€ â‰  ì¸ê³¼).
2. êµë€ ë³€ìˆ˜(Confounder)ê°€ ìˆë‹¤ë©´ ì ì ˆí•œ ë°©í–¥ìœ¼ë¡œ ì—°ê²°í•˜ì„¸ìš”.
3. Reflexion ë©”ëª¨ë¦¬ì˜ êµí›ˆì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSON)
```json
{{
  "edges": [["ì›ì¸1", "ê²°ê³¼1"], ["ì›ì¸2", "ê²°ê³¼2"]],
  "confidence": 0.7,
  "rationale": "ì´ ê°€ì„¤ì„ ì œì•ˆí•œ ì´ìœ "
}}
```

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        response = llm_client.generate(prompt, max_tokens=1024)
        if response is None:
            return None

        # JSON íŒŒì‹±
        try:
            # ì½”ë“œ ë¸”ë¡ ë‚´ JSON ì¶”ì¶œ
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group(1))
            else:
                parsed = json.loads(response)

            edges = [tuple(e) for e in parsed.get("edges", [])]
            confidence = float(parsed.get("confidence", 0.5))
            rationale = parsed.get("rationale", "LLM ìƒì„± ê°€ì„¤")

            logger.info("ğŸ¤– LLM ê°€ì„¤: ì—£ì§€ %dê°œ, í™•ì‹ ë„ %.2f", len(edges), confidence)
            return CausalHypothesis(
                edges=edges,
                confidence=confidence,
                rationale=f"LLM ë°˜ë³µ {iteration}: {rationale}",
                iteration=iteration,
            )
        except (json.JSONDecodeError, KeyError, TypeError) as e:
            logger.warning("LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: %s", e)
            return None

    def _hypothesize_rule_based(
        self,
        df: pd.DataFrame,
        all_vars: List[str],
        treatment: str,
        outcome: str,
        state: LoopState,
    ) -> CausalHypothesis:
        """ê·œì¹™ ê¸°ë°˜ ì¸ê³¼ ê°€ì„¤ ìƒì„± (ê¸°ì¡´ ë¡œì§)."""
        iteration = state.iterations
        numeric_cols = [c for c in all_vars if df[c].dtype in [np.float64, np.int64, float, int]]

        if not numeric_cols:
            return CausalHypothesis(edges=[(treatment, outcome)], iteration=iteration)

        corr_matrix = df[numeric_cols].corr().abs()
        edges = []

        # Treatment â†’ Outcome (í•µì‹¬ ì—£ì§€)
        if treatment in numeric_cols and outcome in numeric_cols:
            edges.append((treatment, outcome))

        # ê°•í•œ ìƒê´€ê´€ê³„ ê¸°ë°˜ ì—£ì§€ í›„ë³´
        for i, v1 in enumerate(numeric_cols):
            for v2 in numeric_cols[i + 1:]:
                if v1 == v2:
                    continue
                r = corr_matrix.loc[v1, v2] if v1 in corr_matrix.index and v2 in corr_matrix.columns else 0
                if r > 0.3:
                    if v2 == outcome:
                        edges.append((v1, v2))
                    elif v1 == outcome:
                        edges.append((v2, v1))
                    elif v1 == treatment:
                        edges.append((v1, v2))
                    elif v2 == treatment:
                        edges.append((v2, v1))
                    else:
                        edges.append((v1, v2))

        # Reflexion ë©”ëª¨ë¦¬ ë°˜ì˜: ì´ì „ì— ê¸°ê°ëœ ì—£ì§€ ì œê±°
        if state.reflexion_memory:
            all_rejected = set()
            for entry in state.reflexion_memory:
                all_rejected.update(tuple(e) for e in entry.rejected_edges)
            edges = [e for e in edges if e not in all_rejected]

        # ì´ì „ ë°˜ì¦ì—ì„œ ê¸°ê°ëœ ì—£ì§€ë„ ì œê±° (í•˜ìœ„ í˜¸í™˜)
        if state.refutations:
            last_refutation = state.refutations[-1]
            rejected = set(tuple(e) for e in last_refutation.get("rejected_edges", []))
            edges = [e for e in edges if e not in rejected]

        edges = list(set(edges))

        return CausalHypothesis(
            edges=edges,
            confidence=0.5 + 0.1 * iteration,
            rationale=f"ë°˜ë³µ {iteration}: ìƒê´€ ê¸°ë°˜ + Reflexion ë°˜ì˜",
            iteration=iteration,
        )

    def _reflect(
        self,
        hypothesis: CausalHypothesis,
        validation: Dict[str, Any],
        refutation: Dict[str, Any],
        iteration: int,
    ) -> ReflexionEntry:
        """Reflexion ë©”ëª¨ë¦¬ í•­ëª©ì„ ìƒì„±í•©ë‹ˆë‹¤.

        ì´ì „ ë°˜ë³µì˜ ì„±ê³µ/ì‹¤íŒ¨ë¥¼ êµ¬ì¡°í™”ëœ êµí›ˆìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        validated = [v["edge"] for v in validation.get("validated", [])]
        rejected = [tuple(e) for e in refutation.get("rejected_edges", [])]

        # êµí›ˆ ìƒì„±
        lessons = []
        if rejected:
            lessons.append(f"ì—£ì§€ {rejected}ì€ ë¶€ë¶„ìƒê´€ ê²€ì •ì—ì„œ ê¸°ê°ë¨ â€” êµë€ì— ì˜í•œ í—ˆìœ„ ìƒê´€ì¼ ê°€ëŠ¥ì„±.")
        if validation.get("validation_rate", 0) < 0.5:
            lessons.append("ê²€ì¦ë¥ ì´ 50% ë¯¸ë§Œ â€” ê°€ì„¤ì´ ë„ˆë¬´ ê³µê²©ì . ë³´ìˆ˜ì  ì ‘ê·¼ í•„ìš”.")
        if validation.get("validation_rate", 0) > 0.9 and not rejected:
            lessons.append("ë†’ì€ ê²€ì¦ë¥  + ê¸°ê° ì—†ìŒ â€” í˜„ì¬ ë°©í–¥ ìœ ì§€.")
        new_cands = refutation.get("new_candidates", [])
        if new_cands:
            lessons.append(f"ëˆ„ë½ëœ ì—£ì§€ í›„ë³´ ë°œê²¬: {new_cands[:3]}")

        lesson = " | ".join(lessons) if lessons else "íŠ¹ê¸°ì‚¬í•­ ì—†ìŒ."

        return ReflexionEntry(
            iteration=iteration,
            hypothesis_summary=f"ì—£ì§€ {len(hypothesis.edges)}ê°œ",
            validated_edges=validated,
            rejected_edges=rejected,
            lesson=lesson,
        )

    def _validate(
        self,
        df: pd.DataFrame,
        hypothesis: CausalHypothesis,
        all_vars: List[str],
    ) -> Dict[str, Any]:
        """ê°€ì„¤ì˜ ê° ì—£ì§€ë¥¼ ì¡°ê±´ë¶€ ë…ë¦½ ê²€ì •ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤."""
        from scipy import stats

        validated = []
        failed = []

        for cause, effect in hypothesis.edges:
            if cause not in df.columns or effect not in df.columns:
                failed.append((cause, effect, "ë³€ìˆ˜ ì—†ìŒ"))
                continue

            try:
                # ë‹¨ìˆœ ìƒê´€ ê²€ì •
                if df[cause].dtype in [np.float64, np.int64, float, int] and \
                   df[effect].dtype in [np.float64, np.int64, float, int]:
                    r, p_val = stats.pearsonr(df[cause].dropna(), df[effect].dropna())

                    if p_val < self.significance_level:
                        validated.append({
                            "edge": (cause, effect),
                            "correlation": round(float(r), 4),
                            "p_value": round(float(p_val), 6),
                            "status": "validated",
                        })
                    else:
                        failed.append((cause, effect, f"p={p_val:.4f}"))
                else:
                    validated.append({
                        "edge": (cause, effect),
                        "status": "skipped_non_numeric",
                    })
            except Exception as e:
                failed.append((cause, effect, str(e)))

        return {
            "validated": validated,
            "failed": failed,
            "validation_rate": len(validated) / max(len(hypothesis.edges), 1),
        }

    def _refute(
        self,
        df: pd.DataFrame,
        hypothesis: CausalHypothesis,
        treatment: str,
        outcome: str,
        features: List[str],
    ) -> Dict[str, Any]:
        """ê°€ì„¤ì— ëŒ€í•œ ë°˜ì¦ ì¦ê±°ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤."""
        from scipy import stats

        rejected_edges = []
        new_candidates = []

        for cause, effect in hypothesis.edges:
            if cause not in df.columns or effect not in df.columns:
                continue

            # ë°˜ì¦ 1: ì—­ë°©í–¥ì´ ë” ê°•í•œê°€?
            try:
                if df[cause].dtype in [np.float64, np.int64, float, int] and \
                   df[effect].dtype in [np.float64, np.int64, float, int]:
                    # ë¶€ë¶„ ìƒê´€ â€” ë‹¤ë¥¸ ë³€ìˆ˜ í†µì œ í›„ì—ë„ ìœ ì§€ë˜ëŠ”ì§€
                    other_vars = [v for v in features if v != cause and v != effect and v in df.columns]
                    if other_vars:
                        # ì”ì°¨ ê¸°ë°˜ ë¶€ë¶„ ìƒê´€
                        from sklearn.linear_model import LinearRegression
                        valid_others = [v for v in other_vars[:5] if df[v].dtype in [np.float64, np.int64, float, int]]
                        if valid_others:
                            mask = df[[cause, effect] + valid_others].dropna().index
                            if len(mask) > 10:
                                X_ctrl = df.loc[mask, valid_others].values
                                res_cause = cause
                                res_effect = effect

                                lr1 = LinearRegression().fit(X_ctrl, df.loc[mask, cause])
                                lr2 = LinearRegression().fit(X_ctrl, df.loc[mask, effect])

                                resid1 = df.loc[mask, cause] - lr1.predict(X_ctrl)
                                resid2 = df.loc[mask, effect] - lr2.predict(X_ctrl)

                                partial_r, partial_p = stats.pearsonr(resid1, resid2)

                                if partial_p > self.significance_level:
                                    rejected_edges.append((cause, effect))
            except Exception:
                pass

        # ë°˜ì¦ 2: ëˆ„ë½ëœ ì—£ì§€ í›„ë³´ íƒìƒ‰
        existing = set(hypothesis.edges)
        numeric_features = [f for f in features if f in df.columns and df[f].dtype in [np.float64, np.int64, float, int]]

        for feat in numeric_features[:10]:
            if outcome in df.columns and df[outcome].dtype in [np.float64, np.int64, float, int]:
                try:
                    r, p = stats.pearsonr(df[feat].dropna(), df[outcome].dropna())
                    if abs(r) > 0.2 and p < 0.01 and (feat, outcome) not in existing:
                        new_candidates.append((feat, outcome))
                except Exception:
                    pass

        return {
            "rejected_edges": rejected_edges,
            "rejected_count": len(rejected_edges),
            "new_candidates": new_candidates,
            "new_candidate_count": len(new_candidates),
        }

    def _check_convergence(
        self,
        state: LoopState,
        hypothesis: CausalHypothesis,
        validation: Dict[str, Any],
        refutation: Dict[str, Any],
    ) -> bool:
        """ìˆ˜ë ´ ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤."""
        # ê¸°ê°ëœ ì—£ì§€ê°€ ì—†ê³ , ìƒˆ í›„ë³´ë„ ì—†ìœ¼ë©´ ìˆ˜ë ´
        if refutation["rejected_count"] == 0 and refutation["new_candidate_count"] == 0:
            return True

        # ë³€ê²½ ë¹„ìœ¨ì´ ì„ê³„ê°’ ì´í•˜ë©´ ìˆ˜ë ´
        total_edges = max(len(hypothesis.edges), 1)
        change_rate = (refutation["rejected_count"] + refutation["new_candidate_count"]) / total_edges

        return change_rate <= self.convergence_threshold
