# -*- coding: utf-8 -*-
"""Gemini LLM ì–´ëŒ‘í„° â€” ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œìš©.

WhyLabì˜ ê·œì¹™ ê¸°ë°˜ ì¦ê±° ìˆ˜ì§‘ ìœ„ì— LLM ìžì—°ì–´ ì¶”ë¡  ë ˆì´ì–´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
ê¸°ì¡´ AdvocateAgent/CriticAgentì˜ êµ¬ì¡°í™”ëœ Evidenceë¥¼ LLMì´ í•´ì„í•˜ì—¬
ë” í’ë¶€í•˜ê³  ë§¥ë½ì ì¸ í† ë¡ ì„ ìƒì„±í•©ë‹ˆë‹¤.

í™˜ê²½ ë³€ìˆ˜:
    GEMINI_API_KEY ë˜ëŠ” GOOGLE_API_KEY: Gemini API ì¸ì¦ í‚¤

ì‚¬ìš©ë²•:
    from engine.agents.llm_adapter import LLMDebateAdapter
    adapter = LLMDebateAdapter()
    result = adapter.run_debate(pipeline_results)
"""

from __future__ import annotations

import json
import logging
import os
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# LLM ëª¨ë¸ ì‹ë³„ìž (í™˜ê²½ ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
DEFAULT_MODEL = os.environ.get("WHYLAB_LLM_MODEL", "gemini-2.0-flash")

# ìµœëŒ€ í† ë¡  ë¼ìš´ë“œ
MAX_DEBATE_ROUNDS = int(os.environ.get("WHYLAB_DEBATE_ROUNDS", "2"))


@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ êµ¬ì¡°ì²´."""
    role: str  # "advocate" | "critic" | "judge"
    content: str  # ìžì—°ì–´ ì‘ë‹µ
    reasoning: str  # ì¶”ë¡  ê³¼ì •
    raw_evidence_count: int  # ê¸°ë°˜ ì¦ê±° ìˆ˜


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Gemini í´ë¼ì´ì–¸íŠ¸ ëž˜í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class GeminiClient:
    """Gemini API ìµœì†Œ ëž˜í¼. ìž¥ì•  ì‹œ graceful fallback."""

    def __init__(self, model_name: str = DEFAULT_MODEL):
        self.model_name = model_name
        self.model = None
        self._initialized = False

    def _ensure_init(self) -> bool:
        """ì§€ì—° ì´ˆê¸°í™”. API í‚¤ ì—†ìœ¼ë©´ False ë°˜í™˜."""
        if self._initialized:
            return self.model is not None
        self._initialized = True

        api_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            logger.warning("[LLM] GEMINI_API_KEY ë¯¸ì„¤ì • â†’ ê·œì¹™ ê¸°ë°˜ Fallback ëª¨ë“œ")
            return False

        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(self.model_name)
            logger.info("[LLM] Gemini ì´ˆê¸°í™” ì„±ê³µ (ëª¨ë¸: %s)", self.model_name)
            return True
        except Exception as e:
            logger.warning("[LLM] Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: %s â†’ Fallback ëª¨ë“œ", e)
            return False

    def generate(self, prompt: str, max_tokens: int = 2048) -> Optional[str]:
        """í”„ë¡¬í”„íŠ¸ë¥¼ Geminiì— ì „ì†¡í•˜ê³  í…ìŠ¤íŠ¸ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            prompt: ìž…ë ¥ í”„ë¡¬í”„íŠ¸.
            max_tokens: ìµœëŒ€ ì¶œë ¥ í† í° ìˆ˜.

        Returns:
            ì‘ë‹µ í…ìŠ¤íŠ¸. ì‹¤íŒ¨ ì‹œ None.
        """
        if not self._ensure_init():
            return None

        try:
            response = self.model.generate_content(
                prompt,
                generation_config={
                    "max_output_tokens": max_tokens,
                    "temperature": 0.3,  # ë¶„ì„ì  íƒœë„ ìœ ì§€
                },
            )
            return response.text
        except Exception as e:
            logger.warning("[LLM] Gemini í˜¸ì¶œ ì‹¤íŒ¨: %s", e)
            return None

    @property
    def is_available(self) -> bool:
        """LLM API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€."""
        return self._ensure_init()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ADVOCATE_PROMPT = """ë‹¹ì‹ ì€ WhyLab ì¸ê³¼ì¶”ë¡  ì‹œìŠ¤í…œì˜ **Growth Hacker(ì˜¹í˜¸ìž)**ìž…ë‹ˆë‹¤.
ì•„ëž˜ ì¦ê±°ë¥¼ ê²€í† í•˜ê³ , ì¸ê³¼ ê´€ê³„ê°€ ì¡´ìž¬í•œë‹¤ê³  ì£¼ìž¥í•˜ëŠ” ë…¼ë³€ì„ ìž‘ì„±í•˜ì„¸ìš”.

## ë¶„ì„ ë§¥ë½
- ì²˜ì¹˜(Treatment): {treatment}
- ê²°ê³¼(Outcome): {outcome}
- ATE: {ate}

## ìˆ˜ì§‘ëœ ê¸ì • ì¦ê±°
{evidence_summary}

## ì§€ì‹œì‚¬í•­
1. ì¦ê±°ë¥¼ ì¢…í•©í•˜ì—¬ ì¸ê³¼ ê´€ê³„ë¥¼ ì˜¹í˜¸í•˜ëŠ” **í•µì‹¬ ë…¼ë³€ 3ê°€ì§€**ë¥¼ ìž‘ì„±í•˜ì„¸ìš”.
2. ê° ë…¼ë³€ì— ëŒ€í•´ ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íšŒ(Revenue, Growth, ROI)ë¥¼ ì—°ê²°í•˜ì„¸ìš”.
3. ë§ˆì§€ë§‰ì— **ì¶”ì²œ ì•¡ì…˜**(ë°°í¬/í™•ìž¥/íƒ€ê²ŸíŒ…)ì„ í•œ ì¤„ë¡œ ì œì‹œí•˜ì„¸ìš”.
4. í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ë˜, ì „ë¬¸ì ì´ì§€ë§Œ ëª…í™•í•˜ê²Œ ì“°ì„¸ìš”.
5. ì´ 300ìž ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìž‘ì„±í•˜ì„¸ìš”."""

CRITIC_PROMPT = """ë‹¹ì‹ ì€ WhyLab ì¸ê³¼ì¶”ë¡  ì‹œìŠ¤í…œì˜ **Risk Manager(ë¹„íŒìž)**ìž…ë‹ˆë‹¤.
ì•„ëž˜ ì¦ê±°ë¥¼ ê²€í† í•˜ê³ , ì¸ê³¼ ê´€ê³„ì˜ ì•½ì ê³¼ ë¦¬ìŠ¤í¬ë¥¼ ì§€ì í•˜ì„¸ìš”.

## ë¶„ì„ ë§¥ë½
- ì²˜ì¹˜(Treatment): {treatment}
- ê²°ê³¼(Outcome): {outcome}
- ATE: {ate}

## ìˆ˜ì§‘ëœ ë¶€ì • ì¦ê±° (ê³µê²© ë²¡í„°)
{evidence_summary}

## ì§€ì‹œì‚¬í•­
1. ì¦ê±°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ê³¼ íŒë‹¨ì˜ **í•µì‹¬ ë¦¬ìŠ¤í¬ 3ê°€ì§€**ë¥¼ ì§€ì í•˜ì„¸ìš”.
2. ê° ë¦¬ìŠ¤í¬ì— ëŒ€í•´ ë¹„ì¦ˆë‹ˆìŠ¤ ìœ„í—˜(Loss, Churn, Compliance)ì„ ì—°ê²°í•˜ì„¸ìš”.
3. ë§ˆì§€ë§‰ì— **ìš”êµ¬ ì‚¬í•­**(ì¶”ê°€ ê²€ì¦/í‘œë³¸ í™•ëŒ€/ëŒ€ì•ˆ ë°©ë²•)ì„ í•œ ì¤„ë¡œ ì œì‹œí•˜ì„¸ìš”.
4. í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ë˜, ì „ë¬¸ì ì´ì§€ë§Œ ëª…í™•í•˜ê²Œ ì“°ì„¸ìš”.
5. ì´ 300ìž ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìž‘ì„±í•˜ì„¸ìš”."""

JUDGE_PROMPT = """ë‹¹ì‹ ì€ WhyLab ì¸ê³¼ì¶”ë¡  ì‹œìŠ¤í…œì˜ **Product Owner(íŒì‚¬)**ìž…ë‹ˆë‹¤.
ì–‘ì¸¡ ì—ì´ì „íŠ¸ì˜ ê³µë°©ì„ ê²€í† í•˜ê³  ìµœì¢… íŒê²°ì„ ë‚´ë¦¬ì„¸ìš”.

## ë¶„ì„ ë§¥ë½
- ì²˜ì¹˜(Treatment): {treatment}
- ê²°ê³¼(Outcome): {outcome}
- ATE: {ate}
- ìŠ¤ì½”ì–´: ì˜¹í˜¸ì¸¡ {pro_score:.2f} vs ë¹„íŒì¸¡ {con_score:.2f}
- í™•ì‹ ë„: {confidence:.1%}

## Growth Hackerì˜ ì£¼ìž¥
{advocate_argument}

## Risk Managerì˜ ë°˜ë¡ 
{critic_argument}

## íŒê²° ê°€ì´ë“œ
- í™•ì‹ ë„ â‰¥ 70%: CAUSAL (ì¸ê³¼ ê´€ê³„ ì¸ì •)
- í™•ì‹ ë„ â‰¤ 30%: NOT_CAUSAL (ì¸ê³¼ ê´€ê³„ ê¸°ê°)
- ê·¸ ì™¸: UNCERTAIN (ì¶”ê°€ ê²€ì¦ í•„ìš”)

## ì§€ì‹œì‚¬í•­
1. ì–‘ì¸¡ ì£¼ìž¥ì˜ í•µì‹¬ì„ í•œ ë¬¸ìž¥ì”© ìš”ì•½í•˜ì„¸ìš”.
2. ìµœì¢… íŒê²°(CAUSAL/NOT_CAUSAL/UNCERTAIN)ê³¼ ê·¼ê±°ë¥¼ ë°ížˆì„¸ìš”.
3. **ë¹„ì¦ˆë‹ˆìŠ¤ ì•¡ì…˜ ì•„ì´í…œ**ì„ êµ¬ì²´ì ìœ¼ë¡œ í•œ ì¤„ ìž‘ì„±í•˜ì„¸ìš”.
   - CAUSALì´ë©´: ë°°í¬ ì „ëžµ (Rollout %, íƒ€ê²Ÿ ì„¸ê·¸ë¨¼íŠ¸)
   - NOT_CAUSALì´ë©´: ë¦¬ì†ŒìŠ¤ íšŒìˆ˜ ë˜ëŠ” ëŒ€ì•ˆ ì‹¤í—˜ ì œì•ˆ
   - UNCERTAINì´ë©´: A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„ ì œì•ˆ
4. í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ì„¸ìš”.
5. ì´ 400ìž ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìž‘ì„±í•˜ì„¸ìš”."""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM Debate ì–´ëŒ‘í„°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class LLMDebateAdapter:
    """ê·œì¹™ ê¸°ë°˜ ì¦ê±° + LLM ìžì—°ì–´ í† ë¡  í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ.

    ê¸°ì¡´ Advocate/Critic/Judgeì˜ êµ¬ì¡°í™”ëœ ì¦ê±° ìˆ˜ì§‘ì€ ìœ ì§€í•˜ë©´ì„œ,
    LLMì´ ì¦ê±°ë¥¼ í•´ì„í•˜ì—¬ ìžì—°ì–´ í† ë¡ ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    LLM ìž¥ì•  ì‹œ ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ìžë™ Fallbackí•©ë‹ˆë‹¤.
    """

    def __init__(self, model_name: str = DEFAULT_MODEL):
        self.client = GeminiClient(model_name)
        self.responses: List[LLMResponse] = []

    def format_evidence(self, evidence_list: list) -> str:
        """Evidence ë¦¬ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜."""
        lines = []
        for i, e in enumerate(evidence_list, 1):
            impact = f" â†’ {e.business_impact}" if e.business_impact else ""
            lines.append(
                f"{i}. [{e.evidence_type}] {e.claim} "
                f"(ê°•ë„: {e.strength:.2f}){impact}"
            )
        return "\n".join(lines) if lines else "(ìˆ˜ì§‘ëœ ì¦ê±° ì—†ìŒ)"

    def generate_advocate_argument(
        self,
        evidence: list,
        context: Dict[str, Any],
    ) -> str:
        """Growth Hackerì˜ ì˜¹í˜¸ ë…¼ë³€ì„ LLMìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        prompt = ADVOCATE_PROMPT.format(
            treatment=context.get("treatment_col", "T"),
            outcome=context.get("outcome_col", "Y"),
            ate=context.get("ate_value", "N/A"),
            evidence_summary=self.format_evidence(evidence),
        )

        response = self.client.generate(prompt)
        if response:
            self.responses.append(LLMResponse(
                role="advocate",
                content=response,
                reasoning="Gemini LLM ê¸°ë°˜ ë…¼ë³€ ìƒì„±",
                raw_evidence_count=len(evidence),
            ))
            return response

        # Fallback: ê·œì¹™ ê¸°ë°˜ ìš”ì•½
        return self._fallback_advocate(evidence)

    def generate_critic_argument(
        self,
        evidence: list,
        context: Dict[str, Any],
    ) -> str:
        """Risk Managerì˜ ë¹„íŒ ë…¼ë³€ì„ LLMìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        prompt = CRITIC_PROMPT.format(
            treatment=context.get("treatment_col", "T"),
            outcome=context.get("outcome_col", "Y"),
            ate=context.get("ate_value", "N/A"),
            evidence_summary=self.format_evidence(evidence),
        )

        response = self.client.generate(prompt)
        if response:
            self.responses.append(LLMResponse(
                role="critic",
                content=response,
                reasoning="Gemini LLM ê¸°ë°˜ ë°˜ë¡  ìƒì„±",
                raw_evidence_count=len(evidence),
            ))
            return response

        return self._fallback_critic(evidence)

    def generate_verdict(
        self,
        advocate_arg: str,
        critic_arg: str,
        verdict_data: dict,
        context: Dict[str, Any],
    ) -> str:
        """Product Ownerì˜ ìµœì¢… íŒê²°ë¬¸ì„ LLMìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        prompt = JUDGE_PROMPT.format(
            treatment=context.get("treatment_col", "T"),
            outcome=context.get("outcome_col", "Y"),
            ate=context.get("ate_value", "N/A"),
            pro_score=verdict_data.get("pro_score", 0),
            con_score=verdict_data.get("con_score", 0),
            confidence=verdict_data.get("confidence", 0),
            advocate_argument=advocate_arg,
            critic_argument=critic_arg,
        )

        response = self.client.generate(prompt, max_tokens=1024)
        if response:
            self.responses.append(LLMResponse(
                role="judge",
                content=response,
                reasoning="Gemini LLM ê¸°ë°˜ ìµœì¢… íŒê²°",
                raw_evidence_count=0,
            ))
            return response

        return self._fallback_verdict(verdict_data)

    # â”€â”€ Fallback ë©”ì„œë“œ â”€â”€

    def _fallback_advocate(self, evidence: list) -> str:
        """LLM ìž¥ì•  ì‹œ ê·œì¹™ ê¸°ë°˜ ì˜¹í˜¸ ìš”ì•½."""
        if not evidence:
            return "ìˆ˜ì§‘ëœ ì˜¹í˜¸ ì¦ê±°ê°€ ì—†ìŠµë‹ˆë‹¤."
        top = sorted(evidence, key=lambda e: e.strength, reverse=True)[:3]
        lines = [f"ðŸ“— **Growth Hacker í•µì‹¬ ì£¼ìž¥:**"]
        for e in top:
            lines.append(f"  â€¢ {e.claim}")
            if e.business_impact:
                lines.append(f"    â†’ {e.business_impact}")
        return "\n".join(lines)

    def _fallback_critic(self, evidence: list) -> str:
        """LLM ìž¥ì•  ì‹œ ê·œì¹™ ê¸°ë°˜ ë¹„íŒ ìš”ì•½."""
        if not evidence:
            return "ìˆ˜ì§‘ëœ ë¹„íŒ ì¦ê±°ê°€ ì—†ìŠµë‹ˆë‹¤."
        top = sorted(evidence, key=lambda e: e.strength, reverse=True)[:3]
        lines = [f"ðŸ“• **Risk Manager í•µì‹¬ ë¦¬ìŠ¤í¬:**"]
        for e in top:
            lines.append(f"  â€¢ {e.claim}")
            if e.business_impact:
                lines.append(f"    âš ï¸ {e.business_impact}")
        return "\n".join(lines)

    def _fallback_verdict(self, verdict_data: dict) -> str:
        """LLM ìž¥ì•  ì‹œ ê·œì¹™ ê¸°ë°˜ íŒê²° ìš”ì•½."""
        verdict = verdict_data.get("verdict", "UNCERTAIN")
        confidence = verdict_data.get("confidence", 0)
        recommendation = verdict_data.get("recommendation", "")
        return (
            f"âš–ï¸ **íŒê²°: {verdict}** (í™•ì‹ ë„: {confidence:.1%})\n"
            f"{recommendation}"
        )

    @property
    def is_llm_active(self) -> bool:
        """LLMì´ ì‹¤ì œë¡œ í™œì„±í™”ë˜ì–´ ìžˆëŠ”ì§€."""
        return self.client.is_available

    def get_debate_summary(self) -> Dict[str, Any]:
        """í† ë¡  ê²°ê³¼ ìš”ì•½ (ëŒ€ì‹œë³´ë“œ JSON ë‚´ë³´ë‚´ê¸°ìš©)."""
        return {
            "llm_active": self.is_llm_active,
            "model": self.client.model_name if self.is_llm_active else "rule_based",
            "rounds": len([r for r in self.responses if r.role == "judge"]),
            "responses": [
                {
                    "role": r.role,
                    "content": r.content,
                    "evidence_count": r.raw_evidence_count,
                }
                for r in self.responses
            ],
        }
