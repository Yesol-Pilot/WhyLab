# -*- coding: utf-8 -*-
"""QuasiExperimentalCell â€” IV/DiD/RDD ì¤€ì‹¤í—˜ ë°©ë²•ë¡ .

ê´€ì°° ë°ì´í„°ì—ì„œ ë¯¸ê´€ì¸¡ êµë€ ë³€ìˆ˜ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ
ì„¸ ê°€ì§€ ì¤€ì‹¤í—˜(Quasi-Experimental) ë°©ë²•ë¡ ì„ ì œê³µí•©ë‹ˆë‹¤.

- **IV (Instrumental Variable)**: 2SLS ì¶”ì •, ì•½í•œ ë„êµ¬ ê²€ì •
- **DiD (Difference-in-Differences)**: ë³‘ë ¬ íŠ¸ë Œë“œ ê²€ì •
- **RDD (Regression Discontinuity)**: Sharp RDD + ëŒ€ì—­í­ ìµœì í™”

Phase 10-1: ë°©ë²•ë¡  í™•ì¥.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from engine.cells.base_cell import BaseCell
from engine.config import WhyLabConfig

logger = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class IVResult:
    """ë„êµ¬ ë³€ìˆ˜(IV) ì¶”ì • ê²°ê³¼."""
    method: str = "2SLS"
    ate: float = 0.0
    se: float = 0.0
    ci_lower: float = 0.0
    ci_upper: float = 0.0
    f_stat: float = 0.0  # 1ë‹¨ê³„ F-í†µê³„ëŸ‰ (> 10ì´ë©´ ê°•í•œ ë„êµ¬)
    weak_instrument: bool = True
    instrument_col: str = ""
    interpretation: str = ""


@dataclass
class DiDResult:
    """ì´ì¤‘ì°¨ë¶„ë²•(DiD) ì¶”ì • ê²°ê³¼."""
    method: str = "DiD"
    ate: float = 0.0
    se: float = 0.0
    ci_lower: float = 0.0
    ci_upper: float = 0.0
    parallel_trend_pvalue: float = 0.0
    parallel_trend_holds: bool = False
    n_treated: int = 0
    n_control: int = 0
    interpretation: str = ""


@dataclass
class RDDResult:
    """íšŒê·€ ë‹¨ì ˆ ì„¤ê³„(RDD) ì¶”ì • ê²°ê³¼."""
    method: str = "Sharp_RDD"
    ate: float = 0.0
    se: float = 0.0
    ci_lower: float = 0.0
    ci_upper: float = 0.0
    cutoff: float = 0.0
    bandwidth: float = 0.0
    n_left: int = 0
    n_right: int = 0
    interpretation: str = ""


class QuasiExperimentalCell(BaseCell):
    """IV/DiD/RDD ì¤€ì‹¤í—˜ ë°©ë²•ë¡  ì…€.

    ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ì ìš© ê°€ëŠ¥í•œ ì¤€ì‹¤í—˜ ë°©ë²•ë¡ ì„ ìë™ìœ¼ë¡œ
    ì‹¤í–‰í•˜ê³ , ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    """

    def __init__(self, config: WhyLabConfig) -> None:
        super().__init__(name="quasi_experimental_cell", config=config)

    def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """ì¤€ì‹¤í—˜ ë°©ë²•ë¡  ì‹¤í–‰.

        Args:
            inputs: íŒŒì´í”„ë¼ì¸ ì»¨í…ìŠ¤íŠ¸.

        Returns:
            ì¤€ì‹¤í—˜ ë¶„ì„ ê²°ê³¼ ì¶”ê°€ëœ inputs.
        """
        df = inputs.get("dataframe")
        if df is None:
            self.logger.warning("ë°ì´í„°í”„ë ˆì„ ì—†ìŒ â†’ QuasiExperimental ê±´ë„ˆëœ€")
            return inputs

        treatment_col = inputs.get("treatment_col", "treatment")
        outcome_col = inputs.get("outcome_col", "outcome")
        feature_names = inputs.get("feature_names", [])

        results = {}

        # IV ì¶”ì • ì‹œë„
        instrument_col = inputs.get("instrument_col")
        if instrument_col and instrument_col in df.columns:
            self.logger.info("ğŸ”§ IV(2SLS) ì¶”ì • ì‹œì‘ (ë„êµ¬: %s)", instrument_col)
            results["iv"] = self._estimate_iv(
                df, treatment_col, outcome_col, instrument_col, feature_names
            )
        else:
            # ì ì¬ ë„êµ¬ ë³€ìˆ˜ ìë™ íƒìƒ‰
            candidate = self._find_instrument_candidate(
                df, treatment_col, outcome_col, feature_names
            )
            if candidate:
                self.logger.info("ğŸ” ì ì¬ ë„êµ¬ ë³€ìˆ˜ ë°œê²¬: %s â†’ IV ì¶”ì • ì‹œë„", candidate)
                results["iv"] = self._estimate_iv(
                    df, treatment_col, outcome_col, candidate, feature_names
                )

        # DiD ì¶”ì • ì‹œë„
        time_col = inputs.get("time_col")
        group_col = inputs.get("group_col")
        if time_col and group_col:
            self.logger.info("ğŸ“Š DiD ì¶”ì • ì‹œì‘ (ì‹œê°„: %s, ê·¸ë£¹: %s)", time_col, group_col)
            results["did"] = self._estimate_did(
                df, outcome_col, time_col, group_col, feature_names
            )
        elif treatment_col in df.columns and df[treatment_col].nunique() == 2:
            # ì´ì§„ ì²˜ì¹˜ â†’ ê°„ì´ DiD ì‹œë®¬ë ˆì´ì…˜
            results["did"] = self._simulate_did(
                df, treatment_col, outcome_col, feature_names
            )

        # RDD ì¶”ì • ì‹œë„
        running_col = inputs.get("running_col")
        cutoff = inputs.get("rdd_cutoff")
        if running_col and cutoff is not None:
            self.logger.info("ğŸ“ RDD ì¶”ì • ì‹œì‘ (ì ˆë‹¨: %s=%.2f)", running_col, cutoff)
            results["rdd"] = self._estimate_rdd(
                df, outcome_col, running_col, cutoff
            )
        elif len(feature_names) > 0:
            # ì—°ì† ë³€ìˆ˜ì—ì„œ RDD í›„ë³´ ìë™ íƒìƒ‰
            rdd_candidate = self._find_rdd_candidate(
                df, treatment_col, feature_names
            )
            if rdd_candidate:
                col, cut = rdd_candidate
                self.logger.info("ğŸ” RDD í›„ë³´ ë°œê²¬: %s (ì ˆë‹¨=%.2f)", col, cut)
                results["rdd"] = self._estimate_rdd(df, outcome_col, col, cut)

        # ê²°ê³¼ ì¢…í•©
        if results:
            methods_used = list(results.keys())
            self.logger.info(
                "âœ… ì¤€ì‹¤í—˜ ë¶„ì„ ì™„ë£Œ: %s", ", ".join(m.upper() for m in methods_used)
            )
        else:
            self.logger.info("â„¹ï¸ ì ìš© ê°€ëŠ¥í•œ ì¤€ì‹¤í—˜ ë°©ë²•ë¡  ì—†ìŒ (ê±´ë„ˆëœ€)")

        return {
            **inputs,
            "quasi_experimental": self._serialize_results(results),
        }

    # â”€â”€ IV (Instrumental Variable) â”€â”€

    def _estimate_iv(
        self,
        df: pd.DataFrame,
        treatment_col: str,
        outcome_col: str,
        instrument_col: str,
        covariates: List[str],
    ) -> IVResult:
        """2SLS(Two-Stage Least Squares) IV ì¶”ì •."""
        from sklearn.linear_model import LinearRegression

        # ìœ íš¨ ì»¬ëŸ¼ í•„í„°
        cov_cols = [c for c in covariates if c in df.columns and c != instrument_col]
        clean_df = df[[treatment_col, outcome_col, instrument_col] + cov_cols].dropna()

        Z = clean_df[instrument_col].values.reshape(-1, 1)
        Y = clean_df[outcome_col].values
        T = clean_df[treatment_col].values
        X = clean_df[cov_cols].values if cov_cols else np.ones((len(clean_df), 1))

        # 1ë‹¨ê³„: T ~ Z + X
        ZX = np.hstack([Z, X])
        stage1 = LinearRegression().fit(ZX, T)
        T_hat = stage1.predict(ZX)

        # 1ë‹¨ê³„ F-í†µê³„ëŸ‰ (ì•½í•œ ë„êµ¬ ê²€ì •)
        ss_res = np.sum((T - T_hat) ** 2)
        ss_tot = np.sum((T - T.mean()) ** 2)
        r2 = 1 - ss_res / (ss_tot + 1e-10)
        n, k = ZX.shape
        f_stat = (r2 / max(1, k)) / ((1 - r2) / max(1, n - k - 1) + 1e-10)

        # 2ë‹¨ê³„: Y ~ T_hat + X
        TX = np.hstack([T_hat.reshape(-1, 1), X])
        stage2 = LinearRegression().fit(TX, Y)
        ate = float(stage2.coef_[0])

        # í‘œì¤€ ì˜¤ì°¨ (ê·¼ì‚¬)
        residuals = Y - stage2.predict(TX)
        n_obs = len(Y)
        se = float(np.std(residuals) / np.sqrt(n_obs)) if n_obs > 0 else 0.0
        ci_lower = ate - 1.96 * se
        ci_upper = ate + 1.96 * se

        weak = f_stat < 10
        interp = (
            f"IV(2SLS) ì¶”ì • ATE = {ate:.4f} [{ci_lower:.4f}, {ci_upper:.4f}]. "
            f"1ë‹¨ê³„ F = {f_stat:.1f} ({'âš ï¸ ì•½í•œ ë„êµ¬' if weak else 'âœ… ê°•í•œ ë„êµ¬'})."
        )

        self.logger.info("   IV: ATE=%.4f, F=%.1f, ì•½í•œ ë„êµ¬=%s", ate, f_stat, weak)

        return IVResult(
            ate=ate, se=se, ci_lower=ci_lower, ci_upper=ci_upper,
            f_stat=float(f_stat), weak_instrument=weak,
            instrument_col=instrument_col, interpretation=interp,
        )

    def _find_instrument_candidate(
        self, df: pd.DataFrame, treatment_col: str,
        outcome_col: str, features: List[str],
    ) -> Optional[str]:
        """ì ì¬ ë„êµ¬ ë³€ìˆ˜ë¥¼ ìë™ íƒìƒ‰í•©ë‹ˆë‹¤.

        ë„êµ¬ ë³€ìˆ˜ ì¡°ê±´: Treatmentê³¼ ìƒê´€ ë†’ê³ , Outcomeê³¼ ì§ì ‘ ìƒê´€ ë‚®ìŒ.
        """
        best_col = None
        best_score = 0
        for col in features:
            if col == treatment_col or col == outcome_col:
                continue
            if col not in df.columns:
                continue
            try:
                corr_t = abs(df[col].corr(df[treatment_col]))
                corr_y = abs(df[col].corr(df[outcome_col]))
                # ë„êµ¬ ì¡°ê±´: Tì™€ ìƒê´€ ë†’ê³  (>0.3), Yì™€ ì§ì ‘ ìƒê´€ ë‚®ìŒ (<0.15)
                if corr_t > 0.3 and corr_y < 0.15:
                    score = corr_t - corr_y
                    if score > best_score:
                        best_score = score
                        best_col = col
            except Exception:
                continue
        return best_col

    # â”€â”€ DiD (Difference-in-Differences) â”€â”€

    def _estimate_did(
        self,
        df: pd.DataFrame,
        outcome_col: str,
        time_col: str,
        group_col: str,
        covariates: List[str],
    ) -> DiDResult:
        """ì´ì¤‘ì°¨ë¶„ë²•(DiD) ì¶”ì •."""
        # ì‚¬ì „/ì‚¬í›„ ê¸°ê°„ êµ¬ë¶„
        times = sorted(df[time_col].unique())
        if len(times) < 2:
            return DiDResult(interpretation="ì‹œê°„ ê¸°ê°„ì´ 2ê°œ ë¯¸ë§Œ")

        pre_time = times[0]
        post_time = times[-1]

        # ê·¸ë£¹ë³„ í‰ê·  ê³„ì‚°
        groups = df[group_col].unique()
        if len(groups) < 2:
            return DiDResult(interpretation="ê·¸ë£¹ì´ 2ê°œ ë¯¸ë§Œ")

        treated_group = groups[0]
        control_group = groups[1]

        pre_treated = df[(df[time_col] == pre_time) & (df[group_col] == treated_group)][outcome_col]
        post_treated = df[(df[time_col] == post_time) & (df[group_col] == treated_group)][outcome_col]
        pre_control = df[(df[time_col] == pre_time) & (df[group_col] == control_group)][outcome_col]
        post_control = df[(df[time_col] == post_time) & (df[group_col] == control_group)][outcome_col]

        # DiD ì¶”ì •
        diff_treated = post_treated.mean() - pre_treated.mean()
        diff_control = post_control.mean() - pre_control.mean()
        ate = float(diff_treated - diff_control)

        # í‘œì¤€ ì˜¤ì°¨ (í’€ë§ëœ ë¶„ì‚°)
        n_t = len(post_treated) + len(pre_treated)
        n_c = len(post_control) + len(pre_control)
        pooled_var = (post_treated.var() + pre_treated.var()) / n_t + \
                     (post_control.var() + pre_control.var()) / n_c
        se = float(np.sqrt(pooled_var + 1e-10))
        ci_lower = ate - 1.96 * se
        ci_upper = ate + 1.96 * se

        # ë³‘ë ¬ íŠ¸ë Œë“œ ê²€ì • (ê°„ì´: ì‚¬ì „ ê¸°ê°„ ì°¨ì´ê°€ ìœ ì˜í•˜ì§€ ì•Šìœ¼ë©´ ë³‘ë ¬)
        pre_diff = abs(pre_treated.mean() - pre_control.mean())
        pre_se = float(np.sqrt(pre_treated.var() / len(pre_treated) + pre_control.var() / len(pre_control) + 1e-10))
        parallel_z = pre_diff / (pre_se + 1e-10)
        from scipy import stats as scipy_stats
        parallel_pvalue = float(2 * (1 - scipy_stats.norm.cdf(parallel_z)))
        parallel_holds = parallel_pvalue > 0.05

        interp = (
            f"DiD ATE = {ate:.4f} [{ci_lower:.4f}, {ci_upper:.4f}]. "
            f"ë³‘ë ¬ íŠ¸ë Œë“œ {'âœ… ì„±ë¦½' if parallel_holds else 'âš ï¸ ë¯¸ì„±ë¦½'} (p={parallel_pvalue:.3f})."
        )

        self.logger.info("   DiD: ATE=%.4f, ë³‘ë ¬íŠ¸ë Œë“œ=%s", ate, parallel_holds)

        return DiDResult(
            ate=ate, se=se, ci_lower=ci_lower, ci_upper=ci_upper,
            parallel_trend_pvalue=parallel_pvalue,
            parallel_trend_holds=parallel_holds,
            n_treated=n_t, n_control=n_c, interpretation=interp,
        )

    def _simulate_did(
        self,
        df: pd.DataFrame,
        treatment_col: str,
        outcome_col: str,
        covariates: List[str],
    ) -> DiDResult:
        """ì´ì§„ ì²˜ì¹˜ì—ì„œ ê°„ì´ DiD ì‹œë®¬ë ˆì´ì…˜.

        ì‹œê°„ ë³€ìˆ˜ ì—†ì´, ì²˜ì¹˜/í†µì œ ê·¸ë£¹ ê°„ ì°¨ì´ë¥¼ DiDì²˜ëŸ¼ ì¶”ì •í•©ë‹ˆë‹¤.
        """
        treated = df[df[treatment_col] == 1][outcome_col]
        control = df[df[treatment_col] == 0][outcome_col]

        if len(treated) == 0 or len(control) == 0:
            return DiDResult(interpretation="ì²˜ì¹˜/í†µì œ ê·¸ë£¹ í¬ê¸° ë¶€ì¡±")

        ate = float(treated.mean() - control.mean())
        se = float(np.sqrt(treated.var() / len(treated) + control.var() / len(control) + 1e-10))

        return DiDResult(
            ate=ate, se=se,
            ci_lower=ate - 1.96 * se, ci_upper=ate + 1.96 * se,
            parallel_trend_pvalue=1.0,  # ì‹œê°„ ì—†ìœ¼ë¯€ë¡œ ê²€ì • ë¶ˆê°€
            parallel_trend_holds=True,
            n_treated=len(treated), n_control=len(control),
            interpretation=f"ê°„ì´ DiD: ATE={ate:.4f} (ì‹œê°„ ë³€ìˆ˜ ì—†ì´ ê·¸ë£¹ ë¹„êµ)",
        )

    # â”€â”€ RDD (Regression Discontinuity Design) â”€â”€

    def _estimate_rdd(
        self,
        df: pd.DataFrame,
        outcome_col: str,
        running_col: str,
        cutoff: float,
        bandwidth: Optional[float] = None,
    ) -> RDDResult:
        """Sharp RDD ì¶”ì • (êµ­ì†Œ ì„ í˜• íšŒê·€)."""
        from sklearn.linear_model import LinearRegression

        running = df[running_col].values
        outcome = df[outcome_col].values

        # ëŒ€ì—­í­ ìë™ ê²°ì • (IK rule of thumb)
        if bandwidth is None:
            bandwidth = float(1.06 * np.std(running) * len(running) ** (-0.2))

        # ëŒ€ì—­í­ ë‚´ ê´€ì¸¡ì¹˜ ì„ íƒ
        mask = np.abs(running - cutoff) <= bandwidth
        local_df = df[mask].copy()

        if len(local_df) < 10:
            return RDDResult(
                cutoff=cutoff, bandwidth=bandwidth,
                interpretation="ëŒ€ì—­í­ ë‚´ ê´€ì¸¡ì¹˜ ë¶€ì¡± (n < 10)",
            )

        local_running = local_df[running_col].values - cutoff  # ì¤‘ì‹¬í™”
        local_outcome = local_df[outcome_col].values
        local_treatment = (local_running >= 0).astype(float)

        # êµ­ì†Œ ì„ í˜• íšŒê·€: Y ~ D + (X-c) + D*(X-c)
        X_rdd = np.column_stack([
            local_treatment,
            local_running,
            local_treatment * local_running,
        ])
        reg = LinearRegression().fit(X_rdd, local_outcome)
        ate = float(reg.coef_[0])

        # í‘œì¤€ ì˜¤ì°¨
        residuals = local_outcome - reg.predict(X_rdd)
        se = float(np.std(residuals) / np.sqrt(len(local_outcome)))
        ci_lower = ate - 1.96 * se
        ci_upper = ate + 1.96 * se

        n_left = int(np.sum(local_treatment == 0))
        n_right = int(np.sum(local_treatment == 1))

        interp = (
            f"RDD ATE = {ate:.4f} [{ci_lower:.4f}, {ci_upper:.4f}] "
            f"(ì ˆë‹¨={cutoff:.2f}, ëŒ€ì—­í­={bandwidth:.2f}, "
            f"ì¢Œ={n_left}, ìš°={n_right})."
        )

        self.logger.info("   RDD: ATE=%.4f, cutoff=%.2f, bw=%.2f", ate, cutoff, bandwidth)

        return RDDResult(
            ate=ate, se=se, ci_lower=ci_lower, ci_upper=ci_upper,
            cutoff=cutoff, bandwidth=bandwidth,
            n_left=n_left, n_right=n_right, interpretation=interp,
        )

    def _find_rdd_candidate(
        self,
        df: pd.DataFrame,
        treatment_col: str,
        features: List[str],
    ) -> Optional[Tuple[str, float]]:
        """RDD ì ìš© ê°€ëŠ¥í•œ ë³€ìˆ˜ì™€ ì ˆë‹¨ì ì„ ìë™ íƒìƒ‰í•©ë‹ˆë‹¤.

        ì²˜ì¹˜ í™•ë¥ ì´ ê¸‰ë³€í•˜ëŠ” ì§€ì ì´ ìˆìœ¼ë©´ RDD í›„ë³´ë¡œ íŒë‹¨.
        """
        if treatment_col not in df.columns or df[treatment_col].nunique() != 2:
            return None

        for col in features:
            if col not in df.columns or df[col].nunique() < 10:
                continue
            try:
                # 10ë¶„ìœ„ë³„ ì²˜ì¹˜ ë¹„ìœ¨ ê³„ì‚°
                quantiles = pd.qcut(df[col], 10, duplicates='drop')
                group_means = df.groupby(quantiles, observed=True)[treatment_col].mean()

                # ì¸ì ‘ ë¶„ìœ„ ê°„ ìµœëŒ€ ì°¨ì´ ì°¾ê¸°
                diffs = group_means.diff().abs()
                max_diff = diffs.max()

                if max_diff > 0.3:  # ì²˜ì¹˜ìœ¨ 30%p ì´ìƒ ê¸‰ë³€
                    max_idx = diffs.idxmax()
                    # ì ˆë‹¨ì  = í•´ë‹¹ ë¶„ìœ„ ê²½ê³„
                    cutoff = float(df[col][quantiles == max_idx].mean())
                    return (col, cutoff)
            except Exception:
                continue
        return None

    # â”€â”€ ì§ë ¬í™” â”€â”€

    def _serialize_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ê²°ê³¼ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜."""
        serialized = {}
        for method, result in results.items():
            if hasattr(result, '__dict__'):
                serialized[method] = {
                    k: v for k, v in result.__dict__.items()
                    if not k.startswith('_')
                }
            else:
                serialized[method] = result
        return serialized
