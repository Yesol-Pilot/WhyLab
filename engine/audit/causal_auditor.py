# -*- coding: utf-8 -*-
"""Causal Auditor â€” ìë™ ì¸ê³¼ ê°ì‚¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.

DecisionOutcomePairë¥¼ ë°›ì•„ WhyLab ì¸ê³¼ì¶”ë¡  íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³ 
AuditResultë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    from engine.audit.causal_auditor import CausalAuditor

    auditor = CausalAuditor()
    result = auditor.audit(decision_outcome_pair)
    print(result.verdict)        # CAUSAL / NOT_CAUSAL / UNCERTAIN
    print(result.recommendation) # ë§ˆí¬ë‹¤ìš´ ê°ì‚¬ ë³´ê³ ì„œ
"""

from __future__ import annotations

import logging
import statistics
from typing import Any, Dict, List, Optional

from engine.audit.schemas import (
    AuditResult,
    AuditVerdict,
    DecisionOutcomePair,
)

logger = logging.getLogger("whylab.audit.auditor")

# ìµœì†Œ ìš”ê±´ ìƒìˆ˜
MIN_PRE_OBSERVATIONS = 7
MIN_POST_OBSERVATIONS = 3
SIGNIFICANCE_THRESHOLD = 0.05


class CausalAuditor:
    """ì—ì´ì „íŠ¸ ê²°ì •ì— ëŒ€í•œ ì¸ê³¼ ê°ì‚¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡  ì•„í‚¤í…ì²˜ (ë¦¬ì„œì¹˜ ê¸°ë°˜):
        - ë°ì´í„° í’ë¶€ â†’ CausalImpact (BSTS)
        - ë°ì´í„° í¬ì†Œ â†’ GSC (Generalized Synthetic Control)
        - Phase 1 ê¸°ë³¸ â†’ lightweight_t_test (scipy ë¯¸ì˜ì¡´)

    Phase 2ì—ì„œ DML/GSC í†µí•© ì‹œ method_routerê°€ ìë™ ìŠ¤ìœ„ì¹­í•©ë‹ˆë‹¤.
    """

    # ì§€ì› ë©”ì„œë“œ (Phaseë³„ í™•ì¥)
    SUPPORTED_METHODS = [
        "lightweight_t_test",    # Phase 1: ê¸°ë³¸ (í˜„ì¬)
        "causal_impact",         # Phase 2: ë°ì´í„° í’ë¶€ ì‹œ
        "gsc",                   # Phase 2: ë°ì´í„° í¬ì†Œ ì‹œ
        "dml",                   # Phase 2: Multi-treatment
    ]

    def __init__(
        self,
        significance_level: float = SIGNIFICANCE_THRESHOLD,
        min_pre: int = MIN_PRE_OBSERVATIONS,
        min_post: int = MIN_POST_OBSERVATIONS,
        preferred_method: str = "auto",
    ) -> None:
        self.significance_level = significance_level
        self.min_pre = min_pre
        self.min_post = min_post
        self.preferred_method = preferred_method

    def audit(self, pair: DecisionOutcomePair) -> AuditResult:
        """DecisionOutcomePairì— ëŒ€í•œ ì¸ê³¼ ê°ì‚¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            pair: ë§¤ì¹­ëœ ê²°ì •-ê²°ê³¼ ìŒ

        Returns:
            AuditResult (íŒê²°, í™•ì‹ ë„, ATE ë“±)
        """
        decision = pair.decision

        # ë°ì´í„° ì¶©ë¶„ì„± ê²€ì‚¬
        if not self._check_data_sufficiency(pair):
            return AuditResult(
                decision_id=decision.decision_id,
                verdict=AuditVerdict.INSUFFICIENT_DATA,
                confidence=0.0,
                method="data_check",
                recommendation=self._render_insufficient_report(pair),
            )

        pre_values = pair.pre_values
        post_values = pair.post_values

        # â”€â”€ ê²½ëŸ‰ ì¸ê³¼ ë¶„ì„ (Phase 1) â”€â”€
        # Phase 2ì—ì„œ WhyLab CausalImpact/DiDë¡œ êµì²´ ì˜ˆì •
        analysis = self._lightweight_causal_analysis(pre_values, post_values)

        # íŒê²° ê²°ì •
        verdict = self._determine_verdict(analysis)
        confidence = analysis["confidence"]
        ate = analysis["ate"]

        # ê°ì‚¬ ë³´ê³ ì„œ ë Œë”ë§
        recommendation = self._render_audit_report(pair, analysis, verdict)

        result = AuditResult(
            decision_id=decision.decision_id,
            verdict=verdict,
            confidence=confidence,
            ate=ate,
            ate_ci=analysis["ate_ci"],
            p_value=analysis.get("p_value"),
            method=analysis["method"],
            refutation_passed=analysis.get("refutation_passed", False),
            recommendation=recommendation,
            pipeline_results=analysis,
        )

        logger.info(
            "ğŸ“‹ ê°ì‚¬ ì™„ë£Œ: [%s] %s â†’ %s (ATE=%.4f, conf=%.1f%%)",
            decision.decision_id[:8],
            decision.agent_name,
            verdict.value,
            ate,
            confidence * 100,
        )

        return result

    def audit_batch(self, pairs: List[DecisionOutcomePair]) -> List[AuditResult]:
        """ì—¬ëŸ¬ ìŒì„ ì¼ê´„ ê°ì‚¬í•©ë‹ˆë‹¤."""
        return [self.audit(pair) for pair in pairs]

    # â”€â”€ ê²½ëŸ‰ ì¸ê³¼ ë¶„ì„ (Phase 1) â”€â”€

    def _lightweight_causal_analysis(
        self,
        pre: List[float],
        post: List[float],
    ) -> Dict[str, Any]:
        """ê²½ëŸ‰ ì¸ê³¼ ë¶„ì„ â€“ CausalImpact êµ¬í˜„ ì „ ëŒ€ì²´.

        ë°©ë²•:
        1. Pre/Post í‰ê·  ì°¨ì´ (ATE ì¶”ì •)
        2. Welch's t-test (ìœ ì˜ì„±)
        3. Effect size (Cohen's d)
        4. ë‹¨ìˆœ Placebo test (pre ê¸°ê°„ ë¶„í• )
        """
        pre_mean = statistics.mean(pre)
        post_mean = statistics.mean(post)
        ate = post_mean - pre_mean

        pre_std = statistics.stdev(pre) if len(pre) > 1 else 1e-10
        post_std = statistics.stdev(post) if len(post) > 1 else 1e-10

        # Welch's t-test (scipy ì—†ì´ ì§ì ‘ ê³„ì‚°)
        n_pre, n_post = len(pre), len(post)
        se = (pre_std**2 / n_pre + post_std**2 / n_post) ** 0.5
        t_stat = ate / se if se > 1e-10 else 0.0

        # ììœ ë„ (Welch-Satterthwaite)
        num = (pre_std**2 / n_pre + post_std**2 / n_post) ** 2
        denom = (
            (pre_std**2 / n_pre) ** 2 / max(n_pre - 1, 1)
            + (post_std**2 / n_post) ** 2 / max(n_post - 1, 1)
        )
        df = num / denom if denom > 0 else 1.0

        # p-value ê·¼ì‚¬ (ì •ê·œ ë¶„í¬ ê·¼ì‚¬, scipy ë¯¸ì˜ì¡´)
        import math
        z = abs(t_stat)
        p_value = 2 * (1 - 0.5 * (1 + math.erf(z / math.sqrt(2))))

        # Cohen's d (íš¨ê³¼ í¬ê¸°)
        pooled_std = ((pre_std**2 + post_std**2) / 2) ** 0.5
        cohens_d = ate / pooled_std if pooled_std > 1e-10 else 0.0

        # ATE ì‹ ë¢°êµ¬ê°„ (95%)
        margin = 1.96 * se
        ate_ci = [ate - margin, ate + margin]

        # ë‹¨ìˆœ Placebo test (pre ê¸°ê°„ì„ ë°˜ìœ¼ë¡œ ë‚˜ëˆ  íš¨ê³¼ í™•ì¸)
        placebo_passed = True
        if len(pre) >= 6:
            mid = len(pre) // 2
            placebo_ate = statistics.mean(pre[mid:]) - statistics.mean(pre[:mid])
            placebo_passed = abs(placebo_ate) < abs(ate) * 0.5

        # Confidence score (0~1)
        confidence = 0.0
        if p_value < self.significance_level:
            confidence += 0.4
        if abs(cohens_d) > 0.3:
            confidence += 0.2
        if placebo_passed:
            confidence += 0.2
        if ate_ci[0] > 0 or ate_ci[1] < 0:  # CIê°€ 0ì„ í¬í•¨í•˜ì§€ ì•ŠìŒ
            confidence += 0.2

        return {
            "method": "lightweight_t_test",
            "ate": round(ate, 4),
            "ate_ci": [round(x, 4) for x in ate_ci],
            "p_value": round(p_value, 6),
            "t_statistic": round(t_stat, 4),
            "df": round(df, 1),
            "cohens_d": round(cohens_d, 4),
            "pre_mean": round(pre_mean, 4),
            "post_mean": round(post_mean, 4),
            "pre_std": round(pre_std, 4),
            "post_std": round(post_std, 4),
            "n_pre": n_pre,
            "n_post": n_post,
            "placebo_passed": placebo_passed,
            "confidence": round(min(confidence, 1.0), 2),
        }

    def _determine_verdict(self, analysis: Dict[str, Any]) -> AuditVerdict:
        """ë¶„ì„ ê²°ê³¼ë¡œë¶€í„° íŒê²°ì„ ê²°ì •í•©ë‹ˆë‹¤."""
        p = analysis["p_value"]
        d = abs(analysis["cohens_d"])
        conf = analysis["confidence"]
        placebo = analysis["placebo_passed"]

        if p < self.significance_level and d > 0.3 and placebo and conf >= 0.6:
            return AuditVerdict.CAUSAL
        elif p > 0.2 or (not placebo) or (d < 0.1):
            return AuditVerdict.NOT_CAUSAL
        else:
            return AuditVerdict.UNCERTAIN

    # â”€â”€ ë°ì´í„° ê²€ì‚¬ â”€â”€

    def _check_data_sufficiency(self, pair: DecisionOutcomePair) -> bool:
        """ê°ì‚¬ì— í•„ìš”í•œ ìµœì†Œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸."""
        if len(pair.pre_outcomes) < self.min_pre:
            logger.warning(
                "âš ï¸ ì‚¬ì „ ê´€ì¸¡ ë¶€ì¡±: %d < %d (decision: %s)",
                len(pair.pre_outcomes), self.min_pre,
                pair.decision.decision_id[:8],
            )
            return False
        if len(pair.post_outcomes) < self.min_post:
            logger.warning(
                "âš ï¸ ì‚¬í›„ ê´€ì¸¡ ë¶€ì¡±: %d < %d (decision: %s)",
                len(pair.post_outcomes), self.min_post,
                pair.decision.decision_id[:8],
            )
            return False
        return True

    # â”€â”€ ë³´ê³ ì„œ ë Œë”ë§ â”€â”€

    def _render_audit_report(
        self,
        pair: DecisionOutcomePair,
        analysis: Dict[str, Any],
        verdict: AuditVerdict,
    ) -> str:
        """ì¸ê³¼ ê°ì‚¬ ë³´ê³ ì„œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë Œë”ë§í•©ë‹ˆë‹¤."""
        d = pair.decision
        icon = {"CAUSAL": "ğŸš€", "NOT_CAUSAL": "ğŸ›‘", "UNCERTAIN": "âš–ï¸"}.get(
            verdict.value, "ğŸ“‹"
        )

        lines = [
            f"## {icon} Causal Audit Report",
            "",
            f"**Agent:** `{d.agent_name}` ({d.agent_type.value})",
            f"**Decision:** {d.treatment}",
            f"**Target:** {d.target_sbu} / {d.target_metric.value}",
            f"**Verdict:** `{verdict.value}` | **Confidence:** {analysis['confidence']:.0%}",
            "",
            "### Statistical Summary",
            "",
            "| Metric | Value |",
            "|---|---|",
            f"| ATE | {analysis['ate']:+.4f} |",
            f"| 95% CI | [{analysis['ate_ci'][0]:.4f}, {analysis['ate_ci'][1]:.4f}] |",
            f"| p-value | {analysis['p_value']:.6f} |",
            f"| Cohen's d | {analysis['cohens_d']:.4f} |",
            f"| Pre Mean | {analysis['pre_mean']:.4f} (n={analysis['n_pre']}) |",
            f"| Post Mean | {analysis['post_mean']:.4f} (n={analysis['n_post']}) |",
            f"| Placebo Test | {'âœ… Passed' if analysis['placebo_passed'] else 'âŒ Failed'} |",
            "",
        ]

        # íŒê²°ë³„ ê¶Œê³ 
        if verdict == AuditVerdict.CAUSAL:
            lines += [
                "### ğŸ“ˆ Recommendation",
                "",
                f"- ì—ì´ì „íŠ¸ ê²°ì • **íš¨ê³¼ í™•ì¸**. ì „ëµ ìœ ì§€ ê¶Œì¥.",
                f"- ATE {analysis['ate']:+.4f}: {d.target_metric.value} ì§€í‘œì— ìœ ì˜ë¯¸í•œ ë³€í™”.",
                f"- Phase 2ì—ì„œ CausalImpactë¡œ ì •ë°€ ì¬ê²€ì¦ ì˜ˆì •.",
            ]
        elif verdict == AuditVerdict.NOT_CAUSAL:
            lines += [
                "### âš ï¸ Recommendation",
                "",
                f"- ì—ì´ì „íŠ¸ ê²°ì •ì˜ **íš¨ê³¼ ë¯¸í™•ì¸**. ì „ëµ ì¬ê²€í†  í•„ìš”.",
                f"- ì—ì´ì „íŠ¸ ì „ëµ ë©”ëª¨ë¦¬ì— 'ë¹„íš¨ê³¼ì ' íƒœê·¸ ì¶”ê°€ ê¶Œì¥.",
            ]
        else:
            lines += [
                "### ğŸ” Recommendation",
                "",
                f"- ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ í›„ ì¬ê°ì‚¬ í•„ìš”.",
                f"- ê´€ì¸¡ ê¸°ê°„ì„ {d.observation_window_days * 2}ì¼ë¡œ ì—°ì¥ ê¶Œì¥.",
            ]

        lines.append("")
        return "\n".join(lines)

    def _render_insufficient_report(self, pair: DecisionOutcomePair) -> str:
        """ë°ì´í„° ë¶€ì¡± ì‹œ ë³´ê³ ì„œ."""
        d = pair.decision
        return (
            f"## âš ï¸ Insufficient Data\n\n"
            f"**Agent:** `{d.agent_name}`\n"
            f"**Decision:** {d.treatment}\n\n"
            f"ê°ì‚¬ì— í•„ìš”í•œ ìµœì†Œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.\n"
            f"- ì‚¬ì „ ê´€ì¸¡: {len(pair.pre_outcomes)}ê±´ (ìµœì†Œ {self.min_pre}ê±´)\n"
            f"- ì‚¬í›„ ê´€ì¸¡: {len(pair.post_outcomes)}ê±´ (ìµœì†Œ {self.min_post}ê±´)\n"
        )
