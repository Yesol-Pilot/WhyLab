# -*- coding: utf-8 -*-
"""ToolAugmentedDebate â€” ë„êµ¬ ê°•í™” í† ë¡  í”„ë¡œí† ì½œ.

DaV í”„ë¡œí† ì½œì˜ Advocate/Criticì—ê²Œ ì‹¤ì œ ë¶„ì„ ë„êµ¬ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤:
  - Advocate: CATE ì¶”ì •, ë©”íƒ€ëŸ¬ë„ˆ ì‹¤í–‰, ë¯¼ê°ë„ ë¶„ì„ ë„êµ¬
  - Critic: ë°˜ì¦ í…ŒìŠ¤íŠ¸, ìœ„ì•½ ëŒ€ì¡°, êµë€ ì²´í¬ ë„êµ¬

ì¼ë°˜ DaVê°€ "ì´ë¯¸ ìˆëŠ” ì¦ê±°"ë§Œ í‰ê°€í•˜ëŠ” ë°˜ë©´,
ToolAugmented DaVëŠ” í† ë¡  ì¤‘ ìƒˆ ì¦ê±°ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

í•™ìˆ  ì°¸ì¡°:
  - Du et al. (2023). "Improving Factuality and Reasoning
    in Language Models through Multiagent Debate."
  - Schick et al. (2023). "Toolformer: Language Models Can
    Teach Themselves to Use Tools."
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional

import numpy as np

from engine.agents.dav_protocol import (
    CrossExamRecord,
    DaVClaim,
    DaVProtocol,
    DaVVerdict,
    Evidence,
)

logger = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„êµ¬ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class Tool:
    """ì—ì´ì „íŠ¸ ì‚¬ìš© ë„êµ¬."""
    name: str
    description: str
    role: str             # "advocate" | "critic" | "both"
    execute: Callable     # (context, claim) -> Evidence

@dataclass
class ToolCallRecord:
    """ë„êµ¬ í˜¸ì¶œ ê¸°ë¡."""
    agent: str
    tool_name: str
    result: Evidence
    round_num: int


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ì œê³µ ë„êµ¬ë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def tool_cate_variance(context: Dict[str, Any], claim: DaVClaim) -> Evidence:
    """ë©”íƒ€ëŸ¬ë„ˆ CATE ë¶„ì‚° ë¶„ì„ ë„êµ¬."""
    meta = context.get("meta_learners", {})
    if not isinstance(meta, dict) or not meta:
        return Evidence(
            source="tool:cate_variance",
            claim="CATE ë¶„ì‚° ë¶„ì„ ë¶ˆê°€",
            direction="neutral",
            strength=0.1,
            detail={"reason": "no_meta_learners"},
        )

    ates = []
    for name, result in meta.items():
        if isinstance(result, dict):
            ates.append(result.get("ate", result.get("mean_cate", 0)))

    if not ates:
        return Evidence(
            source="tool:cate_variance",
            claim="CATE ê°’ ì—†ìŒ",
            direction="neutral",
            strength=0.1,
        )

    cv = np.std(ates) / (abs(np.mean(ates)) + 1e-10)
    consistent = cv < 0.5

    return Evidence(
        source="tool:cate_variance",
        claim=f"ë©”íƒ€ëŸ¬ë„ˆ CATE ë³€ë™ê³„ìˆ˜ CV={cv:.3f}",
        direction="supports" if consistent else "contradicts",
        strength=0.7 if consistent else 0.3,
        detail={"cv": round(cv, 4), "ates": ates, "consistent": consistent},
    )


def tool_effect_size_check(context: Dict[str, Any], claim: DaVClaim) -> Evidence:
    """íš¨ê³¼ í¬ê¸° ì‹¤ì§ˆì  ì˜ë¯¸ ê²€ì¦ ë„êµ¬."""
    ate = claim.ate
    # Cohen's d ê¸°ì¤€: 0.2=ì‘ìŒ, 0.5=ì¤‘ê°„, 0.8=í¼
    y_std = context.get("outcome_std", 1.0)
    cohens_d = abs(ate) / (y_std + 1e-10)

    if cohens_d >= 0.5:
        direction = "supports"
        strength = min(1.0, cohens_d / 1.0)
        msg = f"ì‹¤ì§ˆì  íš¨ê³¼ í¬ê¸° (Cohen's d={cohens_d:.2f})"
    elif cohens_d >= 0.2:
        direction = "supports"
        strength = 0.4
        msg = f"ì†ŒëŸ‰ íš¨ê³¼ (Cohen's d={cohens_d:.2f})"
    else:
        direction = "contradicts"
        strength = 0.5
        msg = f"ë¬´ì‹œí•  íš¨ê³¼ í¬ê¸° (Cohen's d={cohens_d:.2f})"

    return Evidence(
        source="tool:effect_size",
        claim=msg,
        direction=direction,
        strength=strength,
        detail={"cohens_d": round(cohens_d, 4), "ate": ate, "y_std": y_std},
    )


def tool_placebo_refutation(context: Dict[str, Any], claim: DaVClaim) -> Evidence:
    """ìœ„ì•½ ëŒ€ì¡° ë°˜ì¦ ë„êµ¬."""
    refutation = context.get("refutation", {})
    placebo = refutation.get("placebo", {}) if isinstance(refutation, dict) else {}

    if not isinstance(placebo, dict) or not placebo:
        # ìœ„ì•½ ê²€ì • ë¯¸ìˆ˜í–‰ â†’ ìì²´ ê°„ì´ ê²€ì •
        data = context.get("raw_data")
        if data is None:
            return Evidence(
                source="tool:placebo",
                claim="ìœ„ì•½ ê²€ì • ë¶ˆê°€ (ë°ì´í„° ì—†ìŒ)",
                direction="neutral",
                strength=0.2,
            )

        return Evidence(
            source="tool:placebo",
            claim="ìœ„ì•½ ê²€ì • ë¯¸ìˆ˜í–‰ â€” ì¸ê³¼ ì£¼ì¥ ì•½í™”",
            direction="contradicts",
            strength=0.4,
            detail={"reason": "not_conducted"},
        )

    passed = placebo.get("passed", False)
    p_value = placebo.get("p_value", None)

    return Evidence(
        source="tool:placebo",
        claim=f"ìœ„ì•½ ê²€ì • {'í†µê³¼' if passed else 'ì‹¤íŒ¨'}",
        direction="supports" if passed else "contradicts",
        strength=0.8 if passed else 0.6,
        detail={"passed": passed, "p_value": p_value},
    )


def tool_overlap_check(context: Dict[str, Any], claim: DaVClaim) -> Evidence:
    """ì²˜ì¹˜/ëŒ€ì¡° ê·¸ë£¹ ê²¹ì¹¨ ê²€ì¦ ë„êµ¬ (Positivity assumption)."""
    propensity = context.get("propensity_scores")
    if propensity is None:
        return Evidence(
            source="tool:overlap",
            claim="ì„±í–¥ì ìˆ˜ ë¯¸ì œê³µ â€” ê²¹ì¹¨ ê²€ì¦ ë¶ˆê°€",
            direction="neutral",
            strength=0.2,
        )

    ps = np.asarray(propensity)
    # ê²¹ì¹¨ ìœ„ë°˜: ê·¹ë‹¨ì  ì„±í–¥ ì ìˆ˜ ë¹„ìœ¨
    extreme = np.mean((ps < 0.05) | (ps > 0.95))

    if extreme < 0.05:
        return Evidence(
            source="tool:overlap",
            claim=f"ì–‘í˜¸í•œ ê²¹ì¹¨ (ê·¹ë‹¨ê°’ {extreme:.1%})",
            direction="supports",
            strength=0.7,
            detail={"extreme_ratio": round(extreme, 4)},
        )
    else:
        return Evidence(
            source="tool:overlap",
            claim=f"ê²¹ì¹¨ ìœ„ë°˜ ì˜ì‹¬ (ê·¹ë‹¨ê°’ {extreme:.1%})",
            direction="contradicts",
            strength=0.6,
            detail={"extreme_ratio": round(extreme, 4)},
        )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ToolAugmentedDebate
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ToolAugmentedDebate(DaVProtocol):
    """ë„êµ¬ ê°•í™” í† ë¡  ê¸°ë°˜ ì¸ê³¼ ê²€ì¦ í”„ë¡œí† ì½œ.

    DaV í”„ë¡œí† ì½œì„ í™•ì¥í•˜ì—¬ í† ë¡  ì¤‘ ë„êµ¬ë¥¼ ë™ì ìœ¼ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤.

    ì‚¬ìš©ë²•:
        debate = ToolAugmentedDebate()
        verdict = debate.verify(context)  # ë„êµ¬ í˜¸ì¶œ í¬í•¨
    """

    # ê¸°ë³¸ ë„êµ¬ ì„¸íŠ¸
    DEFAULT_TOOLS = [
        Tool(
            name="cate_variance",
            description="ë©”íƒ€ëŸ¬ë„ˆ CATE ë¶„ì‚° ë¶„ì„",
            role="advocate",
            execute=tool_cate_variance,
        ),
        Tool(
            name="effect_size",
            description="Cohen's d íš¨ê³¼ í¬ê¸° ê²€ì¦",
            role="advocate",
            execute=tool_effect_size_check,
        ),
        Tool(
            name="placebo",
            description="ìœ„ì•½ ëŒ€ì¡° ë°˜ì¦ ê²€ì •",
            role="critic",
            execute=tool_placebo_refutation,
        ),
        Tool(
            name="overlap",
            description="ì²˜ì¹˜/ëŒ€ì¡° ê²¹ì¹¨(Positivity) ê²€ì¦",
            role="critic",
            execute=tool_overlap_check,
        ),
    ]

    def __init__(
        self,
        verification_threshold: float = 0.65,
        refutation_threshold: float = 0.60,
        n_rounds: int = 2,
        tools: Optional[List[Tool]] = None,
    ):
        super().__init__(
            verification_threshold=verification_threshold,
            refutation_threshold=refutation_threshold,
        )
        self.n_rounds = n_rounds
        self.tools = tools if tools is not None else self.DEFAULT_TOOLS
        self.tool_call_log: List[ToolCallRecord] = []

    def verify(self, context: Dict[str, Any]) -> DaVVerdict:
        """ë„êµ¬ ê°•í™” ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        1ë‹¨ê³„: ê¸°ë³¸ ì¦ê±° ìˆ˜ì§‘ (DaV ë°©ì‹)
        2ë‹¨ê³„: ë„êµ¬ í˜¸ì¶œ ë¼ìš´ë“œ (Advocate â†’ Critic ë°˜ë³µ)
        3ë‹¨ê³„: í†µí•© íŒê²°
        """
        self.tool_call_log = []

        # 1. ì£¼ì¥ êµ¬ì„±
        claim = self._construct_claim(context)

        # 2. ê¸°ë³¸ ì¦ê±° ìˆ˜ì§‘
        evidence = self._collect_evidence(context, claim)

        # 3. ë„êµ¬ ê°•í™” ë¼ìš´ë“œ
        for round_num in range(self.n_rounds):
            # Advocate ë„êµ¬ í˜¸ì¶œ
            advocate_tools = [t for t in self.tools if t.role in ("advocate", "both")]
            for tool in advocate_tools:
                try:
                    new_evidence = tool.execute(context, claim)
                    evidence.append(new_evidence)
                    self.tool_call_log.append(ToolCallRecord(
                        agent="advocate",
                        tool_name=tool.name,
                        result=new_evidence,
                        round_num=round_num,
                    ))
                except Exception as e:
                    logger.warning("ë„êµ¬ %s í˜¸ì¶œ ì‹¤íŒ¨: %s", tool.name, e)

            # Critic ë„êµ¬ í˜¸ì¶œ
            critic_tools = [t for t in self.tools if t.role in ("critic", "both")]
            for tool in critic_tools:
                try:
                    new_evidence = tool.execute(context, claim)
                    evidence.append(new_evidence)
                    self.tool_call_log.append(ToolCallRecord(
                        agent="critic",
                        tool_name=tool.name,
                        result=new_evidence,
                        round_num=round_num,
                    ))
                except Exception as e:
                    logger.warning("ë„êµ¬ %s í˜¸ì¶œ ì‹¤íŒ¨: %s", tool.name, e)

        # 4. êµì°¨ ì‹¬ë¬¸
        cross_exam = self._cross_examine(evidence, claim)

        # ë„êµ¬ í˜¸ì¶œ ê¸°ë¡ì„ êµì°¨ ì‹¬ë¬¸ì— ì¶”ê°€
        tool_summary = CrossExamRecord(
            agent="tool_augmented",
            argument=(
                f"ë„êµ¬ {len(self.tool_call_log)}íšŒ í˜¸ì¶œ ì™„ë£Œ. "
                f"Advocate ë„êµ¬: {len([t for t in self.tool_call_log if t.agent == 'advocate'])}ê°œ, "
                f"Critic ë„êµ¬: {len([t for t in self.tool_call_log if t.agent == 'critic'])}ê°œ."
            ),
            evidence_refs=[t.tool_name for t in self.tool_call_log],
            strength=0.5,
        )
        cross_exam.append(tool_summary)

        # 5. íŒê²°
        verdict = self._render_verdict(claim, evidence, cross_exam)

        logger.info(
            "ğŸ”§ ToolAugmented DaV: %s (conf=%.1f%%) â€” ì¦ê±° %dê°œ, ë„êµ¬ %díšŒ",
            verdict.verdict,
            verdict.confidence * 100,
            len(evidence),
            len(self.tool_call_log),
        )

        return verdict

    def get_tool_log(self) -> List[Dict[str, Any]]:
        """ë„êµ¬ í˜¸ì¶œ ë¡œê·¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return [
            {
                "agent": t.agent,
                "tool": t.tool_name,
                "direction": t.result.direction,
                "strength": t.result.strength,
                "round": t.round_num,
            }
            for t in self.tool_call_log
        ]
