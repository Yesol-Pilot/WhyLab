# -*- coding: utf-8 -*-
"""R4: Who&When ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ â€” Blame Attribution SOTA ë¹„êµ.

Who&When ë²¤ì¹˜ë§ˆí¬ (2026):
- 127ê°œ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì‹¤íŒ¨ ë¡œê·¸
- SOTA: o1 â‰ˆ 15%, Claude 3.7 Sonnet â‰ˆ 25.1%
- ê³¼ì œ: Root-cause step ì •í™•íˆ ì‹ë³„

WhyLabì˜ MACIE(Shapley) + ECHO ì•Œê³ ë¦¬ì¦˜ì´
ê¸°ì¡´ SOTAë¥¼ ì••ë„í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import logging
import random
import statistics
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

from engine.audit.methods.blame_attribution import BlameAttributionMethod

logger = logging.getLogger("whylab.benchmark.who_when")


# â”€â”€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° êµ¬ì¡° â”€â”€

@dataclass
class FailureScenario:
    """Who&When ìŠ¤íƒ€ì¼ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤."""
    scenario_id: str
    agents: List[str]
    interaction_steps: List[Dict[str, Any]]
    root_cause_agent: str
    root_cause_step: int
    total_steps: int
    difficulty: str = "medium"  # easy, medium, hard


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼."""
    scenario_id: str
    predicted_agent: str
    actual_agent: str
    agent_correct: bool
    blame_scores: Dict[str, float]
    confidence: float


# â”€â”€ í•©ì„± ë²¤ì¹˜ë§ˆí¬ ìƒì„±ê¸° â”€â”€

class WhoWhenBenchmarkGenerator:
    """Who&When ìŠ¤íƒ€ì¼ í•©ì„± ë²¤ì¹˜ë§ˆí¬ ìƒì„±.

    ì‹¤ì œ Who&When ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œ ì´ ìƒì„±ê¸° ëŒ€ì‹ 
    ë¡œë”(Loader)ë¡œ êµì²´í•©ë‹ˆë‹¤.
    """

    FAILURE_PATTERNS = [
        "cascade",      # Aì˜ ì˜¤ë¥˜ê°€ Bâ†’Cë¡œ ì „íŒŒ
        "silent",       # AëŠ” ì •ìƒì¸ ì²™ í•˜ì§€ë§Œ ë¯¸ë¬˜í•œ ì˜¤ë¥˜ ì£¼ì…
        "emergent",     # A+B ê°œë³„ ì •ìƒì´ë‚˜ ì¡°í•© ì‹œ ì‹¤íŒ¨
        "interference", # Aê°€ Bì˜ ì¶œë ¥ì„ ì˜¤ì—¼ì‹œí‚´
    ]

    def generate(
        self,
        n_scenarios: int = 50,
        n_agents_range: Tuple[int, int] = (3, 6),
        seed: int = 42,
    ) -> List[FailureScenario]:
        """í•©ì„± ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±."""
        random.seed(seed)
        scenarios = []

        for i in range(n_scenarios):
            n_agents = random.randint(*n_agents_range)
            agents = [f"agent_{chr(65 + j)}" for j in range(n_agents)]
            pattern = random.choice(self.FAILURE_PATTERNS)
            total_steps = random.randint(5, 15)
            root_idx = random.randint(0, n_agents - 1)
            root_step = random.randint(1, total_steps - 1)

            steps = self._generate_interaction_steps(
                agents, total_steps, root_idx, root_step, pattern
            )

            difficulty = "easy" if n_agents <= 3 else ("hard" if n_agents >= 5 else "medium")

            scenarios.append(FailureScenario(
                scenario_id=f"ww_{i:03d}",
                agents=agents,
                interaction_steps=steps,
                root_cause_agent=agents[root_idx],
                root_cause_step=root_step,
                total_steps=total_steps,
                difficulty=difficulty,
            ))

        return scenarios

    def _generate_interaction_steps(
        self,
        agents: List[str],
        total_steps: int,
        root_idx: int,
        root_step: int,
        pattern: str,
    ) -> List[Dict[str, Any]]:
        """ì—ì´ì „íŠ¸ ìƒí˜¸ì‘ìš© ë‹¨ê³„ ìƒì„±."""
        steps = []
        for t in range(total_steps):
            acting_agent = agents[t % len(agents)]
            is_error_step = (t == root_step and acting_agent == agents[root_idx])

            step = {
                "step": t,
                "agent": acting_agent,
                "action": f"action_{t}",
                "pattern": pattern,
                "is_root_cause": is_error_step,
                "effect_magnitude": random.uniform(0.5, 2.0) if is_error_step else random.uniform(0.0, 0.3),
            }
            steps.append(step)

        return steps


# â”€â”€ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸° â”€â”€

class WhoWhenBenchmarkRunner:
    """Blame Attribution ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰."""

    BASELINES = {
        "random_guess": None,
        "majority_vote": None,
        "openai_o1": 0.15,
        "deepseek_r1": 0.15,
        "claude_3_7_sonnet": 0.251,
    }

    def __init__(self) -> None:
        self.blame_method = BlameAttributionMethod()

    def run(self, scenarios: List[FailureScenario]) -> Dict[str, Any]:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰."""
        results: List[BenchmarkResult] = []

        for scenario in scenarios:
            result = self._evaluate_scenario(scenario)
            results.append(result)

        # ì •í™•ë„ ê³„ì‚°
        agent_accuracy = sum(1 for r in results if r.agent_correct) / len(results)

        # ë‚œì´ë„ë³„ ë¶„ì„
        by_difficulty = {}
        for diff in ["easy", "medium", "hard"]:
            subset = [r for r, s in zip(results, scenarios) if s.difficulty == diff]
            if subset:
                acc = sum(1 for r in subset if r.agent_correct) / len(subset)
                by_difficulty[diff] = {"accuracy": round(acc, 3), "n": len(subset)}

        # SOTA ë¹„êµ
        comparison = {}
        for name, score in self.BASELINES.items():
            if score is not None:
                comparison[name] = {
                    "accuracy": score,
                    "whylab_delta": round(agent_accuracy - score, 3),
                    "whylab_beats": agent_accuracy > score,
                }

        report = {
            "total_scenarios": len(scenarios),
            "agent_identification_accuracy": round(agent_accuracy, 3),
            "by_difficulty": by_difficulty,
            "sota_comparison": comparison,
            "avg_confidence": round(
                statistics.mean(r.confidence for r in results), 3
            ),
        }

        logger.info(
            "ğŸ“Š Who&When ë²¤ì¹˜ë§ˆí¬: accuracy=%.1f%% (SOTA Claude: 25.1%%)",
            agent_accuracy * 100,
        )

        return report

    def _evaluate_scenario(self, scenario: FailureScenario) -> BenchmarkResult:
        """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ í‰ê°€."""
        # ì—ì´ì „íŠ¸ë³„ effect magnitudeë¥¼ treatment_valueë¡œ ì‚¬ìš©
        agent_effects = {}
        for step in scenario.interaction_steps:
            agent = step["agent"]
            mag = step["effect_magnitude"]
            if agent not in agent_effects:
                agent_effects[agent] = {"treatment_value": 0.0, "expected_effect": "positive"}
            agent_effects[agent]["treatment_value"] += mag

        # Pre/Post ì‹œê³„ì—´ í•©ì„±
        pre = [random.gauss(100, 5) for _ in range(14)]
        # Root-cause ì—ì´ì „íŠ¸ì˜ ì˜í–¥ ë°˜ì˜
        root_effect = agent_effects.get(scenario.root_cause_agent, {}).get("treatment_value", 1.0)
        post = [random.gauss(100 - root_effect * 5, 5) for _ in range(7)]

        result = self.blame_method.analyze(
            pre=pre,
            post=post,
            agent_decisions=agent_effects,
        )

        blame_scores = result.diagnostics.get("blame_scores", {})

        # ê°€ì¥ ë†’ì€ ì±…ì„ì„ ê°€ì§„ ì—ì´ì „íŠ¸
        if blame_scores:
            predicted_agent = max(blame_scores, key=lambda k: abs(blame_scores[k]))
        else:
            predicted_agent = scenario.agents[0]

        return BenchmarkResult(
            scenario_id=scenario.scenario_id,
            predicted_agent=predicted_agent,
            actual_agent=scenario.root_cause_agent,
            agent_correct=(predicted_agent == scenario.root_cause_agent),
            blame_scores=blame_scores,
            confidence=result.confidence,
        )


# â”€â”€ í…ŒìŠ¤íŠ¸ â”€â”€

class TestWhoWhenBenchmark:
    def test_generator(self):
        gen = WhoWhenBenchmarkGenerator()
        scenarios = gen.generate(n_scenarios=10)
        assert len(scenarios) == 10
        for s in scenarios:
            assert s.root_cause_agent in s.agents

    def test_benchmark_run(self):
        gen = WhoWhenBenchmarkGenerator()
        scenarios = gen.generate(n_scenarios=20, seed=42)
        runner = WhoWhenBenchmarkRunner()
        report = runner.run(scenarios)

        assert report["total_scenarios"] == 20
        assert 0 <= report["agent_identification_accuracy"] <= 1.0
        assert "sota_comparison" in report

    def test_sota_comparison_structure(self):
        gen = WhoWhenBenchmarkGenerator()
        scenarios = gen.generate(n_scenarios=10)
        runner = WhoWhenBenchmarkRunner()
        report = runner.run(scenarios)

        for baseline in ["openai_o1", "claude_3_7_sonnet"]:
            assert baseline in report["sota_comparison"]
            assert "whylab_delta" in report["sota_comparison"][baseline]

    def test_difficulty_breakdown(self):
        gen = WhoWhenBenchmarkGenerator()
        scenarios = gen.generate(n_scenarios=30)
        runner = WhoWhenBenchmarkRunner()
        report = runner.run(scenarios)

        assert "by_difficulty" in report
