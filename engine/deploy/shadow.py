# -*- coding: utf-8 -*-
"""Phase 4: ÏÑÄÎèÑÏö∞ Î∞∞Ìè¨ Ïª®Ìä∏Î°§Îü¨.

CTO ÎπÑÌèâ Î∞òÏòÅ:
1. ÎπÑÏö© ÏÑúÌÇ∑ Î∏åÎ†àÏù¥Ïª§ ‚Äî ARES Îî• Í∞êÏÇ¨ ÏùºÏùº ÌÜ†ÌÅ∞ ÏÉÅÌïú
2. Dry-run ÏÑÄÎèÑÏö∞ Î™®Îìú ‚Äî Œ∂ ÎØ∏Ï†ÅÏö© Î™®ÎãàÌÑ∞ÎßÅ

Reviewer Í∏∞Ïó¨:
- Ïã§Ï†ú ÎùºÏù¥Î∏å Îç∞Ïù¥ÌÑ∞Î°ú "Î¨¥Ïò§Ïóº(No Data Leakage)" Ïã§Ï¶ù
"""

from __future__ import annotations

import hashlib
import json
import logging
import os
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Dict, List, Optional

logger = logging.getLogger("whylab.deploy.shadow")

# DB Ïñ¥ÎåëÌÑ∞ (ÎÑ§Ïù¥Ìã∞Î∏å PG + ÎπÑÎèôÍ∏∞ DLQ)
try:
    from engine.deploy.db_adapter import AsyncDLQWriter, IntegrityHashWriter
    _HAS_DB_ADAPTER = True
except ImportError:
    _HAS_DB_ADAPTER = False


class DeploymentMode(str, Enum):
    """Î∞∞Ìè¨ Î™®Îìú."""
    SHADOW_DRY_RUN = "shadow_dry_run"   # Œ∂ ÎØ∏Ï†ÅÏö©, Î™®ÎãàÌÑ∞ÎßÅÎßå
    SHADOW_ACTIVE = "shadow_active"      # Œ∂ Ï†ÅÏö©, ÎùºÏù¥Î∏å Î∞òÏòÅ
    PRODUCTION = "production"            # ÏôÑÏ†Ñ ÌîÑÎ°úÎçïÏÖò


@dataclass
class CostBudget:
    """ARES Îî• Í∞êÏÇ¨ ÎπÑÏö© ÏòàÏÇ∞.

    CTO ÏßÄÏãú: ÏùºÏùº ÌÜ†ÌÅ∞/ÎπÑÏö© Hard Limit ÏÑ§Ï†ï.
    """
    daily_token_limit: int = 100_000     # ÏùºÏùº ÏµúÎåÄ ÌÜ†ÌÅ∞
    daily_cost_limit_usd: float = 10.0   # ÏùºÏùº ÏµúÎåÄ ÎπÑÏö© (USD)
    tokens_used_today: int = 0
    cost_used_today_usd: float = 0.0
    last_reset: float = field(default_factory=time.time)
    breaker_tripped: bool = False
    trip_count: int = 0

    def consume(self, tokens: int, cost_usd: float = 0.0) -> bool:
        """ÌÜ†ÌÅ∞/ÎπÑÏö© ÏÜåÎπÑ. ÏòàÏÇ∞ Ï¥àÍ≥º Ïãú False Î∞òÌôò."""
        self._maybe_reset()
        self.tokens_used_today += tokens
        self.cost_used_today_usd += cost_usd

        if (self.tokens_used_today > self.daily_token_limit or
                self.cost_used_today_usd > self.daily_cost_limit_usd):
            self.breaker_tripped = True
            self.trip_count += 1
            logger.warning(
                "üîå Circuit Breaker TRIPPED: tokens=%d/%d, cost=$%.2f/$%.2f",
                self.tokens_used_today, self.daily_token_limit,
                self.cost_used_today_usd, self.daily_cost_limit_usd,
            )
            return False
        return True

    def _maybe_reset(self) -> None:
        """ÏùºÏùº Î¶¨ÏÖã (24ÏãúÍ∞Ñ Í≤ΩÍ≥º Ïãú)."""
        if time.time() - self.last_reset > 86400:
            self.tokens_used_today = 0
            self.cost_used_today_usd = 0.0
            self.breaker_tripped = False
            self.last_reset = time.time()

    @property
    def remaining_tokens(self) -> int:
        return max(0, self.daily_token_limit - self.tokens_used_today)

    @property
    def utilization(self) -> float:
        return self.tokens_used_today / max(self.daily_token_limit, 1)


@dataclass
class ShadowObservation:
    """ÏÑÄÎèÑÏö∞ Î™®ÎìúÏóêÏÑúÏùò Í¥ÄÏ∏° Í≤∞Í≥º (Dry-run)."""
    timestamp: float = field(default_factory=time.time)
    decision_id: str = ""
    proposed_zeta: float = 0.0
    lyapunov_zeta_max: float = 0.0
    would_have_clipped: bool = False
    drift_index: float = 0.0
    ares_penalty: float = 0.0
    ate: float = 0.0
    e_value: float = 0.0
    mode: DeploymentMode = DeploymentMode.SHADOW_DRY_RUN


class ShadowDeployController:
    """ÏÑÄÎèÑÏö∞ Î∞∞Ìè¨ Ïª®Ìä∏Î°§Îü¨.

    CTO ÏßÄÏãú:
    - Dry-run: "ÎßåÏïΩ Œ∂Î•º Î∞òÏòÅÌñàÎã§Î©¥ Ïñ¥ÎñªÍ≤å ÎêòÏóàÏùÑÍπå" Î™®ÎãàÌÑ∞ÎßÅ
    - Circuit Breaker: ARES ÎπÑÏö© ÏÉÅÌïú Ï¥àÍ≥º Ïãú Í≤ΩÎüâ Ìè¥Î∞±

    ÏÇ¨Ïö©Î≤ï:
        controller = ShadowDeployController(mode=DeploymentMode.SHADOW_DRY_RUN)
        result = controller.process_audit(
            decision_id="d1",
            proposed_zeta=0.5,
            audit_result={...},
        )
    """

    def __init__(
        self,
        mode: DeploymentMode = DeploymentMode.SHADOW_DRY_RUN,
        cost_budget: Optional[CostBudget] = None,
    ) -> None:
        self.mode = mode
        self.cost_budget = cost_budget or CostBudget()
        self._observations: List[ShadowObservation] = []
        self._dlq_memory: List[Dict[str, Any]] = []  # DB ÎØ∏Ïó∞Í≤∞ Ïãú Ìè¥Î∞±
        self._dlq_writer: Optional[Any] = None
        self._fallback_count = 0

        # ÎπÑÎèôÍ∏∞ DLQ Writer Ï¥àÍ∏∞Ìôî
        if _HAS_DB_ADAPTER:
            try:
                self._dlq_writer = AsyncDLQWriter()
            except Exception as e:
                logger.warning("‚ö†Ô∏è AsyncDLQWriter init failed: %s", e)

    def should_run_deep_audit(self) -> bool:
        """ARES Îî• Í∞êÏÇ¨Î•º Ïã§ÌñâÌï†ÏßÄ ÌåêÎã®.

        ÏÑúÌÇ∑ Î∏åÎ†àÏù¥Ïª§Í∞Ä Ìä∏Î¶ΩÎêòÎ©¥ Í≤ΩÎüâ Ìè¥Î∞±ÏúºÎ°ú Ï†ÑÌôò.
        """
        if self.cost_budget.breaker_tripped:
            self._fallback_count += 1
            logger.info(
                "‚ö° Fallback mode: ARES skipped (breaker tripped, fallback #%d)",
                self._fallback_count,
            )
            return False
        return True

    def enqueue_to_dlq(
        self,
        decision_id: str,
        payload: Dict[str, Any],
        reason: str = "breaker_tripped",
    ) -> None:
        """DLQ(Dead Letter Queue) Ï†ÅÏû¨.

        Ïö∞ÏÑ†: AsyncDLQWriter (Î∞±Í∑∏ÎùºÏö¥Îìú Ïä§Î†àÎìú, ÎÑ§Ïù¥Ìã∞Î∏å PG)
        Ìè¥Î∞±: Ïù∏Î©îÎ™®Î¶¨ Î¶¨Ïä§Ìä∏ (VOLATILE)

        Î©îÏù∏ Ïä§Î†àÎìú Î∏îÎ°úÌÇπ: 0 (ÌÅê put Îßå ÏàòÌñâ)
        """
        # ÎπÑÎèôÍ∏∞ DLQ Writer (Ï¶âÏãú Î∞òÌôò)
        if self._dlq_writer:
            self._dlq_writer.enqueue(decision_id, payload, reason)
            return

        # Ìè¥Î∞±: Ïù∏Î©îÎ™®Î¶¨
        entry = {
            "decision_id": decision_id,
            "reason": reason,
            "timestamp": time.time(),
            "payload": payload,
        }
        self._dlq_memory.append(entry)
        logger.warning(
            "üì• DLQ in-memory fallback: %s (queue=%d) ‚Äî VOLATILE",
            decision_id, len(self._dlq_memory),
        )

    @property
    def dlq_size(self) -> int:
        return len(self._dlq_memory)

    @property
    def dlq_entries(self) -> List[Dict[str, Any]]:
        return list(self._dlq_memory)

    def record_observation(
        self,
        decision_id: str,
        proposed_zeta: float,
        lyapunov_zeta_max: float,
        drift_index: float = 0.0,
        ares_penalty: float = 0.0,
        ate: float = 0.0,
        e_value: float = 0.0,
    ) -> ShadowObservation:
        """ÏÑÄÎèÑÏö∞ Í¥ÄÏ∏° Í∏∞Î°ù.

        Dry-run Î™®Îìú: Œ∂Î•º Ï†ÅÏö©ÌïòÏßÄ ÏïäÍ≥† Í∏∞Î°ùÎßå.
        Active Î™®Îìú: Œ∂Î•º Ï†ÅÏö©ÌïòÍ≥† Í∏∞Î°ù.
        """
        obs = ShadowObservation(
            decision_id=decision_id,
            proposed_zeta=proposed_zeta,
            lyapunov_zeta_max=lyapunov_zeta_max,
            would_have_clipped=proposed_zeta > lyapunov_zeta_max,
            drift_index=drift_index,
            ares_penalty=ares_penalty,
            ate=ate,
            e_value=e_value,
            mode=self.mode,
        )
        self._observations.append(obs)

        if self.mode == DeploymentMode.SHADOW_DRY_RUN:
            logger.debug(
                "üëÅÔ∏è Shadow observe: Œ∂=%.4f (max=%.4f, clip=%s) [DRY-RUN]",
                proposed_zeta, lyapunov_zeta_max,
                "YES" if obs.would_have_clipped else "no",
            )

        return obs

    def should_apply_feedback(self) -> bool:
        """Œ∂ ÌîºÎìúÎ∞±ÏùÑ Ïã§Ï†úÎ°ú Ï†ÅÏö©Ìï†ÏßÄ ÌåêÎã®."""
        return self.mode in (
            DeploymentMode.SHADOW_ACTIVE,
            DeploymentMode.PRODUCTION,
        )

    def get_dashboard_stats(self) -> Dict[str, Any]:
        """ÎåÄÏãúÎ≥¥ÎìúÏö© ÌÜµÍ≥Ñ."""
        if not self._observations:
            return {"total": 0}

        clip_count = sum(1 for o in self._observations if o.would_have_clipped)
        avg_zeta = sum(o.proposed_zeta for o in self._observations) / len(self._observations)
        avg_di = sum(o.drift_index for o in self._observations) / len(self._observations)

        return {
            "mode": self.mode.value,
            "total_observations": len(self._observations),
            "clip_rate": round(clip_count / len(self._observations), 4),
            "avg_proposed_zeta": round(avg_zeta, 4),
            "avg_drift_index": round(avg_di, 4),
            "cost_budget": {
                "tokens_used": self.cost_budget.tokens_used_today,
                "tokens_limit": self.cost_budget.daily_token_limit,
                "utilization": round(self.cost_budget.utilization, 2),
                "breaker_tripped": self.cost_budget.breaker_tripped,
                "trip_count": self.cost_budget.trip_count,
            },
            "fallback_count": self._fallback_count,
        }

    def promote_to_active(self) -> None:
        """Dry-run ‚Üí Active Î™®Îìú ÏäπÍ≤©."""
        if self.mode == DeploymentMode.SHADOW_DRY_RUN:
            self.mode = DeploymentMode.SHADOW_ACTIVE
            logger.info("üöÄ Promoted: SHADOW_DRY_RUN ‚Üí SHADOW_ACTIVE")

    def promote_to_production(self) -> None:
        """Active ‚Üí Production ÏäπÍ≤©."""
        if self.mode == DeploymentMode.SHADOW_ACTIVE:
            self.mode = DeploymentMode.PRODUCTION
            logger.info("üè≠ Promoted: SHADOW_ACTIVE ‚Üí PRODUCTION")


# ‚îÄ‚îÄ ÏïîÌò∏ÌïôÏ†Å Îç∞Ïù¥ÌÑ∞ Î¨¥Í≤∞ÏÑ± ÏÑúÎ™Ö ‚îÄ‚îÄ

def compute_daily_hash(rollup_data: Dict[str, Any], date_str: str) -> Dict[str, str]:
    """Îç∞ÏùºÎ¶¨ Î°§ÏóÖ Îç∞Ïù¥ÌÑ∞Ïùò SHA-256 Ìï¥Ïãú.

    Ï≤¥Î¶¨ÌîºÌÇπ Î∞©Ïñ¥: ÎÖºÎ¨∏ Ïã¨ÏÇ¨ÏúÑÏõêÏù¥ Îç∞Ïù¥ÌÑ∞ ÏÇ¨ÌõÑ Ï°∞ÏûëÏùÑ ÏùòÏã¨Ìï† Îïå,
    GitHub Ïª§Î∞ã ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ + SHA-256ÏúºÎ°ú Î¨¥Í≤∞ÏÑ± Ï¶ùÎ™Ö.

    Args:
        rollup_data: Î°§ÏóÖ Î†àÏΩîÎìú (JSON-serializable)
        date_str: ÎÇ†Ïßú Î¨∏ÏûêÏó¥ (e.g. "2026-03-15")

    Returns:
        {"date": date_str, "sha256": hex_hash, "record_count": n}
    """
    canonical = json.dumps(rollup_data, sort_keys=True, ensure_ascii=False)
    h = hashlib.sha256(canonical.encode("utf-8")).hexdigest()
    return {
        "date": date_str,
        "sha256": h,
        "record_count": len(rollup_data.get("records", [])),
        "bytes": len(canonical),
    }


def append_hash_log(
    hash_entry: Dict[str, str],
    log_path: str = "data/integrity_hashes.jsonl",
) -> str:
    """Append-only Ìï¥Ïãú Î°úÍ∑∏ ÌååÏùºÏóê Ï∂îÍ∞Ä.

    Ïù¥ ÌååÏùºÏùÑ GitHubÏóê ÏûêÎèô Ïª§Î∞ãÌïòÎ©¥
    ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑÍ∞Ä Ï∞çÌûå Î∂àÎ≥Ä Î¨¥Í≤∞ÏÑ± Î†àÏΩîÎìú Ïó≠Ìï†.
    """
    os.makedirs(os.path.dirname(log_path) or ".", exist_ok=True)
    line = json.dumps(hash_entry, ensure_ascii=False)
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(line + "\n")
    return log_path


class DailyIntegrityWorker:
    """ÎèôÍ∏∞ Î°§ÏóÖ‚ÜíÌï¥Ïãú ÌååÏù¥ÌîÑÎùºÏù∏.

    Í≤ΩÏüÅ ÏÉÅÌÉú(Race Condition) Î∞©Ïñ¥:
    ÌååÏù¥Ïç¨ ÏõåÏª§Í∞Ä Î°§ÏóÖ Îç∞Ïù¥ÌÑ∞Î•º ÏßÅÏ†ë ÎèôÍ∏∞Ï†ÅÏúºÎ°ú Ï°∞Ìöå.
    Ìä∏ÎûúÏû≠ÏÖò Ïª§Î∞ã ÌõÑÏóêÎßå Ìï¥Ïãú Í≥ÑÏÇ∞ ‚Üí Î∂àÏôÑÏ†Ñ Îç∞Ïù¥ÌÑ∞ Ìï¥Ïã± Î∂àÍ∞Ä.

    DB ÌîÑÎ°úÌÜ†ÏΩú: ÎÑ§Ïù¥Ìã∞Î∏å psycopg2 (Ìè¨Ìä∏ 6543 Supavisor).
    REST API urllib ÏÇ¨Ïö© Í∏àÏßÄ.

    ÏÇ¨Ïö©Î≤ï:
        worker = DailyIntegrityWorker()
        result = worker.run("2026-03-15")
    """

    def __init__(
        self,
        hash_log_path: str = "data/integrity_hashes.jsonl",
    ) -> None:
        self.hash_log_path = hash_log_path

    def run(self, date_str: str) -> Dict[str, Any]:
        """ÎèôÍ∏∞ Î°§ÏóÖ‚ÜíÌï¥Ïãú ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ.

        1. ÎÑ§Ïù¥Ìã∞Î∏å PGÎ°ú Î°§ÏóÖ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå (Ìä∏ÎûúÏû≠ÏÖò ÏôÑÎ£å Î≥¥Ïû•)
        2. SHA-256 Ìï¥Ïãú Í≥ÑÏÇ∞
        3. integrity_hashes DB + JSONL Ïù¥Ï§ë Ï†ÄÏû•
        """
        result: Dict[str, Any] = {"date": date_str, "status": "unknown"}

        try:
            # Step 1: ÎÑ§Ïù¥Ìã∞Î∏å PGÎ°ú Î°§ÏóÖ Ï°∞Ìöå
            rollup_data = self._query_rollup_pg(date_str)
            if not rollup_data:
                result["status"] = "no_data"
                return result

            # Step 2: SHA-256 (Ïª§Î∞ãÎêú Îç∞Ïù¥ÌÑ∞Îßå ‚Äî Í≤ΩÏüÅ ÏÉÅÌÉú Î∂àÍ∞Ä)
            hash_entry = compute_daily_hash(rollup_data, date_str)

            # Step 3: DBÏóê Ìï¥Ïãú UPSERT (ÎÑ§Ïù¥Ìã∞Î∏å PG)
            if _HAS_DB_ADAPTER:
                IntegrityHashWriter.store(hash_entry, date_str)

            # Step 4: JSONL ÌååÏùº (GitHub ÏûêÎèô Ïª§Î∞ãÏö©)
            append_hash_log(hash_entry, self.hash_log_path)

            result["status"] = "success"
            result["hash"] = hash_entry
            logger.info(
                "‚úÖ Daily integrity: %s ‚Üí SHA256=%s (%d records)",
                date_str, hash_entry["sha256"][:16] + "...",
                hash_entry["record_count"],
            )

        except Exception as e:
            result["status"] = "error"
            result["error"] = str(e)
            logger.error("‚ùå Daily integrity failed for %s: %s", date_str, e)

        return result

    def _query_rollup_pg(self, date_str: str) -> Optional[Dict]:
        """ÎÑ§Ïù¥Ìã∞Î∏å psycopg2Î°ú Î°§ÏóÖ Ï°∞Ìöå (Supavisor Í≤ΩÏú†)."""
        if not _HAS_DB_ADAPTER:
            logger.warning("‚ö†Ô∏è db_adapter not available ‚Äî skipping rollup")
            return None

        from engine.deploy.db_adapter import _get_pg_connection
        conn = _get_pg_connection()
        if not conn:
            return None

        try:
            with conn:
                with conn.cursor() as cur:
                    cur.execute(
                        "SELECT * FROM daily_agent_rollup WHERE rollup_date = %s",
                        (date_str,),
                    )
                    columns = [desc[0] for desc in cur.description] if cur.description else []
                    rows = [dict(zip(columns, row)) for row in cur.fetchall()]
                    return {"records": rows, "date": date_str}
        except Exception as e:
            logger.error("‚ùå PG rollup query failed: %s", e)
            return None
        finally:
            conn.close()

