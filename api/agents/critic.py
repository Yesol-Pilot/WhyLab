"""
Critic Agent (Kant) â€” ë¹„íŒì  ê²€í†  ëª¨ë“ˆ
========================================
Engineerì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¹„íŒì ìœ¼ë¡œ ê²€í† í•˜ê³  Peer Review ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

[v3: LLM-as-a-Judge + ConstitutionGuard í†µí•© (Sprint 32)]
- êµ¬ì¡°í™” íŒì • ì²´ê³„: ACCEPT / REVISE / REJECT (ê°•ì œ)
- ConstitutionGuard verdict ìë™ ë°˜ì˜
- HALTED ì‹¤í—˜(ì‹¤í–‰ ì‹¤íŒ¨) ê°ì§€ ë° ì¦‰ì‹œ REJECT
- MethodRegistry(UCB1)ë¡œ ë¦¬ë·° ê¸°ì¤€ì„ ì ì‘ì ìœ¼ë¡œ ì„ íƒ
"""
import time
import logging
import random
from datetime import datetime
from typing import Optional

from api.graph import kg
from api.agents.method_registry import method_registry
from api.agents.gemini_client import evaluate_experiment, is_available as is_gemini_available
from api.guards.constitution_guard import guard, AnalysisLevel

logger = logging.getLogger("whylab.critic")

# â”€â”€ íŒì • ì²´ê³„ (LLM-as-a-Judge) â”€â”€
VERDICT_ACCEPT = "ACCEPT"           # ê²°ê³¼ ì±„íƒ, ë…¼ë¬¸í™” ê°€ëŠ¥
VERDICT_REVISE = "REVISE"           # ì¡°ê±´ë¶€ ìˆ˜ì • í›„ ì¬ì‹¤í—˜
VERDICT_REJECT = "REJECT"           # ê²°ê³¼ íê¸°, ê·¼ë³¸ì  ê²°í•¨


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¹„íŒ ê¸°ì¤€ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CRITIQUE_CRITERIA = {
    "sample_size": {
        "min_threshold": 500,
        "warning": "í‘œë³¸ í¬ê¸°ê°€ {n}ìœ¼ë¡œ ì†Œê·œëª¨ì…ë‹ˆë‹¤. í†µê³„ì  ê²€ì •ë ¥ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "pass": "í‘œë³¸ í¬ê¸° n={n}ì€ ì¶©ë¶„í•©ë‹ˆë‹¤."
    },
    "effect_size": {
        "min_threshold": 0.1,  # STEAM í•©ì„± ë°ì´í„°ëŠ” í‘œì¤€í™” ë‹¨ìœ„ (ì´ì „ LaLonde $100 â†’ 0.1ë¡œ ìˆ˜ì •)
        "warning": "ì²˜ë¦¬ íš¨ê³¼(ATE={ate})ê°€ ì‹¤ì§ˆì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ìˆ˜ì¤€ì¸ì§€ ì¬ê²€í†  í•„ìš”.",
        "pass": "ì²˜ë¦¬ íš¨ê³¼(ATE={ate})ëŠ” ì‹¤ì§ˆì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ í¬ê¸°ì…ë‹ˆë‹¤."
    },
    "heterogeneity": {
        "p_threshold": 0.05,
        "warning": "ì„œë¸Œê·¸ë£¹ '{moderator}'ì˜ p-value({p})ê°€ ê²½ê³„ì„  ìˆ˜ì¤€ì´ë¼ ì´ì§ˆì„± íŒë‹¨ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
    },
}


def review_experiment(experiment_result: dict) -> dict:
    """
    ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¹„íŒì ìœ¼ë¡œ ê²€í† í•©ë‹ˆë‹¤ (LLM-as-a-Judge v3).
    
    [íŒì • ì²´ê³„]
    - ACCEPT: ê²°ê³¼ ì±„íƒ, KG ë°˜ì˜ ë° ë…¼ë¬¸í™” ê°€ëŠ¥
    - REVISE: ì¡°ê±´ë¶€ ìˆ˜ì • í›„ ì¬ì‹¤í—˜ ìš”êµ¬
    - REJECT: ê²°ê³¼ íê¸°, ê·¼ë³¸ì  ê²°í•¨ (HALTED í¬í•¨)
    
    Returns:
        dict: êµ¬ì¡°í™”ëœ Peer Review ë¦¬í¬íŠ¸
    """
    issues = []
    strengths = []
    
    # â”€â”€ Step 0: HALTED ì‹¤í—˜ ì¦‰ì‹œ REJECT â”€â”€
    if experiment_result.get("conclusion") == "EXECUTION_FAILED":
        logger.warning("HALTED ì‹¤í—˜ ê°ì§€ â†’ ì¦‰ì‹œ REJECT")
        return {
            "review_id": f"REV-{int(time.time()) % 10000:04d}",
            "experiment_id": experiment_result.get("experiment_id", "?"),
            "hypothesis_id": experiment_result.get("hypothesis_id", "?"),
            "verdict": VERDICT_REJECT,
            "verdict_reason": f"ì‹¤í—˜ ì‹¤í–‰ ì‹¤íŒ¨(HALTED): {experiment_result.get('error_reason', 'Unknown')}. ì¬ì‹¤í—˜ í•„ìš”.",
            "strengths": [],
            "issues": [{
                "severity": "CRITICAL",
                "aspect": "ì‹¤í–‰ ì‹¤íŒ¨",
                "detail": experiment_result.get("error_reason", "SandboxExecutor ì‹¤í–‰ ì‹¤íŒ¨"),
            }],
            "constitution_verdict": experiment_result.get("constitution_verdict", {}),
            "adaptive_criteria_used": [],
            "summary_stats": {"critical_issues": 1, "warnings": 0, "info_notes": 0, "strengths_noted": 0},
            "recommendations": ["SandboxExecutor ë¡œê·¸ í™•ì¸", "ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²€ì¦", "íšŒë¡œ ì°¨ë‹¨ê¸° ìƒíƒœ í™•ì¸"],
            "reviewed_at": datetime.utcnow().isoformat(),
        }
    
    # â”€â”€ Step 0.5: ConstitutionGuard verdict ë°˜ì˜ â”€â”€
    constitution = experiment_result.get("constitution_verdict", {})
    if constitution:
        if not constitution.get("passed", True):
            for v in constitution.get("violations", []):
                issues.append({
                    "severity": "CRITICAL",
                    "aspect": "í—Œë²• ìœ„ë°˜",
                    "detail": v,
                })
        for w in constitution.get("warnings", []):
            issues.append({
                "severity": "WARNING",
                "aspect": "í—Œë²• ê²½ê³ ",
                "detail": w,
            })
        if constitution.get("analysis_level") == "exploratory":
            strengths.append("íƒìƒ‰ì  ë¶„ì„ ìˆ˜ì¤€ìœ¼ë¡œ ì¸ê³¼ ì£¼ì¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    
    # 1. í‘œë³¸ í¬ê¸° ê²€ì¦
    n = experiment_result.get("sample_size", 0)
    if n < CRITIQUE_CRITERIA["sample_size"]["min_threshold"]:
        issues.append({
            "severity": "WARNING",
            "aspect": "í‘œë³¸ í¬ê¸°",
            "detail": CRITIQUE_CRITERIA["sample_size"]["warning"].format(n=n)
        })
    else:
        strengths.append(CRITIQUE_CRITERIA["sample_size"]["pass"].format(n=n))
    
    # 2. íš¨ê³¼ í¬ê¸° ê²€ì¦
    ate = experiment_result.get("ate", 0)
    if abs(ate) < CRITIQUE_CRITERIA["effect_size"]["min_threshold"]:
        issues.append({
            "severity": "CRITICAL",
            "aspect": "íš¨ê³¼ í¬ê¸°",
            "detail": CRITIQUE_CRITERIA["effect_size"]["warning"].format(ate=f"{ate:,.0f}")
        })
    else:
        strengths.append(CRITIQUE_CRITERIA["effect_size"]["pass"].format(ate=f"{ate:,.0f}"))
    
    # 3. ì„œë¸Œê·¸ë£¹ ì´ì§ˆì„± ê²€ì¦
    subgroups = experiment_result.get("subgroup_analysis", {})
    significant_count = 0
    marginal_count = 0
    
    for moderator, sub in subgroups.items():
        p = sub.get("heterogeneity_p_value", 1.0)
        if sub.get("is_significant", False):
            significant_count += 1
            if p > 0.01:  # pê°€ ê²½ê³„ì„ (0.01~0.05)
                issues.append({
                    "severity": "INFO",
                    "aspect": "ì´ì§ˆì„± ê²½ê³„",
                    "detail": CRITIQUE_CRITERIA["heterogeneity"]["warning"].format(moderator=moderator, p=f"{p:.4f}")
                })
        else:
            marginal_count += 1
    
    if significant_count > 0:
        strengths.append(f"{significant_count}ê°œ ì„œë¸Œê·¸ë£¹ì—ì„œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì´ì§ˆì  ì²˜ë¦¬ íš¨ê³¼ í™•ì¸.")
    
    # 4. ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦
    perf = experiment_result.get("model_performance", {})
    r2_treated = perf.get("r2_treated", 0)
    r2_control = perf.get("r2_control", 0)
    
    if r2_treated < 0.2 or r2_control < 0.2:
        issues.append({
            "severity": "WARNING",
            "aspect": "ëª¨ë¸ ì í•©ë„",
            "detail": f"RÂ² ì„±ëŠ¥(Treated={r2_treated:.3f}, Control={r2_control:.3f})ì´ ë‚®ì•„ ê²°ê³¼ í•´ì„ì— ì£¼ì˜ í•„ìš”."
        })
    else:
        strengths.append(f"ëª¨ë¸ ì í•©ë„ ì–‘í˜¸ (RÂ²: Treated={r2_treated:.3f}, Control={r2_control:.3f}).")
    
    # 4.5. Ground Truth ê²€ì¦ (estimation_accuracy â€” STEAM í•©ì„± ë°ì´í„° ì „ìš©)
    est_acc = experiment_result.get("estimation_accuracy", {})
    if est_acc:
        rmse = est_acc.get("rmse", float("inf"))
        bias = est_acc.get("bias", float("inf"))
        coverage = est_acc.get("coverage_rate", 0)
        corr = est_acc.get("correlation", 0)
        
        # Coverage ê²€ì¦: true_cateê°€ CI ì•ˆì— ìˆëŠ” ë¹„ìœ¨
        # ì£¼ì˜: LinearDMLì˜ CATE CIëŠ” êµ¬ì¡°ì ìœ¼ë¡œ ì¢ì€ ê²½í–¥ (bootstrap ììœ ë„ ê³¼ëŒ€ì¶”ì •)
        # Coverageê°€ ë‚®ì•„ë„ ATE ì¶”ì • ìì²´ëŠ” ì •ìƒì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ CRITICALì´ ì•„ë‹Œ WARNING
        if coverage < 0.1:
            issues.append({
                "severity": "WARNING",
                "aspect": "Ground Truth Coverage",
                "detail": f"Coverage={coverage:.1%} â€” CATE CIê°€ ì¢ìŒ. CausalForestDML ì¬ì‹¤í—˜ ê¶Œì¥."
            })
        elif coverage < 0.5:
            issues.append({
                "severity": "WARNING",
                "aspect": "Ground Truth Coverage",
                "detail": f"Coverage={coverage:.1%} â€” CI í­ì´ ì¢‹ìŒ. CausalForestDMLë¡œ ì¬ì‹¤í—˜ ê¶Œì¥."
            })
        elif coverage >= 0.85:
            strengths.append(f"Ground Truth Coverage ìš°ìˆ˜: {coverage:.1%}")
        
        # Bias ê²€ì¦: ì¶”ì • ATEì™€ ì°¸ ATE ì°¨ì´
        ate_val = abs(ate) if ate else 1
        bias_ratio = abs(bias) / (ate_val + 1e-8)
        if bias_ratio > 0.5:
            issues.append({
                "severity": "WARNING",
                "aspect": "ì¶”ì • í¸í–¥",
                "detail": f"Bias={bias:.4f} (í¸í–¥ë¹„={bias_ratio:.1%}) â€” ì¶”ì •ê°’ì´ ì°¸ê°’ì—ì„œ 50% ì´ìƒ ê´´ë¦¬."
            })
        
        # RMSE ê²€ì¦
        if rmse < 1.0:
            strengths.append(f"CATE ì¶”ì • ì •í™•ë„ ìš°ìˆ˜: RMSE={rmse:.4f}, Corr={corr:.3f}")
        
        # Correlation ê²€ì¦: CATE ì´ì§ˆì„± ë°©í–¥ ì¼ì¹˜
        if corr > 0.5:
            strengths.append(f"CATE ì´ì§ˆì„± ë°©í–¥ ì¼ì¹˜: r={corr:.3f}")
            if corr > 0.9:
                strengths.append(f"ğŸ¯ CATE ì¶”ì • ë°©í–¥ì„± ê±°ì˜ ì™„ë²½ (r={corr:.3f}) â€” DML ëª¨ë¸ì´ ì°¸ ì´ì§ˆì„±ì„ ì •í™•íˆ í¬ì°©")
        elif corr < 0.1:
            issues.append({
                "severity": "INFO",
                "aspect": "CATE ì´ì§ˆì„±",
                "detail": f"CATE ì¶”ì •ê³¼ ì°¸ê°’ì˜ ìƒê´€ì´ ë‚®ìŒ (r={corr:.3f}). ì´ì§ˆì„± íƒì§€ í•œê³„."
            })
    
    # 5. ì ì‘í˜• ê²€í†  ê¸°ì¤€ (MethodRegistry UCB1 ê¸°ë°˜)
    generation = 1 + min(len(kg.graph.edges) // 10, 3)
    adaptive_criteria = method_registry.select_methods("review", count=3, generation=generation)
    applied_criteria_names = []
    
    for criterion in adaptive_criteria:
        criterion_name = criterion.name
        applied_criteria_names.append(criterion_name)
        
        # E-value ë¯¼ê°ë„ ë¶„ì„
        if "ë¯¼ê°ë„" in criterion_name or "E-value" in criterion_name:
            issues.append({
                "severity": "INFO",
                "aspect": "E-value ë¯¼ê°ë„",
                "detail": f"[ì ì‘í˜• Gen {criterion.generation}] E-value ë¶„ì„: ë¯¸ê´€ì¸¡ êµë€ì— ëŒ€í•œ ê²°ê³¼ ê°•ê±´ì„± í™•ì¸ í•„ìš”."
            })
        # ë‹¤ì¤‘ ë¹„êµ ë³´ì •
        elif "ë‹¤ì¤‘ ë¹„êµ" in criterion_name:
            if len(subgroups) > 2:
                issues.append({
                    "severity": "WARNING",
                    "aspect": "ë‹¤ì¤‘ ë¹„êµ ë³´ì •",
                    "detail": f"[ì ì‘í˜• Gen {criterion.generation}] {len(subgroups)}ê°œ ê²€ì • â†’ BH ë³´ì • ì ìš© ê¶Œì¥."
                })
        # ì™¸ë¶€ íƒ€ë‹¹ë„
        elif "ì™¸ë¶€ íƒ€ë‹¹ë„" in criterion_name:
            issues.append({
                "severity": "INFO",
                "aspect": "ì™¸ë¶€ íƒ€ë‹¹ë„",
                "detail": f"[ì ì‘í˜• Gen {criterion.generation}] ê²°ê³¼ì˜ ì™¸ë¶€ ëª¨ì§‘ë‹¨ ì¼ë°˜í™” ê°€ëŠ¥ì„± ê²€í†  í•„ìš”."
            })
        # ì¬í˜„ì„±
        elif "ì¬í˜„ì„±" in criterion_name:
            strengths.append(f"[ì ì‘í˜• Gen {criterion.generation}] ì¬í˜„ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸ ì ìš© ì™„ë£Œ.")
    
    # â”€â”€ Gemini ì •ì„±ì  í‰ê°€ (2ì°¨ íŒì •) â”€â”€
    gemini_critique = None
    gemini_score = 0
    if is_gemini_available():
        eval_result = evaluate_experiment(experiment_result)
        if eval_result:
            gemini_critique = eval_result.get("critique", "")
            gemini_score = eval_result.get("score", 5)
            
            # Gemini ë¹„í‰ì„ ì´ìŠˆ ë˜ëŠ” ê°•ì ì— ì¶”ê°€
            if gemini_score < 6:
                issues.append({
                    "severity": "WARNING",
                    "aspect": "AI Reviewer",
                    "detail": f"[Gemini Score {gemini_score}/10] {gemini_critique}"
                })
            else:
                strengths.append(f"[AI Reviewer] {gemini_critique} (Score: {gemini_score}/10)")
    
    # 6. ìµœì¢… íŒì • (Gemini ì ìˆ˜ ë°˜ì˜)
    critical_count = sum(1 for i in issues if i["severity"] == "CRITICAL")
    warning_count = sum(1 for i in issues if i["severity"] == "WARNING")
    
    # Gemini ì ìˆ˜ê°€ ë§¤ìš° ë‚®ìœ¼ë©´ REVISE ê°•ì œ
    if gemini_score > 0 and gemini_score <= 3:
        warning_count += 3 # ê°•ì œë¡œ REVISE ìœ ë„
    
    if critical_count > 0:
        verdict = "REJECT"
        verdict_reason = "ì¹˜ëª…ì  ê²°í•¨ì´ ë°œê²¬ë˜ì–´ ì¬ì‹¤í—˜ì´ í•„ìš”í•©ë‹ˆë‹¤."
    elif warning_count >= 3:
        verdict = "REVISE"
        verdict_reason = "ë‹¤ìˆ˜ì˜ ê²½ê³  ì‚¬í•­ì´ ìˆì–´ ë°©ë²•ë¡  ìˆ˜ì • í›„ ì¬ì œì¶œì´ ê¶Œì¥ë©ë‹ˆë‹¤."
    else:
        verdict = "ACCEPT"
        verdict_reason = "ë°©ë²•ë¡ ì  ê±´ì „ì„±ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. Knowledge Graph ë°˜ì˜ì„ ìŠ¹ì¸í•©ë‹ˆë‹¤."
    
    # ë³´ìƒ í”¼ë“œë°±: íŒì • ê²°ê³¼ì— ë”°ë¼ ê¸°ì¤€ë³„ ë³´ìƒ
    reward_map = {"ACCEPT": 1.0, "REVISE": 0.5, "REJECT": 0.3}
    for criterion in adaptive_criteria:
        method_registry.reward_method(criterion.name, "review", reward_map.get(verdict, 0.5))
    
    return {
        "review_id": f"REV-{int(time.time()) % 10000:04d}",
        "experiment_id": experiment_result.get("experiment_id", "?"),
        "hypothesis_id": experiment_result.get("hypothesis_id", "?"),
        "verdict": verdict,
        "verdict_reason": verdict_reason,
        "strengths": strengths,
        "issues": issues,
        "adaptive_criteria_used": applied_criteria_names,
        "summary_stats": {
            "critical_issues": critical_count,
            "warnings": warning_count,
            "info_notes": sum(1 for i in issues if i["severity"] == "INFO"),
            "strengths_noted": len(strengths),
            "adaptive_criteria": len(applied_criteria_names),
        },
        "recommendations": generate_recommendations(verdict, issues),
        "reviewed_at": datetime.utcnow().isoformat(),
    }


def generate_recommendations(verdict: str, issues: list) -> list[str]:
    """íŒì • ê²°ê³¼ì— ë”°ë¥¸ ê¶Œì¥ ì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    recommendations = []
    
    if verdict == "REJECT":
        recommendations.append("ì‹¤í—˜ ì„¤ê³„ë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ ì¬ê²€í† í•˜ê³ , í•©ì„±/ì‹¤í—˜ ë°ì´í„°ë¡œ íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    
    for issue in issues:
        if issue["aspect"] == "í‘œë³¸ í¬ê¸°":
            recommendations.append("ë” í° ë°ì´í„°ì…‹ì„ í™•ë³´í•˜ê±°ë‚˜, Bootstrap ê¸°ë²•ìœ¼ë¡œ ì‹ ë¢°êµ¬ê°„ì„ ë³´ê°•í•˜ì„¸ìš”.")
        elif issue["aspect"] == "ë‹¤ì¤‘ ë¹„êµ":
            recommendations.append("Bonferroni ë³´ì •(Î±/k)ì„ ì ìš©í•˜ì—¬ ê°€ì–‘ì„±(False Positive) ìœ„í—˜ì„ ì¤„ì´ì„¸ìš”.")
        elif issue["aspect"] == "ëª¨ë¸ ì í•©ë„":
            recommendations.append("í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë˜ëŠ” ë¹„ì„ í˜• ëª¨ë¸(GBM, Neural Net)ì„ ê³ ë ¤í•˜ì„¸ìš”.")
    
    if verdict == "ACCEPT":
        recommendations.append("ê²°ê³¼ë¥¼ Knowledge Graphì— í™•ì • ë°˜ì˜í•˜ê³ , í›„ì† ì—°êµ¬ ì£¼ì œë¥¼ ë„ì¶œí•˜ì„¸ìš”.")
    
    return recommendations


def run_critic_cycle() -> list[dict]:
    """
    Criticì˜ ì „ì²´ ë¦¬ë·° ì‚¬ì´í´ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Returns:
        list[dict]: ë¦¬ë·° ê³¼ì • ë¡œê·¸
    """
    logs = []
    
    def log(step: str, message: str):
        entry = {"step": step, "message": message, "timestamp": datetime.utcnow().isoformat()}
        logs.append(entry)
        return entry
    
    # Phase 1: ê²€ì¦ ì™„ë£Œëœ ì‹¤í—˜ ê²°ê³¼ ì¡°íšŒ
    log("FETCH", "Knowledge Graphì—ì„œ ìµœê·¼ ì‹¤í—˜ ê²°ê³¼ ì¡°íšŒ ì¤‘...")
    time.sleep(0.3)
    
    # KGì—ì„œ ì‹¤í—˜ ê²°ê³¼ê°€ ìˆëŠ” ì—£ì§€ íƒìƒ‰
    verified_edges = []
    for u, v, data in kg.graph.edges(data=True):
        if data.get("verified", False) and data.get("experiment_id"):
            verified_edges.append({
                "source": u, "target": v,
                "experiment_id": data["experiment_id"],
                "hypothesis_id": data.get("hypothesis_id", "?"),
                "hypothesis_text": data.get("hypothesis_text", ""),
            })
    
    if not verified_edges:
        log("ABORT", "ë¦¬ë·° ëŒ€ìƒ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. Engineer(Tesla)ì˜ í™œì„±í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return logs
    
    target = verified_edges[0]
    log("FETCH", f"ì‹¤í—˜ [{target['experiment_id']}] / ê°€ì„¤ [{target['hypothesis_id']}] ë¦¬ë·° ì‹œì‘.")
    
    # Phase 2: ì‹¤í—˜ ê²°ê³¼ ì¬êµ¬ì„± (KG ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
    log("RECONSTRUCT", "KGì—ì„œ ì‹¤í—˜ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    
    nodes = list(kg.graph.nodes(data=True))
    confounders = [n for n, d in nodes if d.get("category") == "Confounder"]
    
    # KG ì—£ì§€ì—ì„œ ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    edge_data = {}
    for u, v, data in kg.graph.edges(data=True):
        if data.get("experiment_id") == target["experiment_id"]:
            edge_data = data
            break
    
    # ì‹¤ì œ KG ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ê³¼ êµ¬ì„±
    experiment_result = {
        "experiment_id": target["experiment_id"],
        "hypothesis_id": target["hypothesis_id"],
        "sample_size": edge_data.get("sample_size", 2000),
        "ate": edge_data.get("ate", 0),
        "ate_ci": edge_data.get("ate_ci", []),
        "conclusion": edge_data.get("conclusion", "N/A"),
        "method": edge_data.get("method", "DML"),
        "estimator": edge_data.get("estimator", "LinearDML"),
        "subgroup_analysis": edge_data.get("subgroup_analysis", {
            mod: {
                "is_significant": False,
                "heterogeneity_p_value": 1.0,
            } for mod in confounders
        }),
        "model_performance": edge_data.get("model_performance", {
            "r2_treated": 0.3,
            "r2_control": 0.25,
        }),
        "constitution_verdict": edge_data.get("constitution_verdict", {}),
    }
    
    # Phase 3: ë¹„íŒì  ê²€í† 
    log("REVIEW", "ë°©ë²•ë¡ ì  íƒ€ë‹¹ì„± ê²€ì¦ ì¤‘...")
    time.sleep(0.5)
    
    review = review_experiment(experiment_result)
    
    # ê°•ì  ë¡œê¹…
    for strength in review["strengths"]:
        log("STRENGTH", f"âœ… {strength}")
    
    # ë¬¸ì œì  ë¡œê¹…
    for issue in review["issues"]:
        emoji = "ğŸ”´" if issue["severity"] == "CRITICAL" else "ğŸŸ¡" if issue["severity"] == "WARNING" else "ğŸ”µ"
        log("ISSUE", f"{emoji} [{issue['severity']}] {issue['aspect']}: {issue['detail']}")
    
    # Phase 4: ìµœì¢… íŒì •
    time.sleep(0.2)
    verdict_emoji = "ğŸŸ¢" if review["verdict"] == "ACCEPT" else "ğŸŸ¡" if review["verdict"] == "REVISE" else "ğŸ”´"
    log("VERDICT", f"{verdict_emoji} ìµœì¢… íŒì •: **{review['verdict']}** â€” {review['verdict_reason']}")
    
    # ê¶Œì¥ì‚¬í•­ ë¡œê¹…
    for rec in review["recommendations"]:
        log("RECOMMEND", f"ğŸ’¡ {rec}")
    
    stats = kg.get_stats()
    log("COMPLETE", f"ë¦¬ë·° ì™„ë£Œ [{review['review_id']}]. KG: {stats['nodes']}ë…¸ë“œ, {stats['edges']}ì—£ì§€. â†’ Archivistì—ê²Œ ê¸°ë¡ ìš”ì²­.")
    
    return logs
