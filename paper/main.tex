% WhyLab: Causal Audit Framework for Autonomous Agent Self-Improvement
% Target: AAAI 2027 / KDD ADS Track
% Draft skeleton — fill Experiments after shadow deployment data collection

\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\title{Causal Auditing of Autonomous Agents in the Wild:\\
A Pragmatic Framework for Safe Self-Improvement}

\author{Yesol Heo}

\begin{document}
\maketitle

\begin{abstract}
Autonomous AI agents that self-improve through feedback loops face a
critical risk: hallucination-driven policy oscillation leading to
performance divergence. We present WhyLab, a pragmatic causal audit
framework that (1) detects causal drift via information-theoretic
weighted indices, (2) quantifies robustness against unobserved
confounders using E-values, and (3) guarantees bounded strategy updates
through Lyapunov stability analysis. Deployed as a shadow system
alongside production agents, WhyLab's cost-aware circuit breaker and
three-phase promotion pipeline ensure safe deployment under real-world
API budget and latency constraints.

% TODO: Add quantitative results after shadow deployment (3-4 months)
\end{abstract}


%% ============================================================
\section{Introduction}

% The problem: agents self-improve but may diverge
% Why existing tools (DoWhy, EconML) are insufficient
% Our contribution: pragmatic, deployment-tested causal audit
% NOT: "world's first" or SOTA claims without real data

\textbf{[PLACEHOLDER — write after shadow deployment data is collected]}


%% ============================================================
\section{Methodology}

% Mathematical backbone: R1, R2, R5
% Honest scope: these are production-ready components

\subsection{Information-Theoretic Drift Index (R1)}

\begin{definition}[Drift Index]
Given verdict, ATE, and confidence signal streams $S_1, S_2, S_3$
with Shannon entropies $H(S_i)$, the drift index is:
\[
DI = \sum_{i=1}^{3} w_i \cdot d_i, \quad
w_i = \frac{1/(H(S_i) + \epsilon)}{\sum_j 1/(H(S_j) + \epsilon)}
\]
where $\epsilon = 0.1$ prevents zero-entropy explosion and
$w_i \leq 0.6$ prevents winner-takes-all collapse.
Bin count follows Sturges' Rule: $k = \lceil 1 + \log_2 N \rceil$.
\end{definition}


\subsection{Sensitivity Analysis via E-value (R2)}

\begin{definition}[E-value \cite{vanderweele2017}]
For risk ratio $RR = \exp(|ATE| / \sigma_{pre})$:
\[
E = RR + \sqrt{RR \cdot (RR - 1)}
\]
Interpretation: an unobserved confounder must associate with both
treatment and outcome at $RR \geq E$ to nullify the observed effect.
\end{definition}

Robustness Value $RV_q \approx \sqrt{\partial R^2_T}$
provides the Cinelli \& Hazlett (2020) partial $R^2$ bound.


\subsection{Lyapunov-Bounded Damping (R5)}

\begin{theorem}[Convergence of Adaptive Damping]
Let $\theta_t$ be the agent's strategy, updated as
$\theta_{t+1} = \theta_t - \zeta_t \hat{g}_t$ where
$\hat{g}_t = g_t + \omega_t$ ($g_t$: true causal gradient,
$\omega_t$: noise from LLM judgment and environmental drift).

Define Lyapunov energy $V(\theta_t) = \frac{1}{2}\|\theta_t - \theta^*\|^2$.

Then $\mathbb{E}[\Delta V] \leq 0$ if and only if:
\[
\zeta_t \leq \zeta_{max} = \frac{2 \langle \theta_t - \theta^*, g_t \rangle}
{\mathbb{E}[\|\hat{g}_t\|^2]}
\]

Since $\mathbb{E}[\|\hat{g}_t\|^2] \propto (DI + \epsilon_{ARES})$,
reducing $\zeta_t$ under high uncertainty is not heuristic but
\emph{mathematically necessary} for stability.
\end{theorem}

\begin{proof}
\textbf{[Full proof in Appendix A]}
\end{proof}

\begin{corollary}[Heavy-Tail Robustness]
When gradient noise $\omega_t$ follows a heavy-tailed distribution
(e.g., Pareto with infinite variance), $\zeta_{max} \to 0$,
causing permanent learning deadlock. To prevent this, we enforce
a hard floor-ceiling constraint:
\[
\zeta_t \in [\epsilon_{\text{floor}}, \mathcal{C}]
\quad \text{where } \epsilon_{\text{floor}} = 0.01,\; \mathcal{C} = 0.8
\]
This guarantees $\zeta_t \geq \epsilon_{\text{floor}} > 0$
even under arbitrarily large noise variance, ensuring the agent
continues learning at a minimal but non-zero rate.
\end{corollary}

\begin{remark}[Observable Energy Proxy]
Since the optimal strategy $\theta^*$ is unobservable in practice,
we define an empirical proxy for the Lyapunov energy function using
the agent's task reward $R_t$:
\[
V_t \approx \frac{1}{2}(R_{max} - R_t)^2
\]
where $R_{max}$ is the achievable upper bound (e.g., maximum observed
conversion rate over a calibration window). This proxy satisfies the
key requirement: $V_t \to 0$ as $R_t \to R_{max}$, and $V_t$ decreasing
after $\zeta$-clipping events provides empirical evidence of stabilization.
\end{remark}

%% ============================================================
\section{System Architecture \& Deployment Challenges}

% This section targets KDD ADS track reviewers
% Honest about limitations and cost tradeoffs

\subsection{Infrastructure Constraints}

Deploying a causal audit layer atop a production agent introduces
four engineering challenges, each addressed by a dedicated component:

\textbf{C1: GA4 Quota Limits.}
Google Analytics 4 imposes strict API quotas. We decouple event
ingestion via a Transactional Outbox pattern (C3): audit events are
first persisted to a local outbox table, then drained asynchronously
by a background worker. This guarantees zero data loss under quota
throttling.

\textbf{C2: Database Growth.}
With 12 monitored sites generating $\sim$500 decision events per day,
raw audit logs grow $\sim$180K rows/month. We partition by week and
aggregate via \texttt{daily\_agent\_rollup}, compressing 7 days of
raw data into a single summary row per site.

\textbf{C3: Trace Explosion.}
OpenTelemetry spans for every LLM call produce $>$10K spans/day.
We implement head-based dynamic sampling: high-drift decisions are
traced at 100\%, stable decisions at 10\%.

\textbf{C4: LLM Cost Control.}
ARES deep audit calls cost \$0.003--0.01 per evaluation. A daily
hard budget (\$10/day, 100K tokens) with automatic circuit breaker
prevents runaway costs. Breaker-blocked audits are preserved in a
Dead Letter Queue (DLQ) for offline batch recovery, ensuring no
sample loss for the experiment dataset.


\subsection{Safe Deployment via Shadow Mode}

We adopt a three-phase promotion pipeline inspired by
canary deployment practices in production ML systems:

\begin{enumerate}
\item \textbf{DRY\_RUN:} The audit engine observes agent decisions
  and computes $\zeta_{max}$, DI, and E-values, but does \emph{not}
  apply any feedback. All observations are logged for offline analysis.
\item \textbf{SHADOW\_ACTIVE:} Feedback is applied to a shadow copy
  of the agent's strategy memory. Performance is compared against
  the unmodified control path.
\item \textbf{PRODUCTION:} Full integration after statistical
  significance is established via Bayesian decision criteria.
\end{enumerate}

\textbf{Non-blocking integration.}
The shadow adapter uses fire-and-forget semantics with a 2-second
AbortController timeout. The main agent pipeline experiences
zero latency overhead: shadow failures are silently caught.

\textbf{Deterministic canary routing.}
Traffic allocation uses a hash-based deterministic router
(\texttt{djb2(cycleId) \% 100 < threshold}) rather than random
sampling, ensuring complete trace integrity across multi-step
agent decisions.

\textbf{Data integrity.}
Daily rollup snapshots are SHA-256 hashed and committed to an
append-only log via automated CI pipeline, providing a
timestamped evidence chain against post-hoc data manipulation.


%% ============================================================
\section{Experiments}

% CRITICAL: Use ONLY real shadow deployment data here
% Do NOT cite synthetic Who&When or CausalFlip as SOTA comparisons

\subsection{Live Shadow Deployment}

We structure the evaluation around three hypotheses, each
addressing a distinct contribution claim:

\textbf{H1: Lyapunov clipping prevents policy oscillation.}
Using the observable energy proxy $V_t \approx \frac{1}{2}(R_{max} - R_t)^2$
where $R_{max}$ is the rolling 30-day maximum conversion rate,
we plot $V_t$ over time with clipping events ($\zeta_t > \zeta_{max}$)
marked as vertical indicators. If $V_t$ consistently decreases or
stabilizes after clipping events, H1 is supported.

\textbf{H2: Cost circuit breaker maintains reliability under budget.}
We overlay daily traffic volume against ARES token consumption
and USD cost. When the breaker trips at the \$10 daily limit,
blocked audits are routed to the DLQ while the system continues
operating on lightweight fallback. We report the fraction of
decisions processed by real vs.\ fallback audit, and measure whether
agent performance degrades during fallback periods.

\textbf{H3: Causal audit reveals false positives in na\"ive evaluation.}
We aggregate E-values from \texttt{daily\_agent\_rollup} and report
the fraction of agent decisions that are robust to unobserved confounders
($E > 2.0$). We frame this conservatively: ``Of $N$ decisions over 30 days,
only $M$ (\%$p$) exhibited causal effects robust to moderate unmeasured
confounding. Standard A/B testing would have attributed all $N$ as successes.''

\textbf{[Data collection in progress --- results after 30-day shadow period.]}


\subsection{Ablation Study}

% Which components matter most?
% Table: Full system vs. no-R1 vs. no-R5 vs. no-C3

\textbf{[PLACEHOLDER --- run after live data collection]}


\subsection{Limitations}
\label{sec:limitations}

We explicitly acknowledge the following limitations:
\begin{itemize}
\item \textbf{ARES evaluator}: Currently uses mock LLM judges in
  the default configuration. Real-LLM opt-in mode (Claude 3.5 Haiku,
  GPT-4o-mini) is implemented with 3-second timeout and exponential
  backoff, but production-scale cost-performance tradeoffs remain
  under evaluation during the shadow deployment period.
\item \textbf{Observable energy proxy}: The Lyapunov energy function
  $V_t \approx \frac{1}{2}(R_{max} - R_t)^2$ is a pragmatic
  approximation. It assumes reward monotonicity and a known upper
  bound $R_{max}$, which may not hold in non-stationary environments.
\item \textbf{Shapley attribution}: Full MACIE computation is
  $O(2^n)$; our implementation uses a lightweight heuristic
  approximation, not exact Shapley values.
\item \textbf{Data integrity}: SHA-256 hash logs provide a
  timestamped evidence chain but do not constitute cryptographic
  proof against a determined adversary with repository admin access.
  We characterize this as an automated audit trail, not a
  blockchain-grade guarantee.
\item \textbf{DLQ recovery}: Breaker-blocked audits preserved in
  the Dead Letter Queue are processed offline. This introduces a
  survivorship bias: the real-time audit stream during high-cost
  periods reflects only fallback (mock) evaluations.
\end{itemize}


%% ============================================================
\section{Related Work}

% DoWhy, EconML, CausalML — inference tools, not audit frameworks
% ARES (Saad-Falcon et al. 2024) — our simplified version
% Lyapunov in RL — Chow et al. 2018, Berkenkamp et al. 2017

\textbf{[PLACEHOLDER]}


%% ============================================================
\section{Conclusion}

\textbf{[PLACEHOLDER — write after experiments]}


%% ============================================================
\bibliographystyle{aaai}
\bibliography{references}

% Key references:
% \cite{vanderweele2017} VanderWeele & Ding 2017 — E-value
% \cite{cinelli2020} Cinelli & Hazlett 2020 — Partial R²
% \cite{jaynes2003} Jaynes 2003 — Maximum Entropy
% \cite{sturges1926} Sturges 1926 — Binning rule
% \cite{agresti1998} Agresti & Coull 1998 — Beta-Binomial CI

\end{document}
