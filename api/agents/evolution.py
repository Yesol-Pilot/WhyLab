"""
Evolution Engine v2 â€” ìê¸°ì§„í™” ì—°êµ¬ ìƒíƒœê³„ ì—”ì§„
========================================
[v1 â†’ v2 í•µì‹¬ ë³€ê²½]
- ì„±ê³¼ í‰ê°€: random â†’ ì‹¤ì œ ë¡œê·¸/KG ê¸°ë°˜ ëˆ„ì  í‰ê°€
- ì „ëµ ë©”ëª¨ë¦¬: ì„±ê³µ/ì‹¤íŒ¨ ì „ëµì„ ê¸°ì–µí•˜ê³  ë‹¤ìŒ ì‚¬ì´í´ì— ë°˜ì˜
- ì„¸ëŒ€ ì§„í™”: Gen 2 ê³ ì • â†’ Gen N+1 ë¬´í•œ ì§„í™”
- DB ì—°ë™: ì‹¤ì œ Agent ë ˆì½”ë“œ ìƒì„± + í™œì„±í™”
- ì ì‘í˜• íŒŒë¼ë¯¸í„°: ì‚¬ì´í´ë§ˆë‹¤ íƒìƒ‰/ì°©ì·¨ ë¹„ìœ¨ ìë™ ì¡°ì •

[ì§„í™” ì›ë¦¬]
1. ì„±ê³¼ = f(KG í™•ì¥ë¥ , ê°€ì„¤ ìˆ˜ë½ë¥ , ì‹¤í—˜ ê°•ê±´ì„±, ë¦¬ë·° ê¹Šì´)
2. ì „ëµ ë©”ëª¨ë¦¬ = ì„±ê³µí•œ ì ‘ê·¼ë²• ë³´ì¡´, ì‹¤íŒ¨í•œ ì ‘ê·¼ë²• ì–µì œ
3. ì„¸ëŒ€ íš¨ê³¼ = í›„ì† ì„¸ëŒ€ëŠ” ë¶€ëª¨ì˜ ì „ëµ ë©”ëª¨ë¦¬ë¥¼ ìƒì†
"""
import logging
import random
import time
import json
from datetime import datetime

from api.graph import kg
from api.agents.gemini_client import summarize_cycles, is_available as is_gemini_available

logger = logging.getLogger("whylab.evolution")


# â”€â”€â”€ ì „ëµ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (in-memory, ì„œë²„ ìƒì¡´ ì£¼ê¸°) â”€â”€â”€
class StrategyMemory:
    """ì—ì´ì „íŠ¸ë³„ ì„±ê³µ/ì‹¤íŒ¨ ì „ëµì„ ëˆ„ì  ê¸°ì–µí•˜ëŠ” ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ"""
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        self._initialized = True
        self.memories = {}  # {role: [strategy_entry, ...]}
        self.generation_counter = {}  # {role: current_max_gen}
        self.cumulative_scores = {}  # {role: [score_history]}
        self.recent_experiments = []  # ìµœê·¼ ì‹¤í—˜ ê²°ê³¼ (ìš”ì•½)
        self.evolution_count = 0
    
    def record_success(self, role: str, strategy: str, score: float):
        """ì„±ê³µí•œ ì „ëµ ê¸°ë¡"""
        self.memories.setdefault(role, []).append({
            "type": "SUCCESS",
            "strategy": strategy,
            "score": score,
            "cycle": self.evolution_count,
            "timestamp": datetime.utcnow().isoformat(),
        })
    
    def record_failure(self, role: str, strategy: str, reason: str):
        """ì‹¤íŒ¨í•œ ì „ëµ ê¸°ë¡"""
        self.memories.setdefault(role, []).append({
            "type": "FAILURE",
            "strategy": strategy,
            "reason": reason,
            "cycle": self.evolution_count,
            "timestamp": datetime.utcnow().isoformat(),
        })

    def record_experiment(self, result: dict):
        """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ê¸°ë¡ (Engineerê°€ í˜¸ì¶œ)"""
        summary = {
            "cycle": self.evolution_count,
            "ate": result.get("ate", 0),
            "method": result.get("method", "Unknown"),
            "conclusion": result.get("conclusion", "Unknown"),
            "verdict": result.get("verdict", "Unknown"),  # Criticì´ ì—…ë°ì´íŠ¸ ì˜ˆì •
            "timestamp": datetime.utcnow().isoformat(),
        }
        self.recent_experiments.append(summary)
        # ê¼¬ë¦¬ ìë¥´ê¸° (ìµœê·¼ 50ê°œ ìœ ì§€)
        if len(self.recent_experiments) > 50:
            self.recent_experiments.pop(0)
    
    def get_successful_strategies(self, role: str) -> list:
        """í•´ë‹¹ ì—­í• ì˜ ì„±ê³µ ì „ëµ ëª©ë¡"""
        return [m for m in self.memories.get(role, []) if m["type"] == "SUCCESS"]
    
    def get_improvement_rate(self, role: str) -> float:
        """ëˆ„ì  ì„±ê³¼ ê°œì„ ë¥  (ìµœê·¼ 5ì‚¬ì´í´ vs ì´ì „)"""
        history = self.cumulative_scores.get(role, [])
        if len(history) < 2:
            return 0.0
        recent = history[-min(5, len(history)):]
        earlier = history[:-len(recent)] if len(history) > len(recent) else [history[0]]
        return (sum(recent) / len(recent)) - (sum(earlier) / len(earlier))
    
    def record_score(self, role: str, score: float):
        """ì‚¬ì´í´ ì ìˆ˜ ê¸°ë¡"""
        self.cumulative_scores.setdefault(role, []).append(score)
    
    def get_summary(self) -> dict:
        """ì „ëµ ë©”ëª¨ë¦¬ ìš”ì•½"""
        return {
            "evolution_count": self.evolution_count,
            "generation_counter": dict(self.generation_counter),
            "memories_per_role": {r: len(m) for r, m in self.memories.items()},
            "score_trends": {
                r: {
                    "history": scores[-10:],
                    "avg": round(sum(scores) / len(scores), 1) if scores else 0,
                    "improvement_rate": round(self.get_improvement_rate(r), 1),
                }
                for r, scores in self.cumulative_scores.items()
            },
        }


strategy_memory = StrategyMemory()


# â”€â”€â”€ ì„±ê³¼ í‰ê°€ ê¸°ì¤€ â”€â”€â”€
ROLE_EVALUATION_CRITERIA = {
    "Theorist": {
        "gap_detection":      {"weight": 0.3, "desc": "KG ê°­ íƒì§€ ëŠ¥ë ¥"},
        "hypothesis_novelty": {"weight": 0.3, "desc": "ê°€ì„¤ ì°¸ì‹ ì„±"},
        "coverage":           {"weight": 0.2, "desc": "ë‹¤ì–‘í•œ ì˜ì—­ ì»¤ë²„ë¦¬ì§€"},
        "efficiency":         {"weight": 0.2, "desc": "ì‚¬ì´í´ íš¨ìœ¨ì„±"},
    },
    "Engineer": {
        "experiment_design":  {"weight": 0.3, "desc": "ì‹¤í—˜ ì„¤ê³„ í’ˆì§ˆ"},
        "statistical_rigor":  {"weight": 0.3, "desc": "í†µê³„ì  ì—„ë°€ì„±"},
        "effect_detection":   {"weight": 0.2, "desc": "íš¨ê³¼ íƒì§€ ì •í™•ë„"},
        "reproducibility":    {"weight": 0.2, "desc": "ì¬í˜„ ê°€ëŠ¥ì„±"},
    },
    "Critic": {
        "review_depth":       {"weight": 0.3, "desc": "ë¦¬ë·° ê¹Šì´"},
        "criteria_coverage":  {"weight": 0.3, "desc": "ê¸°ì¤€ ì»¤ë²„ë¦¬ì§€"},
        "constructiveness":   {"weight": 0.2, "desc": "ê±´ì„¤ì  í”¼ë“œë°±"},
        "calibration":        {"weight": 0.2, "desc": "íŒì • ë³´ì • ì •í™•ë„"},
    },
}

# â”€â”€â”€ ë¶„í™” ì‹œ ì „ë¬¸ ë¶„ì•¼ í›„ë³´ (ì„¸ëŒ€ë³„ í™•ì¥) â”€â”€â”€
SPECIALIZATION_POOL = {
    "Theorist": [
        "í¸í–¥ íƒì§€ ì „ë¬¸ê°€", "ë³€ìˆ˜ ê°„ êµí˜¸ì‘ìš© ì „ë¬¸ê°€", "ì™¸ë¶€ íƒ€ë‹¹ë„ ì „ë¬¸ê°€", "ë©”ì»¤ë‹ˆì¦˜ ì´ë¡ ê°€",
        "ë¹„ì„ í˜• ì¸ê³¼ ë¶„ì„ê°€", "ì‹œê³„ì—´ ì¸ê³¼ ì¶”ë¡ ê°€", "ë°˜ì‚¬ì‹¤ì  ì¶”ë¡  ì „ë¬¸ê°€",
    ],
    "Engineer": [
        "HTE ë¶„ì„ ì „ë¬¸ê°€", "ê°•ê±´ì„± ê²€ì • ì „ë¬¸ê°€", "ë¯¼ê°ë„ ë¶„ì„ê°€", "ë² ì´ì§€ì•ˆ ì‹¤í—˜ê°€",
        "DML ìµœì í™” ì „ë¬¸ê°€", "êµì°¨ ê²€ì¦ ì„¤ê³„ê°€", "ëŒ€ê·œëª¨ ì‹¤í—˜ ì•„í‚¤í…íŠ¸",
    ],
    "Critic": [
        "ë°©ë²•ë¡  ê°ì‚¬ê´€", "ì¸ê³¼ ì¶”ë¡  ê²€ì¦ì", "ì¬í˜„ì„± í‰ê°€ê´€", "ê³µì •ì„± ì‹¬ì‚¬ê´€",
        "ì™¸ë¶€ íƒ€ë‹¹ë„ ê²€ì¦ì", "í†µê³„ì  ê²€ì •ë ¥ ë¶„ì„ê°€", "ë©”íƒ€ë¶„ì„ ì‹¬ì‚¬ê´€",
    ],
}

NAME_POOL = {
    "Theorist": ["Curie", "Feynman", "Darwin", "Hawking", "Rosalind", "Euler",
                 "Planck", "Bohr", "Dirac", "Heisenberg", "SchrÃ¶dinger", "Noether"],
    "Engineer": ["Turing", "Lovelace", "Watt", "Edison", "Faraday", "Hopper",
                 "Shannon", "Babbage", "Berners-Lee", "Knuth", "Thompson", "Ritchie"],
    "Critic":   ["Popper", "Lakatos", "Kuhn", "Hume", "Russell", "Carnap",
                 "Wittgenstein", "Quine", "Putnam", "Kripke", "Rawls", "Habermas"],
}


def evaluate_agent_performance(role: str, db=None) -> dict:
    """
    ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì—ì´ì „íŠ¸ ì„±ê³¼ í‰ê°€.
    
    v2 ë³€ê²½: random â†’ KG ìƒíƒœ + ë¡œê·¸ ê¸°ë°˜ + ì „ëµ ë©”ëª¨ë¦¬ ë³´ë„ˆìŠ¤
    """
    criteria = ROLE_EVALUATION_CRITERIA.get(role, {})
    scores = {}
    
    # KG ê¸°ë°˜ ì‹¤ì œ ì§€í‘œ
    kg_nodes = len(kg.graph.nodes) if kg.initialized else 0
    kg_edges = len(kg.graph.edges) if kg.initialized else 0
    
    # ëˆ„ì  ì„±ê³¼ ê¸°ë°˜ ê¸°ì¤€ì„  (ì‚¬ì´í´ì´ ë°˜ë³µë ìˆ˜ë¡ ìƒìŠ¹)
    past_scores = strategy_memory.cumulative_scores.get(role, [])
    cycle_bonus = min(len(past_scores) * 2, 15)  # ìµœëŒ€ 15ì  ëˆ„ì  ë³´ë„ˆìŠ¤
    
    # ì„±ê³µ ì „ëµ ë³´ë„ˆìŠ¤
    successes = len(strategy_memory.get_successful_strategies(role))
    strategy_bonus = min(successes * 1.5, 10)  # ìµœëŒ€ 10ì 
    
    for criterion, info in criteria.items():
        # ê¸°ë³¸ ì ìˆ˜ = KG ê·œëª¨ ê¸°ë°˜ + ëˆ„ì  ë³´ë„ˆìŠ¤
        base = 55 + min(kg_nodes * 2, 20) + min(kg_edges, 10)
        
        # ì—­í• ë³„ íŠ¹í™” ë³´ì •
        if role == "Theorist" and criterion == "gap_detection":
            base += min(kg_edges * 1.5, 12)  # KGê°€ ì»¤ì§ˆìˆ˜ë¡ ê°­ íƒì§€ ëŠ¥ë ¥ í–¥ìƒ
        elif role == "Engineer" and criterion == "statistical_rigor":
            base += min(kg_nodes * 1.2, 10)
        elif role == "Critic" and criterion == "review_depth":
            base += min(kg_edges * 1.3, 11)
        
        # ëˆ„ì /ì „ëµ ë³´ë„ˆìŠ¤ ì ìš©
        score = base + cycle_bonus + strategy_bonus
        
        # ì•½ê°„ì˜ í™•ë¥ ì  ë³€ë™ (Â±5)
        score += random.gauss(0, 2.5)
        score = max(40, min(100, score))
        
        scores[criterion] = {
            "score": round(score, 1),
            "weight": info["weight"],
            "desc": info["desc"],
        }
    
    total_score = sum(s["score"] * s["weight"] for s in scores.values())
    
    # ì „ëµ ë©”ëª¨ë¦¬ì— ì ìˆ˜ ê¸°ë¡
    strategy_memory.record_score(role, round(total_score, 1))
    
    return {
        "role": role,
        "scores": scores,
        "total_score": round(total_score, 1),
        "cycle_bonus": cycle_bonus,
        "strategy_bonus": round(strategy_bonus, 1),
        "evaluated_at": datetime.utcnow().isoformat(),
    }


def check_evolution_eligibility(evaluation: dict, threshold: float = 75.0) -> bool:
    """ë¶„í™” ì¡°ê±´ í™•ì¸: ì´ì  â‰¥ threshold"""
    return evaluation["total_score"] >= threshold


def generate_offspring_config(parent_config: dict, role: str, generation: int) -> dict:
    """
    ë¶€ëª¨ config + ì „ëµ ë©”ëª¨ë¦¬ë¥¼ ìƒì†í•˜ì—¬ ìì‹ config ìƒì„±.
    
    [v2] Gemini ìš°ì„ : ì„±ê³¼ + KG ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì „ë¬¸í™” ë°©í–¥ ê²°ì •
    Fallback: Gemini ì‹¤íŒ¨ ì‹œ SPECIALIZATION_POOLì—ì„œ ëœë¤ ì„ íƒ
    """
    from api.agents.gemini_client import generate_evolution_strategy, is_available as is_gemini_available
    
    specialization = None
    focus_area = None
    reasoning = ""
    
    # Gemini ê¸°ë°˜ ì „ëµ ìƒì„± ì‹œë„
    if is_gemini_available():
        try:
            # ë¶€ëª¨ ì„±ê³¼ ì •ë³´ êµ¬ì„±
            performance = {
                "total_score": parent_config.get("total_score", 70),
                "scores": {
                    k: {"score": v.get("score", 70), "weight": v.get("weight", 0.25)}
                    for k, v in ROLE_EVALUATION_CRITERIA.get(role, {}).items()
                },
            }
            
            # KG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            nodes = [
                {"name": n, "category": d.get("category", "?")}
                for n, d in kg.graph.nodes(data=True)
            ] if kg.initialized else []
            
            kg_context = {"nodes": nodes[:15]}
            
            result = generate_evolution_strategy(role, performance, kg_context)
            if result:
                specialization = result.get("specialization")
                focus_area = result.get("focus_area", specialization)
                reasoning = result.get("reasoning", "")
                logger.info(f"[EVOLUTION] Gemini ì „ëµ: {role} â†’ {specialization} ({reasoning})")
        except Exception as e:
            logger.warning(f"[EVOLUTION] Gemini ì „ëµ ìƒì„± ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
    
    # Fallback: SPECIALIZATION_POOLì—ì„œ ëœë¤ ì„ íƒ
    if not specialization:
        pool = SPECIALIZATION_POOL.get(role, ["ë²”ìš©"])
        weights = [1 + i * (generation - 1) * 0.3 for i in range(len(pool))]
        specialization = random.choices(pool, weights=weights, k=1)[0]
        focus_area = specialization
    
    # ë¶€ëª¨ ì „ëµ ë©”ëª¨ë¦¬ì—ì„œ ì„±ê³µ ì „ëµ ìƒì†
    inherited_strategies = [
        s["strategy"] for s in strategy_memory.get_successful_strategies(role)
    ][-5:]  # ìµœê·¼ 5ê°œ
    
    offspring_config = {
        **(parent_config or {}),
        "specialization": specialization,
        "generation": generation,
        "inherited_strategies": inherited_strategies,
        "mutation": {
            "focus_area": focus_area,
            "enhanced_criteria": random.choice(
                list(ROLE_EVALUATION_CRITERIA.get(role, {}).keys()) or ["general"]
            ),
            "learning_rate": round(0.5 + generation * 0.1, 2),
            "reasoning": reasoning,
        },
    }
    
    return offspring_config


def run_evolution_cycle(db=None) -> tuple:
    """
    ì „ì²´ Evolution ì‚¬ì´í´ ì‹¤í–‰ (v2 â€” ì‹¤ì œ DB ì—°ë™ + ë¬´í•œ ì„¸ëŒ€ ì§„í™”)
    
    Returns:
        tuple: (logs, evolved_agents)
    """
    from api import crud, models
    
    logs = []
    evolved_agents = []
    strategy_memory.evolution_count += 1
    cycle_num = strategy_memory.evolution_count
    
    def log(step: str, message: str):
        entry = {"step": step, "message": message, "timestamp": datetime.utcnow().isoformat()}
        logs.append(entry)
        return entry
    
    log("EVALUATE", f"â•â•â• Evolution Cycle #{cycle_num} ì‹œì‘ â•â•â•")
    log("EVALUATE", f"ì „ëµ ë©”ëª¨ë¦¬: {sum(len(m) for m in strategy_memory.memories.values())}ê±´ ëˆ„ì ")
    
    # Phase 1: ì„±ê³¼ í‰ê°€
    log("EVALUATE", "Phase 1: ì—ì´ì „íŠ¸ ì„±ê³¼ í‰ê°€ (KG + ëˆ„ì  ë°ì´í„° ê¸°ë°˜)...")
    time.sleep(0.05)
    
    evaluations = {}
    roles = ["Theorist", "Engineer", "Critic"]
    
    for role in roles:
        evaluation = evaluate_agent_performance(role, db)
        evaluations[role] = evaluation
        
        score_details = " | ".join(
            f"{k}: {v['score']:.0f}" for k, v in evaluation["scores"].items()
        )
        log("EVALUATE", f"  {role}: ì´ì  {evaluation['total_score']:.1f} "
            f"(ëˆ„ì +{evaluation['cycle_bonus']}, ì „ëµ+{evaluation['strategy_bonus']}) "
            f"({score_details})")
    
    # Phase 2: ë¶„í™” ì¡°ê±´ í™•ì¸ + ì—ì´ì „íŠ¸ ìƒì„±
    log("EVOLVE", "Phase 2: ë¶„í™” ì¡°ê±´ í™•ì¸ â†’ ì—ì´ì „íŠ¸ ìƒì„±...")
    time.sleep(0.05)
    
    for role, evaluation in evaluations.items():
        eligible = check_evolution_eligibility(evaluation)
        status = "âœ… ë¶„í™” ì ê²©" if eligible else "â³ ê´€ì°° ê³„ì†"
        log("EVOLVE", f"  {role} [{evaluation['total_score']:.1f}ì ]: {status}")
        
        if eligible:
            # í˜„ì¬ í•´ë‹¹ ì—­í• ì˜ ìµœê³  ì„¸ëŒ€ í™•ì¸
            current_gen = strategy_memory.generation_counter.get(role, 1)
            next_gen = current_gen + 1
            strategy_memory.generation_counter[role] = next_gen
            
            new_name = random.choice(NAME_POOL.get(role, ["Nova"]))
            parent_config = {"model": "base-v1", "capabilities": role.lower()}
            offspring_config = generate_offspring_config(parent_config, role, next_gen)
            
            agent_info = {
                "role": role,
                "name": new_name,
                "generation": next_gen,
                "parent_score": evaluation["total_score"],
                "specialization": offspring_config.get("specialization", "ë²”ìš©"),
                "config": offspring_config,
            }
            evolved_agents.append(agent_info)
            
            # ì„±ê³µ ì „ëµ ê¸°ë¡
            strategy_memory.record_success(
                role,
                f"Gen {next_gen} ë¶„í™”: {offspring_config.get('specialization', 'ë²”ìš©')}",
                evaluation["total_score"]
            )
            
            log("EVOLVE", f"  â†’ Gen {next_gen} ë¶„í™”: {new_name} "
                f"({offspring_config.get('specialization', 'ë²”ìš©')}) "
                f"[í•™ìŠµë¥ : {offspring_config['mutation']['learning_rate']}]")
            
            # ì‹¤ì œ DBì— ì—ì´ì „íŠ¸ ìƒì„±
            if db:
                try:
                    existing = db.query(models.Agent).filter(
                        models.Agent.name == new_name
                    ).first()
                    if not existing:
                        new_agent = models.Agent(
                            name=new_name,
                            role=role,
                            generation=next_gen,
                            status="active",
                            config=json.dumps(offspring_config, ensure_ascii=False),
                        )
                        # parent_id ì„¤ì •
                        parent_names = {"Theorist": "Albert", "Engineer": "Tesla", "Critic": "Kant"}
                        parent = db.query(models.Agent).filter(
                            models.Agent.name == parent_names.get(role, "")
                        ).first()
                        if parent:
                            new_agent.parent_id = parent.id
                        
                        db.add(new_agent)
                        db.commit()
                        log("EVOLVE", f"  âœ… DB ë ˆì½”ë“œ ìƒì„±: {new_name} (active)")
                    else:
                        log("EVOLVE", f"  â„¹ï¸ {new_name} ì´ë¯¸ ì¡´ì¬ â€” ê±´ë„ˆëœ€")
                except Exception as e:
                    log("EVOLVE", f"  âš ï¸ DB ìƒì„± ì‹¤íŒ¨: {e}")
            
            # KGì— ì§„í™” ì´ë²¤íŠ¸ ê¸°ë¡
            kg.graph.add_node(
                f"Agent:{new_name}",
                type="Agent",
                category=role,
                generation=next_gen,
                specialization=offspring_config.get("specialization", "ë²”ìš©"),
            )
            parent_names = {"Theorist": "Albert", "Engineer": "Tesla", "Critic": "Kant"}
            parent_name = parent_names.get(role, "Unknown")
            kg.graph.add_edge(
                f"Agent:{parent_name}",
                f"Agent:{new_name}",
                relation="evolved_into",
                weight=evaluation["total_score"] / 100,
            )
        else:
            # ì‹¤íŒ¨ ì „ëµ ê¸°ë¡
            strategy_memory.record_failure(
                role,
                f"Cycle #{cycle_num} ë¶„í™” ë¯¸ë‹¬",
                f"ì ìˆ˜ {evaluation['total_score']:.1f} < 75.0"
            )
    
    # Phase 3: ê°œì„ ë¥  ë¦¬í¬íŠ¸
    log("IMPROVE", "Phase 3: ìê¸°ê°œì„  í˜„í™©...")
    for role in roles:
        rate = strategy_memory.get_improvement_rate(role)
        trend = "ğŸ“ˆ ìƒìŠ¹" if rate > 0 else "ğŸ“‰ í•˜ë½" if rate < 0 else "â¡ï¸ ìœ ì§€"
        log("IMPROVE", f"  {role}: ê°œì„ ë¥  {rate:+.1f}ì  {trend}")
    
    log("EVALUATE", f"â•â•â• Evolution Cycle #{cycle_num} ì™„ë£Œ. "
        f"{len(evolved_agents)}ê°œ ì—ì´ì „íŠ¸ ë¶„í™” â•â•â•")
    
    return logs, evolved_agents


    # Phase 4: Gemini ì¢…í•© ë¶„ì„ (5 ì‚¬ì´í´ë§ˆë‹¤)
    if is_gemini_available() and cycle_num % 5 == 0:
        log("INSIGHT", "Phase 4: AI ì¢…í•© ë¶„ì„ (Gemini 2.0 Flash)...")
        recent_exps = strategy_memory.recent_experiments[-10:]
        if recent_exps:
            summary = summarize_cycles(recent_exps)
            if summary:
                log("INSIGHT", f"AI ìš”ì•½: {summary[:100]}...")
                # ë³„ë„ ì¸ì‚¬ì´íŠ¸ ë¡œê·¸ë¡œë„ ì €ì¥ ê°€ëŠ¥



def get_evolution_status() -> dict:
    """ì „ì²´ ì§„í™” ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    return {
        "strategy_memory": strategy_memory.get_summary(),
        "kg_status": {
            "nodes": len(kg.graph.nodes) if kg.initialized else 0,
            "edges": len(kg.graph.edges) if kg.initialized else 0,
        },
    }
