# -*- coding: utf-8 -*-
"""BenchmarkCell â€” í•™ìˆ  ë²¤ì¹˜ë§ˆí¬ ìë™ í‰ê°€.

IHDP/ACIC/Jobs ë²¤ì¹˜ë§ˆí¬ì—ì„œ WhyLab ë©”íƒ€ëŸ¬ë„ˆë¥¼ í‰ê°€í•˜ê³ ,
ê¸°ì¤€ì„ (LinearDML, CausalForest) ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµí‘œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.

í‰ê°€ ì§€í‘œ:
  âˆšPEHE = âˆš(1/n Â· Î£(Ï„Ì‚(x)-Ï„(x))Â²)
  ATE Bias = |ATE_est - ATE_true|
  Coverage = ì˜ˆì¸¡êµ¬ê°„ ì ì¤‘ë¥ 
  CI Width = í‰ê·  êµ¬ê°„ í­
"""

from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd

from engine.cells.base_cell import BaseCell
from engine.cells.meta_learner_cell import (
    SLearner, TLearner, XLearner, DRLearner, RLearner,
)
from engine.config import WhyLabConfig
from engine.data.benchmark_data import BENCHMARK_REGISTRY, BenchmarkData

logger = logging.getLogger(__name__)


class BenchmarkCell(BaseCell):
    """í•™ìˆ  ë²¤ì¹˜ë§ˆí¬ ìë™ í‰ê°€ ì…€.

    íŒŒì´í”„ë¼ì¸ ë…ë¦½ ì‹¤í–‰: ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œì—ì„œë§Œ í˜¸ì¶œë˜ë©°,
    ë©”ì¸ íŒŒì´í”„ë¼ì¸ê³¼ëŠ” ë¶„ë¦¬ëœ í‰ê°€ ë£¨í‹´ì…ë‹ˆë‹¤.
    """

    LEARNER_REGISTRY = {
        "S-Learner": SLearner,
        "T-Learner": TLearner,
        "X-Learner": XLearner,
        "DR-Learner": DRLearner,
        "R-Learner": RLearner,
    }

    def __init__(self, config: WhyLabConfig) -> None:
        super().__init__(name="benchmark_cell", config=config)

    def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹¤í–‰.

        Args:
            inputs: ë¹ˆ dict ë˜ëŠ” ì‚¬ì „ ì„¤ì •.
                ì„ íƒ í‚¤: "benchmark_datasets" (list[str])

        Returns:
            ê²°ê³¼ í…Œì´ë¸” + ê°œë³„ ì§€í‘œ.
        """
        cfg = self.config.benchmark
        datasets = inputs.get("benchmark_datasets", cfg.datasets)

        all_results = {}

        for ds_name in datasets:
            if ds_name not in BENCHMARK_REGISTRY:
                self.logger.warning("ì•Œ ìˆ˜ ì—†ëŠ” ë²¤ì¹˜ë§ˆí¬: %s", ds_name)
                continue

            loader = BENCHMARK_REGISTRY[ds_name]()

            self.logger.info("=" * 60)
            self.logger.info("ğŸ“Š ë²¤ì¹˜ë§ˆí¬: %s", ds_name.upper())
            self.logger.info("=" * 60)

            # ì—¬ëŸ¬ ë°˜ë³µ(replication)ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
            ds_results = self._evaluate_dataset(
                loader, ds_name, n_reps=cfg.n_replications,
            )
            all_results[ds_name] = ds_results

        # ë¹„êµí‘œ ìƒì„±
        table = self._format_comparison_table(all_results)
        self.logger.info("\n%s", table)

        return {
            **inputs,
            "benchmark_results": all_results,
            "benchmark_table": table,
        }

    def _evaluate_dataset(
        self,
        loader,
        ds_name: str,
        n_reps: int = 10,
    ) -> Dict[str, Dict[str, float]]:
        """ë‹¨ì¼ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë“  ë©”íƒ€ëŸ¬ë„ˆë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

        n_repsë²ˆ ë°˜ë³µ í›„ í‰ê· /í‘œì¤€í¸ì°¨ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤.
        """
        # ë©”íƒ€ëŸ¬ë„ˆë³„ ì§€í‘œ ëˆ„ì 
        metrics_acc = {name: {"pehe": [], "ate_bias": []}
                       for name in self.LEARNER_REGISTRY}
        # ê¸°ì¤€ì„  ì¶”ê°€
        metrics_acc["LinearDML"] = {"pehe": [], "ate_bias": []}
        metrics_acc["Ensemble"] = {"pehe": [], "ate_bias": []}

        for rep in range(n_reps):
            data = loader.load(seed=42 + rep)

            # â”€â”€ ê°œë³„ ë©”íƒ€ëŸ¬ë„ˆ â”€â”€
            learner_cates = {}
            for name, LearnerClass in self.LEARNER_REGISTRY.items():
                try:
                    learner = LearnerClass(config=self.config)
                    learner.fit(data.X, data.T, data.Y)
                    tau_hat = learner.predict_cate(data.X)
                    learner_cates[name] = tau_hat

                    pehe = self._sqrt_pehe(tau_hat, data.tau_true)
                    ate_bias = self._ate_bias(tau_hat, data.tau_true)
                    metrics_acc[name]["pehe"].append(pehe)
                    metrics_acc[name]["ate_bias"].append(ate_bias)

                except Exception as e:
                    self.logger.warning("  %s (rep=%d) ì‹¤íŒ¨: %s", name, rep, e)
                    metrics_acc[name]["pehe"].append(float("nan"))
                    metrics_acc[name]["ate_bias"].append(float("nan"))

            # â”€â”€ ê¸°ì¤€ì„ : LinearDML (EconML) â”€â”€
            try:
                from econml.dml import LinearDML
                from engine.gpu_factory import create_lgbm_regressor

                model_y = create_lgbm_regressor(self.config)
                model_t = create_lgbm_regressor(self.config)
                dml = LinearDML(model_y=model_y, model_t=model_t, cv=3,
                                random_state=42 + rep)
                dml.fit(Y=data.Y, T=data.T, X=data.X)
                tau_dml = dml.effect(data.X).flatten()

                metrics_acc["LinearDML"]["pehe"].append(
                    self._sqrt_pehe(tau_dml, data.tau_true))
                metrics_acc["LinearDML"]["ate_bias"].append(
                    self._ate_bias(tau_dml, data.tau_true))
            except Exception as e:
                self.logger.warning("  LinearDML (rep=%d) ì‹¤íŒ¨: %s", rep, e)
                metrics_acc["LinearDML"]["pehe"].append(float("nan"))
                metrics_acc["LinearDML"]["ate_bias"].append(float("nan"))

            # â”€â”€ ì•™ìƒë¸” (MSE ì—­ìˆ˜ ê°€ì¤‘) â”€â”€
            if learner_cates:
                try:
                    cate_stack = np.column_stack(list(learner_cates.values()))
                    # ê°œë³„ PEHEë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš© (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                    pehe_vals = np.array([
                        self._sqrt_pehe(c, data.tau_true)
                        for c in learner_cates.values()
                    ])
                    weights = np.exp(-pehe_vals / (pehe_vals.mean() + 1e-10))
                    weights = weights / weights.sum()
                    ensemble = (cate_stack * weights[np.newaxis, :]).sum(axis=1)

                    metrics_acc["Ensemble"]["pehe"].append(
                        self._sqrt_pehe(ensemble, data.tau_true))
                    metrics_acc["Ensemble"]["ate_bias"].append(
                        self._ate_bias(ensemble, data.tau_true))
                except Exception:
                    metrics_acc["Ensemble"]["pehe"].append(float("nan"))
                    metrics_acc["Ensemble"]["ate_bias"].append(float("nan"))

            self.logger.info("  âœ… Replication %d/%d ì™„ë£Œ", rep + 1, n_reps)

        # í‰ê·  Â± í‘œì¤€í¸ì°¨
        results = {}
        for name, metrics in metrics_acc.items():
            pehe_arr = np.array(metrics["pehe"])
            bias_arr = np.array(metrics["ate_bias"])
            results[name] = {
                "pehe_mean": float(np.nanmean(pehe_arr)),
                "pehe_std": float(np.nanstd(pehe_arr)),
                "ate_bias_mean": float(np.nanmean(bias_arr)),
                "ate_bias_std": float(np.nanstd(bias_arr)),
            }
            self.logger.info(
                "  %s: âˆšPEHE=%.4fÂ±%.4f, ATE Bias=%.4fÂ±%.4f",
                name.ljust(12),
                results[name]["pehe_mean"], results[name]["pehe_std"],
                results[name]["ate_bias_mean"], results[name]["ate_bias_std"],
            )

        return results

    def _format_comparison_table(
        self, all_results: Dict[str, Dict[str, Dict[str, float]]],
    ) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë¹„êµí‘œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ë…¼ë¬¸ì— ì§ì ‘ ë¶™ì—¬ë„£ì„ ìˆ˜ ìˆëŠ” í˜•ì‹ì…ë‹ˆë‹¤.
        """
        lines = ["| Method |"]
        separator = ["|---|"]

        ds_names = list(all_results.keys())
        for ds in ds_names:
            lines[0] += f" {ds.upper()} âˆšPEHE | {ds.upper()} ATE Bias |"
            separator[0] += "---|---|"

        header = lines[0]
        sep = separator[0]

        rows = []
        # ëª¨ë“  ë©”ì„œë“œ ìˆ˜ì§‘
        all_methods = set()
        for ds_result in all_results.values():
            all_methods.update(ds_result.keys())

        # ìˆœì„œ ê³ ì •
        ordered = ["S-Learner", "T-Learner", "X-Learner", "DR-Learner",
                    "R-Learner", "LinearDML", "Ensemble"]
        for method in ordered:
            if method not in all_methods:
                continue
            row = f"| {method} |"
            for ds in ds_names:
                if ds in all_results and method in all_results[ds]:
                    r = all_results[ds][method]
                    pehe_str = f" {r['pehe_mean']:.4f}Â±{r['pehe_std']:.4f} |"
                    bias_str = f" {r['ate_bias_mean']:.4f}Â±{r['ate_bias_std']:.4f} |"
                else:
                    pehe_str = " â€” |"
                    bias_str = " â€” |"
                row += pehe_str + bias_str
            rows.append(row)

        table = "\n".join([header, sep] + rows)
        return table

    @staticmethod
    def _sqrt_pehe(tau_hat: np.ndarray, tau_true: np.ndarray) -> float:
        """âˆšPEHE (Precision in Estimation of HTE)."""
        return float(np.sqrt(np.mean((tau_hat - tau_true) ** 2)))

    @staticmethod
    def _ate_bias(tau_hat: np.ndarray, tau_true: np.ndarray) -> float:
        """ATE Bias = |ATE_est - ATE_true|."""
        return float(np.abs(np.mean(tau_hat) - np.mean(tau_true)))
