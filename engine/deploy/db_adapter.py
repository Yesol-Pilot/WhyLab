# -*- coding: utf-8 -*-
"""ë„¤ì´í‹°ë¸Œ Postgres ì–´ëŒ‘í„° (DLQ/ë¬´ê²°ì„± í•´ì‹œìš©).

í”„ë¡œí† ì½œ ë¶„ë¦¬:
- DATABASE_URL (í¬íŠ¸ 6543, Supavisor): DLQ/í•´ì‹œ ì“°ê¸° (psycopg2)
- SUPABASE_URL (HTTPS): ì½ê¸° ì „ìš© REST API (urllib, í´ë°±)

ê³ ë¶€í•˜ ë°©ì–´:
- ThreadPoolExecutor ë¹„ë™ê¸° ë˜í¼: ë©”ì¸ ìŠ¤ë ˆë“œ ë¸”ë¡œí‚¹ ì°¨ë‹¨
- ë°°ì¹˜ ì‚½ì… ë²„í¼: 1ì´ˆ ê°„ê²© ì¼ê´„ INSERT
"""

from __future__ import annotations

import json
import logging
import os
import queue
import threading
import time
from typing import Any, Dict, List, Optional

logger = logging.getLogger("whylab.deploy.db_adapter")

# â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ë„¤ì´í‹°ë¸Œ PG (Supavisor Transaction Mode, í¬íŠ¸ 6543)
DATABASE_URL = os.environ.get("DATABASE_URL", "")

# HTTPS REST API (PostgREST, í´ë°± ì „ìš©)
SUPABASE_URL = os.environ.get("SUPABASE_URL", "")
SUPABASE_KEY = os.environ.get("SUPABASE_ANON_KEY", "")


def _get_pg_connection():
    """psycopg2 ì»¤ë„¥ì…˜ ìƒì„± (Supavisor ê²½ìœ )."""
    if not DATABASE_URL:
        return None
    try:
        import psycopg2
        return psycopg2.connect(DATABASE_URL, connect_timeout=5)
    except ImportError:
        logger.warning("psycopg2 not installed â€” falling back to REST API")
        return None
    except Exception as e:
        logger.error("PG connection failed: %s", e)
        return None


class AsyncDLQWriter:
    """ë¹„ë¸”ë¡œí‚¹ DLQ ì“°ê¸° â€” ë°±ê·¸ë¼ìš´ë“œ ë°°ì¹˜ ì‚½ì….

    ë©”ì¸ ìŠ¤ë ˆë“œëŠ” íì— ë„£ê¸°ë§Œ í•˜ê³  ì¦‰ì‹œ ë°˜í™˜.
    ë°±ê·¸ë¼ìš´ë“œ ë°ëª¬ ìŠ¤ë ˆë“œê°€ 1ì´ˆ ê°„ê²©ìœ¼ë¡œ ë°°ì¹˜ INSERT.

    ìŠ¤ë ˆë“œ ê³ ê°ˆ ë°©ì§€:
    - ë©”ì¸ ìŠ¤ë ˆë“œëŠ” queue.put() ë§Œ ìˆ˜í–‰ (< 1Î¼s)
    - DB I/OëŠ” ì „ìš© ìŠ¤ë ˆë“œ 1ê°œì—ì„œë§Œ ë°œìƒ
    - í ìƒí•œ(maxsize=10000)ìœ¼ë¡œ ë©”ëª¨ë¦¬ í­ë°œ ë°©ì§€
    """

    def __init__(self, batch_interval: float = 1.0, max_queue: int = 10000) -> None:
        self._queue: queue.Queue = queue.Queue(maxsize=max_queue)
        self._batch_interval = batch_interval
        self._stop_event = threading.Event()
        self._memory_fallback: List[Dict] = []  # DB ë¯¸ì—°ê²° ì‹œ
        self._persisted_count = 0
        self._error_count = 0

        # ë°±ê·¸ë¼ìš´ë“œ ë°ëª¬ ìŠ¤ë ˆë“œ ì‹œì‘
        self._thread = threading.Thread(
            target=self._batch_worker,
            name="dlq-writer",
            daemon=True,
        )
        self._thread.start()
        logger.info("ğŸ“¥ DLQ writer started (batch=%ss, max_queue=%d)", batch_interval, max_queue)

    def enqueue(self, decision_id: str, payload: Dict, reason: str = "breaker_tripped") -> None:
        """ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ í˜¸ì¶œ â€” ì¦‰ì‹œ ë°˜í™˜ (Non-blocking).

        íê°€ ê°€ë“ ì°¨ë©´ ì¸ë©”ëª¨ë¦¬ í´ë°±ì— ì €ì¥.
        """
        entry = {
            "decision_id": decision_id,
            "reason": reason,
            "payload": payload,
            "timestamp": time.time(),
        }
        try:
            self._queue.put_nowait(entry)
        except queue.Full:
            self._memory_fallback.append(entry)
            logger.warning("âš ï¸ DLQ queue full â€” memory fallback (size=%d)", len(self._memory_fallback))

    def _batch_worker(self) -> None:
        """ë°±ê·¸ë¼ìš´ë“œ ë°°ì¹˜ ì‚½ì… ë£¨í”„."""
        while not self._stop_event.is_set():
            time.sleep(self._batch_interval)
            batch = self._drain_queue()
            if not batch:
                continue

            # ë„¤ì´í‹°ë¸Œ PG ì‚½ì… ì‹œë„
            if self._batch_insert_pg(batch):
                self._persisted_count += len(batch)
                continue

            # REST API í´ë°±
            if self._batch_insert_rest(batch):
                self._persisted_count += len(batch)
                continue

            # ìµœì¢… í´ë°±: ì¸ë©”ëª¨ë¦¬ ë³´ì¡´
            self._memory_fallback.extend(batch)
            self._error_count += 1
            logger.error("âŒ DLQ batch failed â€” %d entries in memory fallback", len(batch))

    def _drain_queue(self, max_batch: int = 100) -> List[Dict]:
        """íì—ì„œ í•­ëª© êº¼ë‚´ê¸°."""
        batch = []
        while len(batch) < max_batch:
            try:
                batch.append(self._queue.get_nowait())
            except queue.Empty:
                break
        return batch

    def _batch_insert_pg(self, batch: List[Dict]) -> bool:
        """psycopg2 ë„¤ì´í‹°ë¸Œ ë°°ì¹˜ INSERT."""
        conn = _get_pg_connection()
        if not conn:
            return False

        try:
            with conn:
                with conn.cursor() as cur:
                    args = [
                        (e["decision_id"], e["reason"], json.dumps(e["payload"]))
                        for e in batch
                    ]
                    cur.executemany(
                        "INSERT INTO audit_dlq (decision_id, reason, payload) "
                        "VALUES (%s, %s, %s::jsonb)",
                        args,
                    )
            logger.info("ğŸ“¥ DLQ batch persisted via PG: %d entries", len(batch))
            return True
        except Exception as e:
            logger.warning("âš ï¸ PG batch insert failed: %s", e)
            return False
        finally:
            conn.close()

    def _batch_insert_rest(self, batch: List[Dict]) -> bool:
        """REST API í´ë°± ë°°ì¹˜ INSERT."""
        if not (SUPABASE_URL and SUPABASE_KEY):
            return False

        try:
            import urllib.request
            url = f"{SUPABASE_URL}/rest/v1/audit_dlq"
            headers = {
                "apikey": SUPABASE_KEY,
                "Authorization": f"Bearer {SUPABASE_KEY}",
                "Content-Type": "application/json",
                "Prefer": "return=minimal",
            }
            body = json.dumps([
                {"decision_id": e["decision_id"], "reason": e["reason"], "payload": e["payload"]}
                for e in batch
            ]).encode()
            req = urllib.request.Request(url, data=body, headers=headers, method="POST")
            with urllib.request.urlopen(req, timeout=5) as resp:
                if resp.status in (200, 201):
                    logger.info("ğŸ“¥ DLQ batch persisted via REST: %d entries", len(batch))
                    return True
        except Exception as e:
            logger.warning("âš ï¸ REST batch insert failed: %s", e)
        return False

    def shutdown(self, timeout: float = 5.0) -> None:
        """ì¢…ë£Œ â€” ì”ì—¬ í•­ëª© í”ŒëŸ¬ì‹œ."""
        self._stop_event.set()
        self._thread.join(timeout=timeout)
        # ì”ì—¬ í ì²˜ë¦¬
        remaining = self._drain_queue(max_batch=10000)
        if remaining:
            if not self._batch_insert_pg(remaining):
                self._batch_insert_rest(remaining)

    @property
    def stats(self) -> Dict[str, Any]:
        return {
            "queue_size": self._queue.qsize(),
            "persisted_count": self._persisted_count,
            "memory_fallback_size": len(self._memory_fallback),
            "error_count": self._error_count,
            "thread_alive": self._thread.is_alive(),
        }


class IntegrityHashWriter:
    """ë¬´ê²°ì„± í•´ì‹œ DB ì €ì¥ (ë„¤ì´í‹°ë¸Œ PG)."""

    @staticmethod
    def store(hash_entry: Dict, date_str: str) -> bool:
        """integrity_hashes í…Œì´ë¸”ì— UPSERT."""
        conn = _get_pg_connection()
        if not conn:
            return False

        try:
            with conn:
                with conn.cursor() as cur:
                    cur.execute(
                        """
                        INSERT INTO integrity_hashes 
                            (rollup_date, sha256_hash, record_count, data_bytes)
                        VALUES (%s, %s, %s, %s)
                        ON CONFLICT (rollup_date) DO UPDATE SET
                            sha256_hash = EXCLUDED.sha256_hash,
                            record_count = EXCLUDED.record_count,
                            data_bytes = EXCLUDED.data_bytes
                        """,
                        (
                            date_str,
                            hash_entry["sha256"],
                            hash_entry["record_count"],
                            hash_entry["bytes"],
                        ),
                    )
            logger.info("âœ… Integrity hash stored via PG: %s", date_str)
            return True
        except Exception as e:
            logger.warning("âš ï¸ PG hash write failed: %s", e)
            return False
        finally:
            conn.close()
