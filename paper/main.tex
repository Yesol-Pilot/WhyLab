% WhyLab: Causal Audit Framework for Autonomous Agent Self-Improvement
% Target: AAAI 2027 / KDD ADS Track
% Draft skeleton â€” fill Experiments after shadow deployment data collection

\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}

\title{Causal Auditing of Autonomous Agents in the Wild:\\
A Pragmatic Framework for Safe Self-Improvement}

\author{Yesol Heo}

\begin{document}
\maketitle

\begin{abstract}
Autonomous AI agents that self-improve through feedback loops face a
critical risk: hallucination-driven policy oscillation leading to
performance divergence. We present WhyLab, a pragmatic causal audit
framework that (1) detects causal drift via information-theoretic
weighted indices, (2) quantifies robustness against unobserved
confounders using E-values, and (3) guarantees bounded strategy updates
through Lyapunov stability analysis. Deployed as a shadow system
alongside production agents, WhyLab's cost-aware circuit breaker and
three-phase promotion pipeline ensure safe deployment under real-world
API budget and latency constraints.

% TODO: Add quantitative results after shadow deployment (3-4 months)
\end{abstract}


%% ============================================================
\section{Introduction}

Autonomous AI agents that iteratively refine their own strategies
through feedback loops---e.g., via RAG knowledge updates, prompt
mutation, or reinforcement from user signals---represent a powerful
paradigm for continuous improvement~\cite{shinn2023reflexion,yao2023react}.
However, this self-improvement mechanism introduces a critical failure
mode: when hallucinated or spuriously correlated feedback accumulates,
the agent's policy begins to oscillate between contradictory strategies,
a phenomenon we term \emph{cognitive policy oscillation}. Left unchecked,
this oscillation degrades system performance below the pre-improvement
baseline, silently and irreversibly.

Existing causal inference libraries such as DoWhy~\cite{dowhy2022}
and EconML~\cite{econml2019} provide rigorous statistical tools for
effect estimation, but they were designed for offline research
workflows, not for continuous integration with live production agents.
Specifically, they do not address: (1) real-time API quota constraints
that throttle data ingestion, (2) the computational cost of LLM-based
evaluators that must operate under strict daily budgets, or (3) the
heavy-tailed noise distributions arising from multi-agent environments
where gradient variance can be unbounded.

We present \textbf{WhyLab}, a pragmatic causal audit framework
designed for deployment alongside production AI agents. Our system
makes three contributions:

\begin{enumerate}
\item \textbf{Information-theoretic drift detection.} An entropy-weighted
  drift index (\S\ref{sec:drift}) that automatically adapts signal
  importance based on information content, preventing
  winner-takes-all collapse across heterogeneous audit streams.
\item \textbf{Sensitivity-aware effect estimation.} Integration of
  E-values~\cite{vanderweele2017} and partial $R^2$
  bounds~\cite{cinelli2020} to quantify robustness against unmeasured
  confounders, moving beyond na\"ive significance testing.
\item \textbf{Lyapunov-bounded strategy updates.} A mathematically
  grounded damping mechanism (\S\ref{sec:lyapunov}) that clips agent
  learning rates to guarantee bounded energy dissipation, with
  explicit heavy-tail robustness guarantees.
\end{enumerate}

Critically, WhyLab wraps these theoretical components in production
infrastructure: a cost-aware circuit breaker with dead letter queue,
non-blocking shadow deployment via deterministic canary routing, and
SHA-256 append-only audit trails. We deploy WhyLab as a shadow system
alongside \texttt{XX} production agent sites, collecting \texttt{XX}
decisions over \texttt{XX} days to validate our three hypotheses
(\S\ref{sec:experiments}).


%% ============================================================
\section{Methodology}

% Mathematical backbone: R1, R2, R5
% Honest scope: these are production-ready components

\subsection{Information-Theoretic Drift Index (R1)}
\label{sec:drift}

\begin{definition}[Drift Index]
Given verdict, ATE, and confidence signal streams $S_1, S_2, S_3$
with Shannon entropies $H(S_i)$, the drift index is:
\[
DI = \sum_{i=1}^{3} w_i \cdot d_i, \quad
w_i = \frac{1/(H(S_i) + \epsilon)}{\sum_j 1/(H(S_j) + \epsilon)}
\]
where $\epsilon = 0.1$ prevents zero-entropy explosion and
$w_i \leq 0.6$ prevents winner-takes-all collapse.
Bin count follows Sturges' Rule: $k = \lceil 1 + \log_2 N \rceil$.
\end{definition}


\subsection{Sensitivity Analysis via E-value (R2)}

\begin{definition}[E-value \cite{vanderweele2017}]
For risk ratio $RR = \exp(|ATE| / \sigma_{pre})$:
\[
E = RR + \sqrt{RR \cdot (RR - 1)}
\]
Interpretation: an unobserved confounder must associate with both
treatment and outcome at $RR \geq E$ to nullify the observed effect.
\end{definition}

Robustness Value $RV_q \approx \sqrt{\partial R^2_T}$
provides the Cinelli \& Hazlett (2020) partial $R^2$ bound.


\subsection{Lyapunov-Bounded Damping (R5)}
\label{sec:lyapunov}

\begin{theorem}[Convergence of Adaptive Damping]
Let $\theta_t$ be the agent's strategy, updated as
$\theta_{t+1} = \theta_t - \zeta_t \hat{g}_t$ where
$\hat{g}_t = g_t + \omega_t$ ($g_t$: true causal gradient,
$\omega_t$: noise from LLM judgment and environmental drift).

Define Lyapunov energy $V(\theta_t) = \frac{1}{2}\|\theta_t - \theta^*\|^2$.

Then $\mathbb{E}[\Delta V] \leq 0$ if and only if:
\[
\zeta_t \leq \zeta_{max} = \frac{2 \langle \theta_t - \theta^*, g_t \rangle}
{\mathbb{E}[\|\hat{g}_t\|^2]}
\]

Since $\mathbb{E}[\|\hat{g}_t\|^2] \propto (DI + \epsilon_{ARES})$,
reducing $\zeta_t$ under high uncertainty is not heuristic but
\emph{mathematically necessary} for stability.
\end{theorem}

\begin{proof}
\textbf{[Full proof in Appendix A]}
\end{proof}

\begin{corollary}[Heavy-Tail Robustness]
When gradient noise $\omega_t$ follows a heavy-tailed distribution
(e.g., Pareto with infinite variance), $\zeta_{max} \to 0$,
causing permanent learning deadlock. To prevent this, we enforce
a hard floor-ceiling constraint:
\[
\zeta_t \in [\epsilon_{\text{floor}}, \mathcal{C}]
\quad \text{where } \epsilon_{\text{floor}} = 0.01,\; \mathcal{C} = 0.8
\]
This guarantees $\zeta_t \geq \epsilon_{\text{floor}} > 0$
even under arbitrarily large noise variance, ensuring the agent
continues learning at a minimal but non-zero rate.
\end{corollary}

\begin{remark}[Observable Energy Proxy]
Since the optimal strategy $\theta^*$ is unobservable in practice,
we define an empirical proxy for the Lyapunov energy function using
the agent's task reward $R_t$:
\[
V_t \approx \frac{1}{2}(R_{max} - R_t)^2
\]
where $R_{max}$ is the achievable upper bound (e.g., maximum observed
conversion rate over a calibration window). This proxy satisfies the
key requirement: $V_t \to 0$ as $R_t \to R_{max}$, and $V_t$ decreasing
after $\zeta$-clipping events provides empirical evidence of stabilization.
To account for non-stationary environments, we apply a rolling-window
exponential moving average (EMA) to $R_{max}$ with decay factor $\alpha = 0.1$.
\end{remark}

%% ============================================================
\section{System Architecture \& Deployment Challenges}

% This section targets KDD ADS track reviewers
% Honest about limitations and cost tradeoffs

\subsection{Infrastructure Constraints}

Deploying a causal audit layer atop a production agent introduces
four engineering challenges, each addressed by a dedicated component:

\textbf{C1: GA4 Quota Limits.}
Google Analytics 4 imposes strict API quotas. We decouple event
ingestion via a Transactional Outbox pattern (C3): audit events are
first persisted to a local outbox table, then drained asynchronously
by a background worker. This guarantees zero data loss under quota
throttling.

\textbf{C2: Database Growth.}
With 12 monitored sites generating $\sim$500 decision events per day,
raw audit logs grow $\sim$180K rows/month. We partition by week and
aggregate via \texttt{daily\_agent\_rollup}, compressing 7 days of
raw data into a single summary row per site.

\textbf{C3: Trace Explosion.}
OpenTelemetry spans for every LLM call produce $>$10K spans/day.
We implement head-based dynamic sampling: high-drift decisions are
traced at 100\%, stable decisions at 10\%.

\textbf{C4: LLM Cost Control.}
ARES deep audit calls cost \$0.003--0.01 per evaluation. A daily
hard budget (\$10/day, 100K tokens) with automatic circuit breaker
prevents runaway costs. Breaker-blocked audits are preserved in a
Dead Letter Queue (DLQ) for offline batch recovery, ensuring no
sample loss for the experiment dataset.


\subsection{Safe Deployment via Shadow Mode}

We adopt a three-phase promotion pipeline inspired by
canary deployment practices in production ML systems:

\begin{enumerate}
\item \textbf{DRY\_RUN:} The audit engine observes agent decisions
  and computes $\zeta_{max}$, DI, and E-values, but does \emph{not}
  apply any feedback. All observations are logged for offline analysis.
\item \textbf{SHADOW\_ACTIVE:} Feedback is applied to a shadow copy
  of the agent's strategy memory. Performance is compared against
  the unmodified control path.
\item \textbf{PRODUCTION:} Full integration after statistical
  significance is established via Bayesian decision criteria.
\end{enumerate}

\textbf{Non-blocking integration.}
The shadow adapter uses fire-and-forget semantics with a 2-second
AbortController timeout. The main agent pipeline experiences
zero latency overhead: shadow failures are silently caught.

\textbf{Deterministic canary routing.}
Traffic allocation uses a hash-based deterministic router
(\texttt{djb2(cycleId) \% 100 < threshold}) rather than random
sampling, ensuring complete trace integrity across multi-step
agent decisions.

\textbf{Data integrity.}
Daily rollup snapshots are SHA-256 hashed and committed to an
append-only log via automated CI pipeline, providing a
timestamped evidence chain against post-hoc data manipulation.


%% ============================================================
\section{Experiments}
\label{sec:experiments}

% CRITICAL: Use ONLY real shadow deployment data here
% Do NOT cite synthetic Who&When or CausalFlip as SOTA comparisons

\subsection{Live Shadow Deployment}

We structure the evaluation around three hypotheses, each
addressing a distinct contribution claim:

\textbf{H1: Lyapunov clipping prevents policy oscillation.}
Using the observable energy proxy $V_t \approx \frac{1}{2}(R_{max} - R_t)^2$
where $R_{max}$ is the rolling 30-day maximum conversion rate,
we plot $V_t$ over time with clipping events ($\zeta_t > \zeta_{max}$)
marked as vertical indicators. If $V_t$ consistently decreases or
stabilizes after clipping events, H1 is supported.

\textbf{H2: Cost circuit breaker maintains reliability under budget.}
We overlay daily traffic volume against ARES token consumption
and USD cost. When the breaker trips at the \$10 daily limit,
blocked audits are routed to the DLQ while the system continues
operating on lightweight fallback. We report the fraction of
decisions processed by real vs.\ fallback audit, and measure whether
agent performance degrades during fallback periods.

\textbf{H3: Causal audit distinguishes robust from fragile successes.}
We aggregate E-values from \texttt{daily\_agent\_rollup} and report
the fraction of agent decisions robust to unobserved confounders
($E > 2.0$). We frame this as sensitivity analysis: ``Of $N$ decisions
over 30 days, only $M$ (\%$p$) exhibited causal effects that an
unmeasured confounder would need RR $\geq E$ to explain away.
Standard A/B testing would have attributed all $N$ as successes.''

\textbf{[Data collection in progress --- results after 30-day shadow period.]}


\subsection{Ablation Study}

% Which components matter most?
% Table: Full system vs. no-R1 vs. no-R5 vs. no-C3

\textbf{[PLACEHOLDER --- run after live data collection]}


\subsection{Limitations}
\label{sec:limitations}

We explicitly acknowledge the following limitations:
\begin{itemize}
\item \textbf{ARES evaluator}: Currently uses mock LLM judges in
  the default configuration. Real-LLM opt-in mode (Claude 3.5 Haiku,
  GPT-4o-mini) is implemented with 3-second timeout and exponential
  backoff, but production-scale cost-performance tradeoffs remain
  under evaluation during the shadow deployment period.
\item \textbf{Observable energy proxy}: The Lyapunov energy function
  $V_t \approx \frac{1}{2}(R_{max} - R_t)^2$ is a pragmatic
  approximation. It assumes reward monotonicity and a known upper
  bound $R_{max}$, which may not hold in non-stationary environments.
\item \textbf{Shapley attribution}: Full MACIE computation is
  $O(2^n)$; our implementation uses a lightweight heuristic
  approximation, not exact Shapley values.
\item \textbf{Data integrity}: SHA-256 hash logs provide a
  timestamped evidence chain but do not constitute cryptographic
  proof against a determined adversary with repository admin access.
  We characterize this as an automated audit trail, not a
  blockchain-grade guarantee.
\item \textbf{DLQ recovery}: Breaker-blocked audits preserved in
  the Dead Letter Queue are processed offline. This introduces a
  survivorship bias: the real-time audit stream during high-cost
  periods reflects only fallback (mock) evaluations.
\item \textbf{DAG discovery degradation}: In cases of extreme
  multicollinearity (e.g., deterministic agent exploration with
  zero variance in treatment), the PC algorithm degrades to a
  correlation-based heuristic with fixed threshold. Consequently,
  downstream Shapley attribution in these sub-routines reflects
  associative rather than strictly causal contributions.
\end{itemize}


%% ============================================================
\section{Related Work}

\textbf{Causal inference tools.}
DoWhy~\cite{dowhy2022} provides a principled four-step workflow
(model, identify, estimate, refute) for causal effect estimation,
while EconML~\cite{econml2019} extends this with heterogeneous
treatment effect estimators such as Double Machine Learning.
These libraries excel at offline analysis but assume unbounded
computation time and static datasets. WhyLab differs in that it
operates as a \emph{continuous, real-time audit layer} atop
production agents, subject to API quotas, cost budgets, and
latency constraints that preclude batch-mode inference.

\textbf{Multi-agent failure attribution.}
Zhang et al.~\cite{zhang2025whowhen} formalize the problem of
identifying \emph{which} agent causes task failures and \emph{when},
introducing the Who\&When benchmark for LLM multi-agent systems.
Their work convincingly demonstrates the complexity of failure
attribution in collaborative settings. WhyLab complements this
line of research by moving from offline benchmark evaluation to
live shadow deployment, where attribution results directly feed
back into agent strategy updates via Lyapunov-bounded damping.
Our lightweight Shapley approximation trades exact attribution
for real-time feasibility under production constraints.

\textbf{LLM evaluation with statistical guarantees.}
ARES~\cite{saadfacon2025ares} proposes using LLM judges with
prediction-powered inference to provide statistical guarantees
on RAG system quality. We adopt a simplified version of the
ARES evaluation paradigm but introduce a critical pragmatic
extension: a cost circuit breaker that caps daily LLM evaluation
spend at a fixed budget (\$10/day), routing excess evaluations to
a Dead Letter Queue for offline batch processing. This ensures
that the audit system itself does not become a cost liability.

\textbf{Safe reinforcement learning via Lyapunov methods.}
Chow et al.~\cite{chow2018lyapunov} establish Lyapunov-based
constraints for safe policy optimization in continuous control
tasks, while Berkenkamp et al.~\cite{berkenkamp2017safe} extend
this to model-based settings with stability guarantees. These
works focus on physical safety in simulated environments.
WhyLab adapts the Lyapunov energy framework to a fundamentally
different domain: bounding \emph{cognitive} oscillation in LLM
agent strategy updates, where the ``gradient noise'' arises not
from sensor noise but from hallucinated LLM judgments and
non-stationary web traffic distributions. Our heavy-tail
robustness corollary (\S\ref{sec:lyapunov}) addresses the
specific challenge of unbounded variance in LLM evaluation noise.


%% ============================================================
\section{Conclusion}

\textbf{[PLACEHOLDER --- write after 30-day data collection]}


%% ============================================================
\appendix
\section{Lyapunov Stability Proof}
\label{sec:lyapunov-proof}

We provide the full derivation of the adaptive damping bound
$\zeta_{max}$ referenced in Theorem~1.

\begin{proof}
Let $\theta_t \in \mathbb{R}^d$ denote the agent's strategy vector
at time $t$, and $\theta^*$ the (unknown) optimal strategy. The update
rule is:
\[
\theta_{t+1} = \theta_t - \zeta_t \hat{g}_t
\]
where $\hat{g}_t = g_t + \omega_t$ is the noisy gradient estimate,
$g_t = \nabla_{\theta} L(\theta_t)$ is the true causal gradient, and
$\omega_t$ is noise from LLM judgment uncertainty and environmental
drift.

\textbf{Step 1: Energy change.}
Define the Lyapunov energy function $V(\theta_t) = \frac{1}{2}
\|\theta_t - \theta^*\|^2$. The change in energy is:
\begin{align}
\Delta V &= V(\theta_{t+1}) - V(\theta_t) \nonumber \\
&= \frac{1}{2}\|\theta_t - \zeta_t \hat{g}_t - \theta^*\|^2
   - \frac{1}{2}\|\theta_t - \theta^*\|^2 \nonumber \\
&= -\zeta_t \langle \theta_t - \theta^*, \hat{g}_t \rangle
   + \frac{\zeta_t^2}{2}\|\hat{g}_t\|^2
\end{align}

\textbf{Step 2: Expectation.}
Taking expectation over the noise $\omega_t$:
\begin{align}
\mathbb{E}[\Delta V] &= -\zeta_t \langle \theta_t - \theta^*, g_t \rangle
   + \frac{\zeta_t^2}{2}\mathbb{E}[\|\hat{g}_t\|^2]
\end{align}
since $\mathbb{E}[\omega_t] = 0$ by the unbiasedness assumption.
While raw LLM hallucinations often exhibit directional bias
(i.e., $\mathbb{E}[\omega_t] = \mu \neq 0$ due to sycophancy or
positivity bias), our integration of the ARES evaluator explicitly
rejects ungrounded premises and propagated errors, acting as a
debiasing filter that effectively centers the residual noise
distribution. Any remaining uncentered bias $\mu$ would introduce
an additional term $-\zeta_t \langle \theta_t - \theta^*, \mu \rangle$,
strictly requiring a tighter bound on $\zeta_{max}$---which our
conservative floor-ceiling guard ($\zeta_t \in [\epsilon_{\text{floor}},
\mathcal{C}]$) empirically accommodates.

\textbf{Step 3: Stability condition.}
For $\mathbb{E}[\Delta V] \leq 0$, we require:
\[
\zeta_t \leq \frac{2\langle \theta_t - \theta^*, g_t \rangle}
{\mathbb{E}[\|\hat{g}_t\|^2]} \triangleq \zeta_{max}
\]

\textbf{Step 4: Connection to system modules.}
The denominator $\mathbb{E}[\|\hat{g}_t\|^2] = \|g_t\|^2 +
\mathbb{E}[\|\omega_t\|^2]$ decomposes into true gradient magnitude
and noise variance. In WhyLab, the noise variance is empirically
proportional to two observable quantities:
\begin{itemize}
\item The \textbf{drift index} $DI$ (R1, \S\ref{sec:drift}):
  higher drift indicates more volatile underlying distributions,
  inflating $\mathbb{E}[\|\omega_t\|^2]$.
\item The \textbf{ARES evaluation uncertainty} $\epsilon_{ARES}$:
  disagreement among LLM judges increases effective noise.
\end{itemize}
Thus, $\mathbb{E}[\|\hat{g}_t\|^2] \propto (DI + \epsilon_{ARES})$,
and the clipping bound $\zeta_{max}$ automatically tightens when the
system detects high drift or low evaluator confidence---connecting
R1, R3, and R5 into a unified feedback control loop.

\textbf{Step 5: Heavy-tail guard.}
When $\omega_t$ follows a heavy-tailed distribution (e.g., Pareto
with shape parameter $\alpha \leq 2$), $\mathbb{E}[\|\omega_t\|^2]
\to \infty$, driving $\zeta_{max} \to 0$ and causing learning
deadlock. We prevent this by enforcing the hard constraint:
\[
\zeta_t \in [\epsilon_{\text{floor}}, \mathcal{C}]
\quad \text{where } \epsilon_{\text{floor}} = 0.01,\;
\mathcal{C} = 0.8
\]
This guarantees $\zeta_t \geq \epsilon_{\text{floor}} > 0$ even under
arbitrarily large noise variance, ensuring the agent continues
learning at a minimal but non-zero rate while remaining provably
stable within the ceiling $\mathcal{C}$.
\end{proof}


%% ============================================================
\bibliographystyle{aaai}
\bibliography{references}

\end{document}
